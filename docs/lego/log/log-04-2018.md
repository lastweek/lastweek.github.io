# April 2018

## 04/09 Mon
thpool testing. 4 workers. MT-phoenix:

```c
[root@wuklab05 ys]# cat 0409-p | grep __munmap
[  227.054974] CPU14 PID22 strace__munmap([0x7fffb0ba9000 - 0x7fffb4000000], 54882304) = 0, 0x0
[  227.093466] CPU16 PID23 strace__munmap([0x7fffab7ff000 - 0x7fffac000000], 8392704) = 0, 0x0
[  227.102773] CPU14 PID22 strace__munmap([0x7fffb8000000 - 0x7fffb8ba9000], 12226560) = 0, 0x0
[  227.141265] CPU18 PID24 strace__munmap([0x7fffa8000000 - 0x7fffac000000], 67108864) = 0, 0x0
[  227.150669] CPU16 PID23 strace__munmap([0x7fffb0000000 - 0x7fffb37ff000], 58716160) = 0, 0x0
[  227.218248] CPU22 PID26 strace__munmap([0x7fffa0000000 - 0x7fffa4000000], 67108864) = 0, 0x0
[  227.285826] CPU2 PID28 strace__munmap([0x7fff98000000 - 0x7fff9c000000], 67108864) = 0, 0x0
[  227.440567] CPU14 PID31 strace__munmap([0x7fff8a7fd000 - 0x7fff8c000000], 25178112) = 0, 0x0
[  227.449972] CPU12 PID30 strace__munmap([0x7fff88000000 - 0x7fff8c000000], 67108864) = 0, 0x0
[  227.459376] CPU14 PID31 strace__munmap([0x7fff90000000 - 0x7fff927fd000], 41930752) = 0, 0x0
[  227.490109] CPU18 PID33 strace__munmap([0x7fff80000000 - 0x7fff84000000], 67108864) = 0, 0x0
[  227.723140] word_count-pthr[29]: segfault at 0x4e842010 ip 0000000000420354 sp 00007fffb17f9bc0 error 6
0x4e842010
```

- Print mmap on M, if segfault. Printed, the `0x4e842010` is never a valid address. thpool makes Memory side SMP. Probably bring some issues.

Found:
```
P
CPU22 PID26 strace__mmap(addr=0x0, len=0xfb000, prot(0x3)=PROT_READ|PROT_WRITE, flags(0x22)=MAP_PRIVATE|MAP_ANONYMOUS, fd=18446744073709551615( ), off=0x0) = 1317351432, 0x4e853008
word_count-pthr[26]: segfault at 0x4e853010 ip 0000000000420354 sp 00007fff972a8bc0 error 6

M
[  583.120615]   00400000-004d9000 r-xp 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread
[  583.131578]   006d9000-006dc000 rw-p 000d9000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread
[  583.142729]   006dc000-00755000 rw-p 00000000 [heap]
[  583.148254]   7fff529c9000-7fffb93aa000 rw-p 00000000
[  583.153974]   7fffb93aa000-7ffff7fff000 rw-p 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_1GB.txt
[  583.167355]   7ffffffde000-7ffffffff000 rw-p 00000000 [stack]
[  583.173753] ------------[ cut here ]------------
[  583.178892] WARNING: CPU: 4 PID: 31 at managers/memory/handle_pcache/fault.c:55 handle_p2m_pcache_miss+0x18e/0x1d0
[  583.190430] src_nid:0,pid:21,vaddr:0x4e853010
[  583.195279] CPU: 4 PID: 31 Comm: thpool-worker0 4.0.0-lego-ys+ #90
```

Confirmed. I printed added a number to mmap requests. And the compare the results of both P and M. The data is wrong. Btw, I'm only running 1 worker thread at M, which makes it single thread handling. So, I'm going to, 1) first use kmalloc to get the reply buffer, and 2) revert back the IB MAX_OUT config, remove the #ifdef COMP_MEMORY. See if it is this patch's issue.
```c
P:
CPU18 PID24 strace__mmap(30 ..) = -1325940736, 0x7fffb0f7c000
CPU22 PID26 strace__mmap(31 ..) = 2144269992, 0x7fcef6a8

M:
...
handle_p2m_mmap(): 30 7fffb0f7c000
handle_p2m_mmap(): 31 7fffb0efe000
...
```

Anyway, this is {==temporary==} fixed by using kmalloced reply buffer.

Spent whole afternoon and whole night. Finally figure out why timeout happen in P. It is because somewhere in the middle, M has 1 or more requests stucked/unhandled. Deadlock happen in the middle.

Like this one. 5 requests queued waiting, 1 is being handled. And that 1 handler stuck. And it is handle_pcache_miss. Now, I need to find out where it stuck!
```
thpool-worker0 nr_queued: 5 1
```

Oh, I really hope we can have some {==soft/hw lockdep, watchdog stuff==}. This should make out life much much much much much easier!

## 04/08 Sun

Trying the fit_nowait patch.

- First try fit_nowait patch, without any chanegs to other code. See if this patch can work.
- Second, modify pcache to use reply_message_nowait. See if this can work. and improve performance.
- Third, if 2) can improve, perf. Move on to modify thpool patch.

1st, P fail at ib_mad, during boot:
```c
[  349.239220] Online CPU: 0,2,4,6,8,10,12,14,16,18,20,22
[  349.244940] Active CPU: 0,2,6,10,12,14,16,18,20,22
[  349.250272]   [0] Thread[kvictim_flushd:19] pinned at CPU 8
[  349.256478]   [1] Thread[recvpollcq:17] pinned at CPU 4
[  356.188819] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 13
[  356.197545] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020
[  356.206270] IP: [<ffffffff81058287>] ib_mad_completion_handler+0xc7/0x810
```

2st run, P side, config MAX_OUT to 1. Then single-thread pheonix with 1GB data finished. But forgot to turn on the profile point. Run one more time.

3st run. Same with 2st run setting. But with profile on. Bug shows. Ugh. I still think it is because of ib_mad_handler. It must write to wrong memory locations, and corrupt things randomly.
```c
[  456.237913] do_close_on_exec(): TODO, not implemented.
...
[  456.263274] BUG: unable to handle kernel paging request at 00000002f4bfbf58
[  456.270843] IP: [<ffffffff8101bbff>] task_tick_rt+0x1f/0xd0
[  456.277048] PGD 0
[  456.279279] Thread overran stack, or stack corrupted
[  456.284804] Oops: 0000 [#1] SMP PROCESSOR
[  456.289265] CPU: 10 PID: 20 Comm: kevict_sweepd 4.0.0-lego+ #40
[  456.295858] RIP: 0010:[<ffffffff8101bbff>]  [<ffffffff8101bbff>] task_tick_rt+0x1f/0xd0
```

4st run, succeed. But it looks like the perf is very bad. Oh. but 99% of the pcache miss are file-backed, which will go to storage. So the number is actually doubled.
```c
With fit_nowait patch:
[  308.660051] Kernel Profile Points
[  308.663734]  status                  name             total                nr            avg.ns
[  308.673431] -------  --------------------  ----------------  ----------------  ----------------
[  308.683128]     off      flush_tlb_others       0.000130715                53              2467
[  308.692824]     off     __do_kmalloc_node       0.097344056            265647               367
[  308.702521]     off           pcache_miss       4.504660891            258211             17446
[  308.712218]     off          pcache_flush       0.000000000                 0                 0
[  308.721914] -------  --------------------  ----------------  ----------------  ----------------

```

5st run. Just run large malloc test. Looks better than yesterday's result. But I'm using 15 as P today, instead of 13. So, let me try one more time to see if it is the machine.
```c
With fit_nowait patch:
[  674.382592] Kernel Profile Points
[  674.386277]  status                  name             total                nr            avg.ns
[  674.395974] -------  --------------------  ----------------  ----------------  ----------------
[  674.405670]     off      flush_tlb_others       0.000130838                53              2469
[  674.415366]     off     __do_kmalloc_node       1.604700641           1584917              1013
[  674.425062]     off           pcache_miss       6.467938547            786571              8223
[  674.434758]     off          pcache_flush       3.342783614            262225             12748
[  674.444455] -------  --------------------  ----------------  ----------------  ----------------
[  674.554497] nr_pgfault: 786513
[  674.557706] nr_clflush: 262225
[  674.561099] nr_pgfault_wp: 0
[  674.564299] nr_pgfault_wp_cow: 0
[  674.567887] nr_pgfault_wp_reuse: 0
[  674.571668] nr_pgfault_due_to_concurrent_eviction: 0
[  674.577195] nr_pcache_fill_from_memory: 786511
[  674.582139] nr_pcache_fill_from_victim: 2
```

6st run. Looks like the above fit_nowait can have 400ns improvement. But how come? I did not even change the pcache handling to use ibapi_nowait!!! Maybe random variation. Let me run more.
```c
Without fit_nowait patches
[  428.546738] Kernel Profile Points
[  428.550424]  status                  name             total                nr            avg.ns
[  428.560119] -------  --------------------  ----------------  ----------------  ----------------
[  428.569815]     off      flush_tlb_others       0.000131140                53              2475
[  428.579510]     off     __do_kmalloc_node       1.758704197           1331927              1321
[  428.589205]     off           pcache_miss       6.807601189            786575              8655
[  428.598899]     off          pcache_flush       3.699044847            262227             14107
[  428.608594] -------  --------------------  ----------------  ----------------  ----------------
[  428.618289]
[  428.718670] nr_pgfault: 786515
[  428.721878] nr_clflush: 262227
[  428.725272] nr_pgfault_wp: 0
[  428.728470] nr_pgfault_wp_cow: 0
[  428.732058] nr_pgfault_wp_reuse: 0
[  428.735840] nr_pgfault_due_to_concurrent_eviction: 0
[  428.741365] nr_pcache_fill_from_memory: 786515
[  428.746310] nr_pcache_fill_from_victim: 0
```

7th run. without fit_nowait.
```c
without fit_nowait.
[  901.223090] Kernel Profile Points
[  901.226775]  status                  name             total                nr            avg.ns
[  901.236472] -------  --------------------  ----------------  ----------------  ----------------
[  901.246168]     off      flush_tlb_others       0.000130802                53              2468
[  901.255865]     off     __do_kmalloc_node       1.862575608           1331923              1399
[  901.265560]     off           pcache_miss       6.814540477            786572              8664
[  901.275257]     off          pcache_flush       3.699187003            262224             14107
[  901.284953] -------  --------------------  ----------------  ----------------  ----------------
```

8th run. without fit_nowait.
```c
[  321.514564] Kernel Profile Points
[  321.518250]  status                  name             total                nr            avg.ns
[  321.527945] -------  --------------------  ----------------  ----------------  ----------------
[  321.537639]     off      flush_tlb_others       0.000130934                53              2471
[  321.547335]     off     __do_kmalloc_node       2.216772665           1331939              1665
[  321.557031]     off           pcache_miss       6.806060415            786573              8653
[  321.566726]     off          pcache_flush       3.725455841            262231             14207
[  321.576421] -------  --------------------  ----------------  ----------------  ----------------
```

9th run. with fit_nowait
```c
[  374.847912] Kernel Profile Points
[  374.851597]  status                  name             total                nr            avg.ns
[  374.861293] -------  --------------------  ----------------  ----------------  ----------------
[  374.870989]     off      flush_tlb_others       0.000130858                53              2470
[  374.880684]     off     __do_kmalloc_node       1.485304454           1331934              1116
[  374.890381]     off           pcache_miss       6.615317677            786582              8411
[  374.900076]     off          pcache_flush       3.508328900            262234             13379
[  374.909772] -------  --------------------  ----------------  ----------------  ----------------
```

10th run, with fit_nowait
```
[  225.211058] Kernel Profile Points
[  225.214743]  status                  name             total                nr            avg.ns
[  225.224440] -------  --------------------  ----------------  ----------------  ----------------
[  225.234137]     off      flush_tlb_others       0.000131029                53              2473
[  225.243833]     off     __do_kmalloc_node       1.211421872           1331984               910  
[  225.253529]     off           pcache_miss       6.583096125            786574              8370
[  225.263226]     off          pcache_flush       3.464430818            262227             13212
[  225.272922] -------  --------------------  ----------------  ----------------  ----------------
```

Sum:
```
with fit_nowait:

[  225.253529]     off           pcache_miss       6.583096125            786574              8370
[  225.263226]     off          pcache_flush       3.464430818            262227             13212

[  374.890381]     off           pcache_miss       6.615317677            786582              8411
[  374.900076]     off          pcache_flush       3.508328900            262234             13379

[  674.425062]     off           pcache_miss       6.467938547            786571              8223
[  674.434758]     off          pcache_flush       3.342783614            262225             12748

Without fit_nowait:
[  428.589205]     off           pcache_miss       6.807601189            786575              8655
[  428.598899]     off          pcache_flush       3.699044847            262227             14107

[  901.265560]     off           pcache_miss       6.814540477            786572              8664
[  901.275257]     off          pcache_flush       3.699187003            262224             14107

[  321.557031]     off           pcache_miss       6.806060415            786573              8653
[  321.566726]     off          pcache_flush       3.725455841            262231             14207
```


## 04/07 Sat
Well, now we finished all the profiling stuff. Continue on other work.

Now I like listening Jazz while coding. Amazing Jazz, really good.

Once again, ib_mad_completion_handler bug will happen. During application run, or even after application exit.
```c
[  465.835447] nr_mremap_pset_diff: 0
[  477.086886] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21
[  477.095620] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020
[  477.104345] IP: [<ffffffff81058277>] ib_mad_completion_handler+0xc7/0x810

ib_mad_completion_handler+0xc7/0x808:
ib_mad_recv_done_handler at drivers/infiniband/core/mad.c:1899
 (inlined by) ib_mad_completion_handler at drivers/infiniband/core/mad.c:2345
```

After remove net from pcache miss:
```c
[  465.572131] Kernel Profile Points
[  465.575815]  status                  name             total                nr            avg.ns
[  465.585510] -------  --------------------  ----------------  ----------------  ----------------
[  465.595206]     off      flush_tlb_others       0.000000000                 0                 0
[  465.604901]     off     __do_kmalloc_node       0.656371295           1762220               373
[  465.614597]     off           pcache_miss       7.172572671            786596              9119
[  465.624291]     off          pcache_flush       3.698294960            262251             14103
[  465.633987] -------  --------------------  ----------------  ----------------  ----------------
```

After remove net from pcache flush:
```c
[  684.984000] Kernel Profile Points
[  684.987683]  status                  name             total                nr            avg.ns
[  684.997379] -------  --------------------  ----------------  ----------------  ----------------
[  685.007074]     off      flush_tlb_others       0.000000000                 0                 0
[  685.016770]     off     __do_kmalloc_node       0.627372836           1500543               419
[  685.026464]     off           pcache_miss       7.128702028            786596              9063
[  685.036159]     off          pcache_flush       3.660772506            262251             13960
[  685.045855] -------  --------------------  ----------------  ----------------  ----------------

```

malloc, miss, flush are too slow. Especially the flush, how can it take 13.9us?

It must be our handlers! lego_copy_to_user stuff.

## 04/06 Fri

Well.
Now we have in-kernel strace, in-kernel readprofile. Yummy.

## 04/05 Thur

Discussion with Yilun.
1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure.
2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch.
3. I probably want to use per-cpu counter.

Anyway, continue strace work first. Finished.

## 04/04 Wed

### STRACE Performance

TF has very bad performance. It is either due to the syscall or pcache. Now I'm adding facilities to track syscall activities, including average latency, total time.

Basic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping.

But this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity.

Oh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime.

So the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it.

Yeh! Let us do this tomorrow. We will have a robust kernel version strace.

### SM Heartbeat
Continue run some experiments on yesterday's case.

One we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval:
```c
[ 1224.034898] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15
[ 1224.130616] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 15
[ 1224.222189] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 16
[ 1224.417181] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 16

[ 1393.159845] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 17
[ 1393.255546] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 17
[ 1393.347132] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 18
[ 1393.538972] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 18

[ 1449.437542] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 19
[ 1449.533248] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 19
[ 1449.624833] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 20
[ 1449.722512] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 20

[ 4322.423624] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21
[ 4322.519328] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 21
[ 4322.610914] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 22
[ 4322.708594] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 22
[ 4350.750574] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 23
[ 4350.846278] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 23
[ 4350.937863] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 24
[ 4351.035543] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 24

[ 4519.690559] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 25
[ 4519.786262] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 25
[ 4519.877848] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 26
[ 4519.975527] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 26

[ 4576.396279] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 27
[ 4576.491979] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 27
[ 4576.583565] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 28
[ 4576.681245] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 28

[ 4942.886820] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 29
[ 4942.982523] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 29
[ 4943.074108] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 30
[ 4943.171789] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 30
```

## 04/03 Tue

### BUG BUG BUG
Finished basic replication mechanism last night.

Today merged several patches. And both Yilun and I think there is something wrong with `ib_mad_completion_handler`. It seems it will break things behind our back.

This is one bug catched today:

#### ib_mad_completion_handler
```c
At very early stage:

[ 1174.406177] newpid: 20 home:1 replica: 1
[ 1174.452983] p2m_fork(cpu10): I cur:20-exe.o new:21
[ 1177.462795] ib_mad_completion_handler 2324 got successful recv cq op 128 mad_got_one 22
[ 1177.556502] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020
[ 1177.650101] IP: [<ffffffff81059104>] ib_mad_completion_handler+0xb4/0x8a0

./scripts/faddr2line vmImage  ib_mad_completion_handler+0xb4
ib_mad_completion_handler+0xb4/0x899:
ib_mad_recv_done_handler at drivers/infiniband/core/mad.c:1899
 (inlined by) ib_mad_completion_handler at drivers/infiniband/core/mad.c:2325

ib_mad_recv_done_handler():
1899: qp_info = mad_list->mad_queue->qp_info;

```

A more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4:
```c
[  863.887705] p2m_fork(cpu10): I cur:20-exe.o new:21
[  868.478424] p2m_fork(cpu10): O succeed cur:20-exe.o new:21
[  868.541991] BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
[  868.635569] IP: [<ffffffff810656d4>] __schedule+0x94/0x1e0
[  868.701090] PGD 0
[  868.725010] general protection fault: 0000 [#1] SMP PROCESSOR
[  868.793651] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #737

Source:
clear_tsk_need_resched(prev);
```

Even this one for Phoenix:
```c
[  763.442043] BUG: unable to handle kernel NULL pointer dereference at 0000000000000010
[  763.535636] IP: [<ffffffff81018d6f>] task_curr+0xf/0x30
[  763.598035] PGD 103e956067 PUD 103e964067 PMD 0
[  763.653154] Oops: 0000 [#1] SMP PROCESSOR
[  763.700992] CPU: 12 PID: 21 Comm: word_count-pthr 4.0.0-lego-ys+ #740
[  763.777950] RIP: 0010:[<ffffffff81018d6f>]  [<ffffffff81018d6f>] task_curr+0xf/0x30
```

This NEVER happen before. And this part of code should be correct. We've ran a
lot things.. I doubt if recent IB merge corrupt things.


#### fit_poll_cq
Another one:
```c
[  690.401626] stat: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_1GB.txt
[  690.507742] SYSC_close() CPU12 PID:21 [fd: 4] -> [/sys/devices/system/cpu/online]
[  713.899884] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 21
[  713.995606] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 21
[  714.087185] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 22
[  714.184871] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 22
[  742.078102] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 23
[  742.173810] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 23
[  742.265399] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 24
[  742.363085] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 24
[  847.063372] mlx4_ib_handle_error_cqe syndrome 21
[  847.116511] mlx4_ib_handle_error_cqe syndrome 5
[  847.170590] send request failed at connection 7 as 12
[  847.230909] mlx4_ib_handle_error_cqe syndrome 5
[  847.284988] mlx4_ib_handle_error_cqe syndrome 5
[  847.339067] mlx4_ib_handle_error_cqe syndrome 5
[  847.393146] fit_poll_cq: failed status (5) for wr_id 1832
[  847.457624] fit_poll_cq: failed status (5) for wr_id 1833
[  847.522103] fit_poll_cq: connection 7 Recv weird event as -1
[  847.589701] fit_poll_cq: failed status (5) for wr_id 1834
[  847.654179] fit_poll_cq: connection 7 Recv weird event as -30704
[  847.725938] fit_poll_cq: failed status (5) for wr_id 1835
[  847.790416] fit_poll_cq: connection 7 Recv weird event as -30704
[  847.862174] mlx4_ib_handle_error_cqe syndrome 5
[  847.916252] mlx4_ib_handle_error_cqe syndrome 5
[  847.970331] mlx4_ib_handle_error_cqe syndrome 5
[  848.024410] mlx4_ib_handle_error_cqe syndrome 5
[  848.078490] fit_poll_cq: failed status (5) for wr_id 1836
[  848.142967] fit_poll_cq: failed status (5) for wr_id 1837
[  848.207446] fit_poll_cq: connection 7 Recv weird event as -1
[  848.275044] fit_poll_cq: failed status (5) for wr_id 1838
[  848.339523] fit_poll_cq: connection 7 Recv weird event as -30704
[  848.411281] fit_poll_cq: failed status (5) for wr_id 1839
[  848.475760] fit_poll_cq: connection 7 Recv weird event as -30704
[  848.547517] mlx4_ib_handle_error_cqe syndrome 5
[  848.601596] mlx4_ib_handle_error_cqe syndrome 5
[  848.655675] mlx4_ib_handle_error_cqe syndrome 5
[  848.709753] mlx4_ib_handle_error_cqe syndrome 5
[  848.763832] fit_poll_cq: failed status (5) for wr_id 1840

[  848.828313] BUG: unable to handle kernel NULL pointer dereference at           (null)
[  848.921908] IP: [<ffffffff8106346d>] fit_poll_cq+0x4ad/0x510
[  848.989507] PGD 0
[  849.013426] Oops: 0002 [#1] SMP PROCESSOR
[  849.061265] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #744
[  849.131983] RIP: 0010:[<ffffffff8106346d>]  [<ffffffff8106346d>] fit_poll_cq+0x4ad/0x510
[  849.228700] RSP: 0000:ffff88103e813d88  EFLAGS: 00010246
[  849.292139] RAX: 0000000000001008 RBX: ffff88103effbad0 RCX: 0000000000000000
[  849.377418] RDX: 0000000000000000 RSI: ffffffff811d46e0 RDI: ffffffff811dbc08
[  849.462695] RBP: ffff88103e813ea8 R08: 0000000000000000 R09: 0000000000000000
[  849.547973] R10: 0000000000000002 R11: 0000000000000004 R12: 0000000000000000
[  849.633251] R13: ffff88103e801008 R14: 0000000000000004 R15: ffff88103e813da0
[  849.718529] FS:  0000000000000000(0000) GS:ffff88107fc40000(0000) knlGS:0000000000000000
[  849.815246] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  849.883884] CR2: 0000000000000000 CR3: 000000000113d000 CR4: 00000000000406a0
[  849.969163] Stack:
[  849.993082] ffffffff81003299 000001b03e813da0 0000000000000004 0000000000000730
[  850.080440] 0000008100000005 00001008000000f9 ffff88103eff8c50 002c222040000000
[  850.167798] 0010004000000002 ffff88107fc20000 0000000000000731 ffffffff00000005
[  850.255156] ffff8810000000f9 ffff88103eff8c50 0000000000000000 ffff88103e813e38
[  850.342513] ffffffff81019854 0000000000000732 ffff881000000005 ffff8810000000f9
[  850.429871] Call Trace:
[  850.458992] <TSK>
[  850.481870] [<ffffffff81003299>] ? native_smp_send_reschedule+0x39/0x50
[  850.560909] [<ffffffff81019854>] ? try_to_wake_up+0xe4/0x1f0
[  850.628506] [<ffffffff81065708>] ? __schedule+0xf8/0x1e0
[  850.691945] [<ffffffff810634d0>] ? fit_poll_cq+0x510/0x510
[  850.757464] [<ffffffff810634e4>] fit_poll_cq_pass+0x14/0x30
[  850.824021] [<ffffffff81020636>] kthread+0xf6/0x120
[  850.882260] [<ffffffff81020540>] ? __kthread_parkme+0x70/0x70
[  850.950898] [<ffffffff8100e572>] ret_from_fork+0x22/0x30

/* handle normal reply */
...
memcpy((void *)ctx->reply_ready_indicators[reply_indicator_index], &length, sizeof(int));
...
(This is a bad memcpy: reply_indicator_index, ctx, etc should be checked.)
```

### IB Spec: QP, CQE, WQE, SEND

The channel adapter detects the WQE posting and accesses the WQE.
The channel adapter interprets the command, validates the WQEâ€™s virtual 12
addresses, translates it to physical addresses, and accesses the data.
The outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source & destination GIDs). The channel adapter then adds the local route header and calculates both the variant
and invariant checksums.

For a Send operation, the QP retrieves the address of
the receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not
the last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing
the receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator.

When the originator receives an acknowledgment, it creates a CQE on the 5
CQ and retires the WQE from the send queue.

A QP can have multiple outstanding messages at any one time but the 8
target always acknowledges in the order sent, thus WQEs are retired in the order that they are posted.

## 04/02 Mon

Patching storage replica handler, able to finish today.

## 04/01 Sun

Anyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover.

I'm tired. :-( A month to go.

Record a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o:
```c
P
[30801.296160] ibapi_send_reply() CPU:8 PID:19 timeout (30010 ms), caller: clflush_one+0x1c9/0x370
[30938.564843] mlx4_ib_handle_error_cqe syndrome 21
[30938.617988] mlx4_ib_handle_error_cqe syndrome 5
[30938.672068] send request failed at connection 6 as 12
[30938.732389] mlx4_ib_handle_error_cqe syndrome 5
[30938.786470] mlx4_ib_handle_error_cqe syndrome 5
[30938.840551] mlx4_ib_handle_error_cqe syndrome 5
[30938.894632] fit_poll_cq: failed status (5) for wr_id 1584
[30938.959112] fit_poll_cq: failed status (5) for wr_id 1585
[30939.023593] fit_poll_cq: connection 6 Recv weird event as -1
[30939.091194] fit_poll_cq: failed status (5) for wr_id 1586
[30939.155676] fit_poll_cq: connection 6 Recv weird event as -30704
[30939.227436] fit_poll_cq: failed status (5) for wr_id 1587
[30939.291917] fit_poll_cq: connection 6 Recv weird event as -30704
[30939.363678] mlx4_ib_handle_error_cqe syndrome 5
[30939.417759] mlx4_ib_handle_error_cqe syndrome 5
[30939.471839] mlx4_ib_handle_error_cqe syndrome 5
[30939.525921] mlx4_ib_handle_error_cqe syndrome 5
[30939.580002] fit_poll_cq: failed status (5) for wr_id 1588
[30939.644483] BUG: unable to handle kernel NULL pointer dereference at           (null)
[30939.738083] IP: [<ffffffff81062fcd>] fit_poll_cq+0x4ad/0x510
[30939.805684] PGD 0
[30939.829604] Oops: 0002 [#1] SMP PROCESSOR
[30939.877445] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #715
[30939.948166] RIP: 0010:[<ffffffff81062fcd>]  [<ffffffff81062fcd>] fit_poll_cq+0x4ad/0x510

fit_poll_cq at net/lego/fit_internal.c:1734
memcpy((void *)ctx->reply_ready_indicators[reply_indicator_index], &length, sizeof(int));

M
[30913.642698] mlx4_ib_handle_error_cqe syndrome 21
[30913.695839] mlx4_ib_handle_error_cqe syndrome 5
[30913.749919] send request failed at connection 1 as 12
[30913.810236] mlx4_ib_handle_error_cqe syndrome 5
[30913.864315] mlx4_ib_handle_error_cqe syndrome 5
[30913.918395] mlx4_ib_handle_error_cqe syndrome 5
[30913.972474] fit_poll_cq: failed status (5) for wr_id 305
[30914.035912] fit_poll_cq: failed status (5) for wr_id 306
```
