{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello! I\u2019m Yizhou Shan, a Ph.D. student at University of California San Diego, CSE , advised by Prof. Yiying Zhang . I am a member of UCSD Wuklab and UCSD SysNet . I was a Purdue Boilermaker during 2016-2019. You can find my CV here . Contact: ys AT ucsd DOT edu Research News [ Oct 2020 ] Serve as EuroSys\u201820 Shadow PC [ Sep 2020 ] Serve as OSDI\u201820 Artifact Evaluation PC [ Sep 2020 ] Serve as ASPLOS\u201821 External Reviewer. First major conference review! [ Apr 2020 ] Disaggregated Persistent Memory accepted to ATC\u201820 [ Sep 2019 ] Moved to UCSD. [ May 2019 ] Intern at VMware Research , with Marcos K. Aguilera [ Apr 2019 ] Storm accpeted to SYSTOR\u201819 . Awarded Best Paper. [ Jan 2019 ] Short paper on Disaggregated Persistent Memory accpeted to NVMW\u201819 [ Jul 2018 ] LegoOS accepted to OSDI\u201818 . Awarded Best Paper. [ May 2018 ] Intern at VMware Research , with Stanko Novakovic . Research \u00b6 During my doctoral research, I have been working on distributed system, networking, operating system, hardware (FPGA), and their intersections. My most recent research focus is on disaggregated datacenter, designing its hardware, OS, and upper layer applications. All of my projects are open source at https://github.com/lastweek and https://github.com/WukLab . As of now, I\u2019m working on network design for disaggregated datacenters. End of the day, many of the system problems boil down to network problems. :) Conferences \u00b6 Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated Key-Value Stores Shin-Yeh Tsai, Yizhou Shan , Yiying Zhang USENIX ATC 2020 [Paper] [Code] [Slide] [Short-Talk] [Full-Talk] [Keynote] Storm: a fast transactional dataplane for remote data structures Stanko Novakovic, Yizhou Shan , Aasheesh Kolli, Michael Cui, Yiying Zhang, Haggai Eran, Liran Liss, Michael Wei, Dan Tsafrir, Marcos Aguilera ACM SYSTOR 2019 (Best Paper Award) [Paper] [Slide] [Talk] LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation Yizhou Shan , Yutong Huang, Yilun Chen, Yiying Zhang USENIX OSDI 2018 (Best Paper Award) [Paper] [Code] [Slide] [Keynote-iCloud] [Talk] Distributed Shared Persistent Memory Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang ACM SoCC 2017 [Paper] [Code] [Slide] [Poster] Workshops \u00b6 Challenges in Building and Deploying Disaggregated Persistent Memory Yizhou Shan , Yutong Huang, Yiying Zhang 10 th Annual Non-Volatile Memories Workshop ( NVMW 2019 ) [Paper] Disaggregating Memory with Software-Managed Virtual Cache Yizhou Shan , Yiying Zhang 2018 Workshop on Warehouse-scale Memory Systems ( WAMS 2018 ) (co-located with ASPLOS \u201818) [Paper] Distributed Shared Persistent Memory Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang 9 th Annual Non-Volatile Memories Workshop ( NVMW 2018 ) [Paper] Disaggregated Operating System Yiying Zhang, Yizhou Shan , Sumukh Hallymysore 17 th International Workshop on High Performance Transaction Systems ( HPTS 2017 ) [Paper] Posters \u00b6 Lego: A Distributed, Decomposed OS for Resource Disaggregation PDF Yizhou Shan , Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang Poster at SOSP 2017 Disaggregated Operating System PDF Yizhou Shan , Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang Poster at SoCC 2017 Social \u00b6 Google Scholar Github Twitter LinkedIn","title":"Home"},{"location":"#research","text":"During my doctoral research, I have been working on distributed system, networking, operating system, hardware (FPGA), and their intersections. My most recent research focus is on disaggregated datacenter, designing its hardware, OS, and upper layer applications. All of my projects are open source at https://github.com/lastweek and https://github.com/WukLab . As of now, I\u2019m working on network design for disaggregated datacenters. End of the day, many of the system problems boil down to network problems. :)","title":"Research"},{"location":"#conferences","text":"Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated Key-Value Stores Shin-Yeh Tsai, Yizhou Shan , Yiying Zhang USENIX ATC 2020 [Paper] [Code] [Slide] [Short-Talk] [Full-Talk] [Keynote] Storm: a fast transactional dataplane for remote data structures Stanko Novakovic, Yizhou Shan , Aasheesh Kolli, Michael Cui, Yiying Zhang, Haggai Eran, Liran Liss, Michael Wei, Dan Tsafrir, Marcos Aguilera ACM SYSTOR 2019 (Best Paper Award) [Paper] [Slide] [Talk] LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation Yizhou Shan , Yutong Huang, Yilun Chen, Yiying Zhang USENIX OSDI 2018 (Best Paper Award) [Paper] [Code] [Slide] [Keynote-iCloud] [Talk] Distributed Shared Persistent Memory Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang ACM SoCC 2017 [Paper] [Code] [Slide] [Poster]","title":"Conferences"},{"location":"#workshops","text":"Challenges in Building and Deploying Disaggregated Persistent Memory Yizhou Shan , Yutong Huang, Yiying Zhang 10 th Annual Non-Volatile Memories Workshop ( NVMW 2019 ) [Paper] Disaggregating Memory with Software-Managed Virtual Cache Yizhou Shan , Yiying Zhang 2018 Workshop on Warehouse-scale Memory Systems ( WAMS 2018 ) (co-located with ASPLOS \u201818) [Paper] Distributed Shared Persistent Memory Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang 9 th Annual Non-Volatile Memories Workshop ( NVMW 2018 ) [Paper] Disaggregated Operating System Yiying Zhang, Yizhou Shan , Sumukh Hallymysore 17 th International Workshop on High Performance Transaction Systems ( HPTS 2017 ) [Paper]","title":"Workshops"},{"location":"#posters","text":"Lego: A Distributed, Decomposed OS for Resource Disaggregation PDF Yizhou Shan , Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang Poster at SOSP 2017 Disaggregated Operating System PDF Yizhou Shan , Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang Poster at SoCC 2017","title":"Posters"},{"location":"#social","text":"Google Scholar Github Twitter LinkedIn","title":"Social"},{"location":"vmware-intern/","text":"Intel Xeon 6138p, integrated FPGA (check it out!) retpoline (perf impact?) Intel Total Memory Encryption. Multi-Key Total Memory Encryption (MKTME). RDMA + NVM: An interesting topic. There are a lot interesting stuff to think about. I discussed this with Sanidhya today, he shared some very valuable findings: RDMA write: when does it mark a persistent point? RDMA write followed by a RDMA read, is kind of implicit memory barrier imposed by memory controller.","title":"Vmware intern"},{"location":"blog/20200404-on-net-transport/","text":"On Hardware-based Network Transport Design and Implementation \u00b6 TODO","title":"2020-04 On-Hardware-based-Network-Transport"},{"location":"blog/20200404-on-net-transport/#on-hardware-based-network-transport-design-and-implementation","text":"TODO","title":"On Hardware-based Network Transport Design and Implementation"},{"location":"blog/20200404-on-read-once/","text":"On READ_ONCE and Compiler Opts \u00b6 Version History Date Description Apr 13, 2020 Initial Version I decide to write this blog after I once again got tricked by GCC optimizations. I was designing a simple single-producer-single-consumer ring buffer. Since there is a small time gap between slot-being-allocated and slot-being-usable (i.e., data filled), the producer will set a non-atomic flag once the data is filled thus usable. The consumer, running on a seperate CPU, will repeatly checking the usable flag after it has grabbed the slot. Simple, right? Yet I ran into a lot random stuck during testing. I didn\u2019t even check the ring buffer design as I was so confident. There was no timeout checking either. After some digging, I realized I missed using READ_ONCE when consumer thread is polling for the usable flag. Yeah, once again, gcc -O2 tricked me: it will optmize away repeated memory accesses if it thinks the accessed variable/data is thread-local. For instance, the following code snippet shows how gcc -O2 removes the memory access part. Without -O2, a simple assembly loop is generated. With -O2, gcc generates a deadlock itself. Original C Assembly Assembly ( gcc - S ) ( gcc - S - O2 ) int x ; | | | . L2 : | . L2 : /* Spin until x becomes true */ | movl x ( % rip ), % eax | jmp . L2 void wait_for_x ( void ) | cmpl $ 1 , % eax | { | je . L2 | while ( x == 1 ) | | ; | | } | | Why this is happening? Because gcc thinks vairable x is thread-local and will not be accessed by multiple threads at the same time. Thus gcc thinks the above while (x == 1) ; check will never break, so generating an assembly deadlock jmp loop. Why does this matter? Assume x is a shared variable. In the following code snippet, there are two threads, A and B. Thread A wait until B change x to 1. If we compile with -O2, thread A will deadlock. And this was my bug above. int x ; /* a global shared variable*/ Thread A Thread B /* Spin until x becomes true */ | /* Set x at some point */ void wait_for_x ( void ) | x = 1 ; { | while ( x == 1 ) | ; | } | The common approach, is to add volatile modifier, to explicitly express the concurrency issue. But volatile is considered harmful by linux kernel, and I agree with it. I generally use READ_ONCE , WRITE_ONCE , ACCESS_ONCE macros. They \u201ctell\u201d gcc that the particualr variable is a shared global variable, thus for each time a C statment is running, the variable should be accessed once and exactly once. The fix for above case is: while (READ_ONCE(x == 1)) ; . I will not go into details about why and how those macros are implemented. For more information, refers to source code , ktsan wiki . Hope you enjoyed this simple bug-documentation blog.","title":"2020-04 On-Read-Once-and-Compiler-Opts"},{"location":"blog/20200404-on-read-once/#on-read_once-and-compiler-opts","text":"Version History Date Description Apr 13, 2020 Initial Version I decide to write this blog after I once again got tricked by GCC optimizations. I was designing a simple single-producer-single-consumer ring buffer. Since there is a small time gap between slot-being-allocated and slot-being-usable (i.e., data filled), the producer will set a non-atomic flag once the data is filled thus usable. The consumer, running on a seperate CPU, will repeatly checking the usable flag after it has grabbed the slot. Simple, right? Yet I ran into a lot random stuck during testing. I didn\u2019t even check the ring buffer design as I was so confident. There was no timeout checking either. After some digging, I realized I missed using READ_ONCE when consumer thread is polling for the usable flag. Yeah, once again, gcc -O2 tricked me: it will optmize away repeated memory accesses if it thinks the accessed variable/data is thread-local. For instance, the following code snippet shows how gcc -O2 removes the memory access part. Without -O2, a simple assembly loop is generated. With -O2, gcc generates a deadlock itself. Original C Assembly Assembly ( gcc - S ) ( gcc - S - O2 ) int x ; | | | . L2 : | . L2 : /* Spin until x becomes true */ | movl x ( % rip ), % eax | jmp . L2 void wait_for_x ( void ) | cmpl $ 1 , % eax | { | je . L2 | while ( x == 1 ) | | ; | | } | | Why this is happening? Because gcc thinks vairable x is thread-local and will not be accessed by multiple threads at the same time. Thus gcc thinks the above while (x == 1) ; check will never break, so generating an assembly deadlock jmp loop. Why does this matter? Assume x is a shared variable. In the following code snippet, there are two threads, A and B. Thread A wait until B change x to 1. If we compile with -O2, thread A will deadlock. And this was my bug above. int x ; /* a global shared variable*/ Thread A Thread B /* Spin until x becomes true */ | /* Set x at some point */ void wait_for_x ( void ) | x = 1 ; { | while ( x == 1 ) | ; | } | The common approach, is to add volatile modifier, to explicitly express the concurrency issue. But volatile is considered harmful by linux kernel, and I agree with it. I generally use READ_ONCE , WRITE_ONCE , ACCESS_ONCE macros. They \u201ctell\u201d gcc that the particualr variable is a shared global variable, thus for each time a C statment is running, the variable should be accessed once and exactly once. The fix for above case is: while (READ_ONCE(x == 1)) ; . I will not go into details about why and how those macros are implemented. For more information, refers to source code , ktsan wiki . Hope you enjoyed this simple bug-documentation blog.","title":"On READ_ONCE and Compiler Opts"},{"location":"ctf/basic/","text":"checksec rename in assembly pwn cyclic De Brujin Sequence ROP Shell code \u00b6 Avoid NULL byte (\\x00) is bad. No hard coded addresses: use indirect references, e.g., short jumps and near calls. First portion of payload can be a bunch of NOPs, IP will slide into the real shellcode http://shell-storm.org/ encrypt shellcode \u00b6","title":"Basic"},{"location":"ctf/basic/#shell-code","text":"Avoid NULL byte (\\x00) is bad. No hard coded addresses: use indirect references, e.g., short jumps and near calls. First portion of payload can be a bunch of NOPs, IP will slide into the real shellcode http://shell-storm.org/","title":"Shell code"},{"location":"ctf/basic/#encrypt-shellcode","text":"","title":"encrypt shellcode"},{"location":"fpga/bitstream/","text":"FPGA Bitstream Explained \u00b6 Version History Date Description Sep 18, 2020 add github link and usenix paper Dec 20, 2019 Update Oct 24, 2019 Created The proof-of-concept code to decode Xilinx bitstream is here: https://github.com/lastweek/fpga_decode_bitstream . USENIX Security 2020 has a paper on decrypting Xilinx bitstream: https://www.usenix.org/conference/usenixsecurity20/presentation/ender . They find a vulnerability in the 7-series chip and in turn able to decrypt a fully encrypted bitstream. WHAT A HACK! Introduction \u00b6 A bitstream can configure an FPGA. A bitstream includes the descriptions of the hardware logic, routing, and initial values of registers and on-chip memory. The common impression is that a bitstream has vendor-specific format and cannot be reversed. The answer is yes and no. The fact is that the bitstream file is more than the bits to configure an FPGA, it also has certain human-readable fields to describe the contents. In addition, it has a simple assembly-like instruction set to describe the FPGA configuration process. This note is trying to walk through this. At a high-level, a bitstream file is similar to an executable program, in the sense that it describes everything of a desired functionality. Analogous to the ELF format, a bistream has its own format to describe the contents. Note that, this file format is publicly documented 1 . That means you can analyze the contents of a bitstream file. The undocument part is the configuration bits: FPGA vendor does not document the format of the configuration bits, and how they may to hardware resources. As a normal FPGA user, you mostly do not need to understand neither of these. These understandings are only required if you plan to do bitstream readback, preemption scheduling and so forth. After reading this note, you will understand that a bitstream file is a sequence of instructions and data. The FPGA has a simple state machine to parse the bitstream and then configure the chip. Part of the bistream file format is public, the mapping between the bitstream configuration bits and the actual physical resource is undocumented. Bistream Related Files \u00b6 In a normal flow, Vivado only generates a simple .bit file. When you click \u201cProgram Device\u201d, Vivado will use this file to configure your FPGA. In addition to generating this file, Vivado is capable of generating a bunch other files. You can find a complete coverage in this link . We give a high level summary here. Most of the files have the same content and have similar file size. For instance, the difference between a .rbt and a .bit is that the former one is in ASCII format while the latter is in binary format, but they have the same contents. As for a .bit and a .bin file, the latter does not have some ASCII headers at the beginning of the file. .ll , the logical link file, is very interesting. It tells you the mapping between user logic and the actual bit offset in the bistream file data section. This file can be used to aid preemption scheduling. However, note that, this file only documents a very small part of the mapping. To the best of my knowledge, I think only the registers, on-chip memory are documented, but the routing information is missing. Thus, this file can help reserve engineer bitstream data section to some extend, but not full of it. Prjxray is an open source project working on cracking everything on 7-series FPGA. Details \u00b6 We use .rbt and .bit to demonstrate the file format. Note that they are essentially the same thing, except the former in human-readable ASCII format. The target board is VCU118, the one used by many cloud vendors. The following snippt is the first few lines of the .rpt file. The first few lines are human-readable ASCII contents describing some general information about the bitstream. Starting from line 8 is the actual bitstream file contents. Note that the .bin file starts directly from line 8, no general header info is attached. The interesting part is the 1s and 0s. Unless otherwise noted, when we refer to bitstream format, we focus on the 1s and 0s only and omit any general ASICC information headers. Xilinx ASCII Bitstream Created by Bitstream 2018.3 SW Build 2405991 on Thu Dec 6 23:36:41 MST 2018 Design name: base_mb_wrapper;UserID=0XFFFFFFFF;Version=2018.3 Architecture: virtexuplus Part: xcvu9p-flga2104-2L-e Date: Wed Nov 20 04:13:05 2019 Bits: 641272864 11111111111111111111111111111111 11111111111111111111111111111111 11111111111111111111111111111111 ... Note that each line has 32 bits, thus 4 bytes. In Xilinx bistream format, each four bytes is a packet (analogous to CPU instruction). Each packet has certain format, it could be a special header packet , or a normal data packet . The header packet follows a simple assembly-like instruction set to dictate the configuration process. The bitstream file is a sequence of these four bytes packets. Why it sounds so complicated, a sequence of instructions?! I think the short answer is that configuraing FPGA is not an easy task, and any wrong doings may permanently harm the chip. Natually, the designer would have a on-chip state machine to control the configuration process, not only to control the whole process but also to ensure safety. Each Xilinx FPGA has an on-chip configuration packet processor . All configuration methods such as JTAG, SelectMAP, ICAP merge into this final narrow bridge to carry out the configuration. The configuration packet processor has many internal registers (similar to x86 RAX, CRn, MSR registers). The bitstream usually interact with one of the registers at a time to do one thing. For a more detailed explanation, check out this blog , and UG570 chapter 9. To this end, a bitstream consits of three parts: 1) Header packets to prepare the configuration process. 2) The actual configuration bits in a contiguous sequence of data packets. AN write to the FDRI register marks the beginning of this section. The length of this section is described by the packet following the FDRI header packet. 3) Header packets to clean up the configuration process. The actual configuration bits are the ones determine the FPGA functionality. Note that if you are using an SSI Xilinx device like VCU118, the bitstream format is a bit more complicated. Basically, each die has the above three parts. If an chip has N dies, it will have N above triplet. I have complained about this is not well documented here and here . I wrote a simple C program to parse the .rbt file and associate a human-reable syntax with each line. I didn\u2019t have a complete coverage of the header packet format. The following snippt shows a parsed .rbt file with header removed. Here, 0xffffffff has no effect, like a NOP. 0x000000bb and 0x11220044 are special bus detect words. 0xaa995566 is another special work marking the synchronization status. The last few lines mark the beginning of the configuration bits section. Parsed from base_mb_wrapper.rbt ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff 000000bb Bus Width Sync 11220044 Bus Width Detect ffffffff ffffffff aa995566 SYNC 20000000 20000000 30022001 Write to regs 17 00000000 30020001 Write to regs 16 00000000 30008001 Write to CMD 00000000 20000000 30008001 Write to CMD 00000007 20000000 20000000 30002001 Write to FAR 00000000 30026001 Write to regs 19 00000000 30012001 Write to regs 9 38003fe5 Write to regs 1 3001c001 Write to regs 14 00400000 30018001 Write to IDCODE 04b31093 IDCODE=4b31093 30008001 Write to CMD 00000009 20000000 3000c001 Write to regs 6 00000001 3000a001 Write to regs 5 00000101 3000c001 Write to regs 6 00000000 30030001 Write to regs 24 00000000 20000000 20000000 20000000 20000000 20000000 20000000 20000000 20000000 30002001 Write to FAR 00000000 30008001 Write to CMD 00000001 20000000 30004000 Write to FDRI 5065eadc <- The length of configuration bits, follows a certain format 00000000 <- The first 4 bytes of the configuration bits! Hope you have learned something. References \u00b6 Xilinx UG570 Xilinx bitstream files Another blog on Xilinx Bitstream Internals Source code to annotate bitstream","title":"Bitstream Explained"},{"location":"fpga/bitstream/#fpga-bitstream-explained","text":"Version History Date Description Sep 18, 2020 add github link and usenix paper Dec 20, 2019 Update Oct 24, 2019 Created The proof-of-concept code to decode Xilinx bitstream is here: https://github.com/lastweek/fpga_decode_bitstream . USENIX Security 2020 has a paper on decrypting Xilinx bitstream: https://www.usenix.org/conference/usenixsecurity20/presentation/ender . They find a vulnerability in the 7-series chip and in turn able to decrypt a fully encrypted bitstream. WHAT A HACK!","title":"FPGA Bitstream Explained"},{"location":"fpga/bitstream/#introduction","text":"A bitstream can configure an FPGA. A bitstream includes the descriptions of the hardware logic, routing, and initial values of registers and on-chip memory. The common impression is that a bitstream has vendor-specific format and cannot be reversed. The answer is yes and no. The fact is that the bitstream file is more than the bits to configure an FPGA, it also has certain human-readable fields to describe the contents. In addition, it has a simple assembly-like instruction set to describe the FPGA configuration process. This note is trying to walk through this. At a high-level, a bitstream file is similar to an executable program, in the sense that it describes everything of a desired functionality. Analogous to the ELF format, a bistream has its own format to describe the contents. Note that, this file format is publicly documented 1 . That means you can analyze the contents of a bitstream file. The undocument part is the configuration bits: FPGA vendor does not document the format of the configuration bits, and how they may to hardware resources. As a normal FPGA user, you mostly do not need to understand neither of these. These understandings are only required if you plan to do bitstream readback, preemption scheduling and so forth. After reading this note, you will understand that a bitstream file is a sequence of instructions and data. The FPGA has a simple state machine to parse the bitstream and then configure the chip. Part of the bistream file format is public, the mapping between the bitstream configuration bits and the actual physical resource is undocumented.","title":"Introduction"},{"location":"fpga/bitstream/#bistream-related-files","text":"In a normal flow, Vivado only generates a simple .bit file. When you click \u201cProgram Device\u201d, Vivado will use this file to configure your FPGA. In addition to generating this file, Vivado is capable of generating a bunch other files. You can find a complete coverage in this link . We give a high level summary here. Most of the files have the same content and have similar file size. For instance, the difference between a .rbt and a .bit is that the former one is in ASCII format while the latter is in binary format, but they have the same contents. As for a .bit and a .bin file, the latter does not have some ASCII headers at the beginning of the file. .ll , the logical link file, is very interesting. It tells you the mapping between user logic and the actual bit offset in the bistream file data section. This file can be used to aid preemption scheduling. However, note that, this file only documents a very small part of the mapping. To the best of my knowledge, I think only the registers, on-chip memory are documented, but the routing information is missing. Thus, this file can help reserve engineer bitstream data section to some extend, but not full of it. Prjxray is an open source project working on cracking everything on 7-series FPGA.","title":"Bistream Related Files"},{"location":"fpga/bitstream/#details","text":"We use .rbt and .bit to demonstrate the file format. Note that they are essentially the same thing, except the former in human-readable ASCII format. The target board is VCU118, the one used by many cloud vendors. The following snippt is the first few lines of the .rpt file. The first few lines are human-readable ASCII contents describing some general information about the bitstream. Starting from line 8 is the actual bitstream file contents. Note that the .bin file starts directly from line 8, no general header info is attached. The interesting part is the 1s and 0s. Unless otherwise noted, when we refer to bitstream format, we focus on the 1s and 0s only and omit any general ASICC information headers. Xilinx ASCII Bitstream Created by Bitstream 2018.3 SW Build 2405991 on Thu Dec 6 23:36:41 MST 2018 Design name: base_mb_wrapper;UserID=0XFFFFFFFF;Version=2018.3 Architecture: virtexuplus Part: xcvu9p-flga2104-2L-e Date: Wed Nov 20 04:13:05 2019 Bits: 641272864 11111111111111111111111111111111 11111111111111111111111111111111 11111111111111111111111111111111 ... Note that each line has 32 bits, thus 4 bytes. In Xilinx bistream format, each four bytes is a packet (analogous to CPU instruction). Each packet has certain format, it could be a special header packet , or a normal data packet . The header packet follows a simple assembly-like instruction set to dictate the configuration process. The bitstream file is a sequence of these four bytes packets. Why it sounds so complicated, a sequence of instructions?! I think the short answer is that configuraing FPGA is not an easy task, and any wrong doings may permanently harm the chip. Natually, the designer would have a on-chip state machine to control the configuration process, not only to control the whole process but also to ensure safety. Each Xilinx FPGA has an on-chip configuration packet processor . All configuration methods such as JTAG, SelectMAP, ICAP merge into this final narrow bridge to carry out the configuration. The configuration packet processor has many internal registers (similar to x86 RAX, CRn, MSR registers). The bitstream usually interact with one of the registers at a time to do one thing. For a more detailed explanation, check out this blog , and UG570 chapter 9. To this end, a bitstream consits of three parts: 1) Header packets to prepare the configuration process. 2) The actual configuration bits in a contiguous sequence of data packets. AN write to the FDRI register marks the beginning of this section. The length of this section is described by the packet following the FDRI header packet. 3) Header packets to clean up the configuration process. The actual configuration bits are the ones determine the FPGA functionality. Note that if you are using an SSI Xilinx device like VCU118, the bitstream format is a bit more complicated. Basically, each die has the above three parts. If an chip has N dies, it will have N above triplet. I have complained about this is not well documented here and here . I wrote a simple C program to parse the .rbt file and associate a human-reable syntax with each line. I didn\u2019t have a complete coverage of the header packet format. The following snippt shows a parsed .rbt file with header removed. Here, 0xffffffff has no effect, like a NOP. 0x000000bb and 0x11220044 are special bus detect words. 0xaa995566 is another special work marking the synchronization status. The last few lines mark the beginning of the configuration bits section. Parsed from base_mb_wrapper.rbt ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff ffffffff 000000bb Bus Width Sync 11220044 Bus Width Detect ffffffff ffffffff aa995566 SYNC 20000000 20000000 30022001 Write to regs 17 00000000 30020001 Write to regs 16 00000000 30008001 Write to CMD 00000000 20000000 30008001 Write to CMD 00000007 20000000 20000000 30002001 Write to FAR 00000000 30026001 Write to regs 19 00000000 30012001 Write to regs 9 38003fe5 Write to regs 1 3001c001 Write to regs 14 00400000 30018001 Write to IDCODE 04b31093 IDCODE=4b31093 30008001 Write to CMD 00000009 20000000 3000c001 Write to regs 6 00000001 3000a001 Write to regs 5 00000101 3000c001 Write to regs 6 00000000 30030001 Write to regs 24 00000000 20000000 20000000 20000000 20000000 20000000 20000000 20000000 20000000 30002001 Write to FAR 00000000 30008001 Write to CMD 00000001 20000000 30004000 Write to FDRI 5065eadc <- The length of configuration bits, follows a certain format 00000000 <- The first 4 bytes of the configuration bits! Hope you have learned something.","title":"Details"},{"location":"fpga/bitstream/#references","text":"Xilinx UG570 Xilinx bitstream files Another blog on Xilinx Bitstream Internals Source code to annotate bitstream","title":"References"},{"location":"fpga/hls_axi/","text":"High-performance AXI-MM in HLS \u00b6 My personal experience: the native AXI-MM in HLS is horrible. It fails to generate efficient code. The best practice I found is the use an external Datamover. In HLS, all memory access is made via AXI-Stream. Using AXI-Stream means we can wait the result asynchronously, hence we can deal with long memory access in a more informed manner. Usually using AXI-Stream and Datamover delivers code with II=1.","title":"Efficient AXI-MM"},{"location":"fpga/hls_axi/#high-performance-axi-mm-in-hls","text":"My personal experience: the native AXI-MM in HLS is horrible. It fails to generate efficient code. The best practice I found is the use an external Datamover. In HLS, all memory access is made via AXI-Stream. Using AXI-Stream means we can wait the result asynchronously, hence we can deal with long memory access in a more informed manner. Usually using AXI-Stream and Datamover delivers code with II=1.","title":"High-performance AXI-MM in HLS"},{"location":"fpga/hls_axis/","text":"AXI-Stream Usage in HLS \u00b6 How you should ultilize the AXI-Stream in HLS code to best describe your system.","title":"AXI-Stream Usage in HLS"},{"location":"fpga/hls_axis/#axi-stream-usage-in-hls","text":"How you should ultilize the AXI-Stream in HLS code to best describe your system.","title":"AXI-Stream Usage in HLS"},{"location":"fpga/language/","text":"On-High-Level-Languages-For-FPGA-Design \u00b6 Version History Date Description May 31, 2020 Initial With FPGA getting popular among system folks, it\u2019s crucial to pick up the right language for the project. Most folks will not use Verilog/VHDL directly, but use higher level languages like Xilinx HLS, Chisel, SpinalHDL etc. All my dicussions and opinions are based on my own limited experience with FPGA (since Oct 2018), it does not reflect any others\u2019 opinions. In short: for folks new to FPGA and want to start a medium- or large- sized network-oriented academic projects, I would recommend avoid using Xilinx HLS, but use SpinalHDL/Chisel or others instead. Of course, you still need to know a bit bout Verilog/VHDL and all the tools (e.g., Vivado) for the final project packaging. I started using HLS from 2018 Oct. I\u2019ve writtin more than 20K HLS code, including but not limited to RDMA-like modules, partial-reconfiguration ICAP3 controller. I pick it because it is C-like and expressive when first using it. However, along the way, me and my labmates have had a lot issues with HLS, some due to compiler, some are still non-explainable. My own opinions about HLS. The good part: HLS is easy to pick up and write. Its semantic is similar to C. Good for prototying small project. HLS has several useful AXI-Stream interfaces. HLS has many options allowing us control FPGA resource usage. The bad part: HLS is not designed around streaming interface, which is a crutial part for network oriented projects. It\u2019s dataflow primitive is very restrictive, hard to construct a system with clear flow. Compiler. Some code pattern generate undefined behaviours, even though totally correct in turns of logic. Ugh, we have had so much trouble for this part, and this is the most annoying part. Hard to control BRAM access, i.e., avoid false-dependency and track consistency. Hard to express bits related ops. HLS has range operators, but really hard to write, a lot macros flying around. Streaming interface is a bit fragile, we found a lot random stucks during runtime due to buffer issue. For code to be really useful, you have to write in a switch-case state machine way. There is no difference with a verilog one, but with more complexity, especially for large-scale projects. Simulation framework is not easy to use, a lot restritions too. We had a lot trouble with HLS. Not until recently, one of my labmate picked up SpinalHDL, and we found it amazing. I\u2019m not personally writing SpinalHDL code, but I felt it is super expressive and match hardware primitive, physically and mentally. Personally, I would use scala-based ones over HLS for my future projects.","title":"Language"},{"location":"fpga/language/#on-high-level-languages-for-fpga-design","text":"Version History Date Description May 31, 2020 Initial With FPGA getting popular among system folks, it\u2019s crucial to pick up the right language for the project. Most folks will not use Verilog/VHDL directly, but use higher level languages like Xilinx HLS, Chisel, SpinalHDL etc. All my dicussions and opinions are based on my own limited experience with FPGA (since Oct 2018), it does not reflect any others\u2019 opinions. In short: for folks new to FPGA and want to start a medium- or large- sized network-oriented academic projects, I would recommend avoid using Xilinx HLS, but use SpinalHDL/Chisel or others instead. Of course, you still need to know a bit bout Verilog/VHDL and all the tools (e.g., Vivado) for the final project packaging. I started using HLS from 2018 Oct. I\u2019ve writtin more than 20K HLS code, including but not limited to RDMA-like modules, partial-reconfiguration ICAP3 controller. I pick it because it is C-like and expressive when first using it. However, along the way, me and my labmates have had a lot issues with HLS, some due to compiler, some are still non-explainable. My own opinions about HLS. The good part: HLS is easy to pick up and write. Its semantic is similar to C. Good for prototying small project. HLS has several useful AXI-Stream interfaces. HLS has many options allowing us control FPGA resource usage. The bad part: HLS is not designed around streaming interface, which is a crutial part for network oriented projects. It\u2019s dataflow primitive is very restrictive, hard to construct a system with clear flow. Compiler. Some code pattern generate undefined behaviours, even though totally correct in turns of logic. Ugh, we have had so much trouble for this part, and this is the most annoying part. Hard to control BRAM access, i.e., avoid false-dependency and track consistency. Hard to express bits related ops. HLS has range operators, but really hard to write, a lot macros flying around. Streaming interface is a bit fragile, we found a lot random stucks during runtime due to buffer issue. For code to be really useful, you have to write in a switch-case state machine way. There is no difference with a verilog one, but with more complexity, especially for large-scale projects. Simulation framework is not easy to use, a lot restritions too. We had a lot trouble with HLS. Not until recently, one of my labmate picked up SpinalHDL, and we found it amazing. I\u2019m not personally writing SpinalHDL code, but I felt it is super expressive and match hardware primitive, physically and mentally. Personally, I would use scala-based ones over HLS for my future projects.","title":"On-High-Level-Languages-For-FPGA-Design"},{"location":"fpga/misc/","text":"Misc \u00b6 If we want to do relocation, we need to be careful: - identical areas in terms of shape, resource distribution within - proxy logic (i.e., partition pins) location within the PR partition - the wire between proxy logic and static region. - I think this might cause timing issue? The lock_design in Vivado is to ensure the routing between static region and all the PR partitions remain the same. Proxy Logic and Bus Macro - S1: Relocation of reconfigurable modules on Xilinx FPGA - S2: A Highly Flexible Reconfigurable System on a Xilinx FPGA Expansion of CONTAIN_ROUTING Area","title":"Misc"},{"location":"fpga/misc/#misc","text":"If we want to do relocation, we need to be careful: - identical areas in terms of shape, resource distribution within - proxy logic (i.e., partition pins) location within the PR partition - the wire between proxy logic and static region. - I think this might cause timing issue? The lock_design in Vivado is to ensure the routing between static region and all the PR partitions remain the same. Proxy Logic and Bus Macro - S1: Relocation of reconfigurable modules on Xilinx FPGA - S2: A Highly Flexible Reconfigurable System on a Xilinx FPGA Expansion of CONTAIN_ROUTING Area","title":"Misc"},{"location":"fpga/pr/","text":"Morphous (Dynamic-sized) Partial Reconfiguration \u00b6 Version History Date Description Feb 6, 2020 Created Traditional partital reconfiguration (PR) is limited to using fix-sized PR regions. With one particular static bitstream, users are restricted to only have few pre-defined PR regions. If you wish to extend the PR region size, a whole chip reprogram is needed to burn a new static bitstream. This practice is suggested by FPGA vendors, and there are reasons behind it. However, during our experiment, we found that it is possible to have dynamic-sized PR regions with one static design. The mechanism is quite straightforward with some simple hacks. I will use a MicroBlaze-based design to demonstrate the approach with a VCU118 board. Stay tuned.","title":"Morphous Partial Reconfiguration"},{"location":"fpga/pr/#morphous-dynamic-sized-partial-reconfiguration","text":"Version History Date Description Feb 6, 2020 Created Traditional partital reconfiguration (PR) is limited to using fix-sized PR regions. With one particular static bitstream, users are restricted to only have few pre-defined PR regions. If you wish to extend the PR region size, a whole chip reprogram is needed to burn a new static bitstream. This practice is suggested by FPGA vendors, and there are reasons behind it. However, during our experiment, we found that it is possible to have dynamic-sized PR regions with one static design. The mechanism is quite straightforward with some simple hacks. I will use a MicroBlaze-based design to demonstrate the approach with a VCU118 board. Stay tuned.","title":"Morphous (Dynamic-sized) Partial Reconfiguration"},{"location":"fpga/scratch/","text":"Scratch Commands \u00b6 get_property LOC [get_cells count_out_OBUF[3]_inst] get_property ROUTE $net This returns a list of *nodes*. We can also see this in the GUI. % get_property ROUTE [get_nets inst_count/count_out[0]] Manually lock a route: set_property FIXED_ROUTE [get_property ROUTE [get_nets inst_count/count_out[0]]] [get_nets inst_count/count_out[0]]","title":"Scratch Commands"},{"location":"fpga/scratch/#scratch-commands","text":"get_property LOC [get_cells count_out_OBUF[3]_inst] get_property ROUTE $net This returns a list of *nodes*. We can also see this in the GUI. % get_property ROUTE [get_nets inst_count/count_out[0]] Manually lock a route: set_property FIXED_ROUTE [get_property ROUTE [get_nets inst_count/count_out[0]]] [get_nets inst_count/count_out[0]]","title":"Scratch Commands"},{"location":"fpga/setup_hold/","text":"Setup and Hold Time \u00b6 This is a few pages from the Digital Design and Computer Architecture book. It is well written and has explained the setup/hold feature so well. Link: http://lastweek.io/pubs/setup_hold.pdf","title":"Setup and Hold Time"},{"location":"fpga/setup_hold/#setup-and-hold-time","text":"This is a few pages from the Digital Design and Computer Architecture book. It is well written and has explained the setup/hold feature so well. Link: http://lastweek.io/pubs/setup_hold.pdf","title":"Setup and Hold Time"},{"location":"fpga/vivado/","text":"Vivado Practice \u00b6 Version History Date Description Nov 5, 2019 More stuff Nov 4, 2019 Add UG903 Oct 31, 2019 Happy Halloween Sep 20, 2019 Created Cheatsheet \u00b6 Partition Pins \u00b6 The partition pins are inserted by Vivado at the boundary of a PR region. PartPin is short for Partition Pins. PPLOC is short for Partpin LOC. Get the list of partition pins: get_pplocs - pins [ get_pins - hier * ] Partition pin (seems) map to a NODE: % report_property [ get_pplocs - pins [ get_pins XXX ]] % report_property [ get_pplocs - pins [ get_pins inst_count / count_out [ 0 ]]] INFO : [ Vivado 12 - 4841 ] Found PartPin: INT_X17Y790 / NN1_E_BEG3 Property Type Read-only Value BASE_CLOCK_REGION string true X0Y13 CLASS string true node Pblocks \u00b6 Semantic of EXCLUDE_PLACEMENT \u00b6 The document describe this as: Pblock property that prevents the placement of any logic not belonging to the Pblock inside the defined Pblock range. During my own simple experiment, I found that even Vivado will not place other logics into the Pblock, the routes of static region can still go across pblock. Semantic of CONTAIN_ROUTING \u00b6 References: UG909 and UG905. The contained routing requirement of RP Pblocks for UltraScale and UltraScale+ devices has been relaxed to allow for improved routing and timing results. Instead of routing being confined strictly to the resources owned by the Pblock, the routing footprint is expanded. Note that this option is enabled by default. When this option is enabled, 1) not all interface ports receive a partition pin, 2) the RP will use routing resources outside its confined area. This is annonying in some way. If this option is disabled, the implications are: 1) each interface port (per bit) receivces a partition pin, 2) RP will only resources confined to its pblocks, 3) the generated PR bitstream will be smaller, 4) hd_visual/ will not be generated. However, this option does not prevent routings from the static region from crossing RPs. This command is useful when you want to do some hacking about Partition Pins. Actually, you can also do this via GUI. set_param hd.routingContainmentAreaExpansion false But you wouldn\u2019t believe that: Static routing is still allowed to use resources inside of the Pblock. The implication is also obvious: all PR bitstreams and even blank bitstream will also have the static routing, if their targeted Pblocks happen to have static routing in the first place. This is also why we will need the static bitstream as the base to do PR bitstream generation. Clear RM and Lock Down Static \u00b6 These commands clear out the Reconfigurable Module logics from the whole design and then lock down the static region and static routing. (Reference: UG947) update_design - cell XXX - black_box lock_design - level routing Routing \u00b6 Get the routing of a net \u00b6 set net [ get_nets XXX ] get_property ROUTE $net Lock the routing of a net \u00b6 We need to lock both the net and the connected cells. Reference is UG903. Following commands lock a route of a net. This net is already routed. You could run one by one. After execution, the route will become dashed (means locked). Replace the net name with your interested one. set net [ get_nets inst_count/count_out [ 0 ]] get_property ROUTE $net set_property FIXED_ROUTE [ get_property ROUTE $net ] $net set_property is_bel_fixed 1 [ get_cells XXX ] set_property is_loc_fixed 1 [ get_cells XXX ] Manual routing \u00b6 A great GUI-based manual routing tutorial can be found at UG986 Lab 3 . The last step of manual routing, of course is to lock down the LOC and set FIXED_ROUTE . But how can we manually route an unrouted net? The difficulty is that we need to manually find out all the connection nodes/tiles etc.. This applies to LOC placement as well. Read-the-docs \u00b6 Basic UG912 Vivado Properties Reference Guide Excellent resource on explaining cell, net, pin, port, and so on. Differentiate Netlist Objects and Device Resource Objects . Netlist Objects pin : A pin is a point of logical connectivity on a primitive or hierarchical cell. A pin allows the contents of a cell to be abstracted away, and the logic simplified for ease-of-use. A pin is attached to a cell and can be connected to pins on other cells by a net. get_pins -of [get_cells XXX] . get_pins XXX port : A port is a special type of hierarchical pin, providing an external connection point at the top-level of a hierarchical design, or an internal connection point in a hierarchical cell or block module to connect the internal logic to the pins on the hierarchical cell. cell : A cell is an instance of a netlist logic object, which can either be a leaf-cell or a hierarchical cell. A leaf-cell is a primitive, or a primitive macro, with no further logic detail in the netlist. A hierarchical cell is a module or block that contains one or more additional levels of logic, and eventually concludes at leaf-cells. .. cells have PINs which are connected to NETs to define the external netlist\u2026 The CELL can be placed onto a BEL object in the case of basic logic such as flops, LUTs, and MUXes; or can be placed onto a SITE object in the case of larger logic cells such as BRAMs and DSPs. net : A net is a set of interconnected pins, ports, and wires. Every wire has a net name, which identifies it. Two or more wires can have the same net name. All wires sharing a common net name are part of a single NET, and all pins or ports connected to these wires are electrically connected. .. In the design netlist, a NET can be connected to the PIN of a CELL, or to a PORT. .. As the design is mapped onto the target Xilinx FPGA, the NET is mapped to routing resources such as WIREs, NODEs, and PIPs on the device, and is connected to BELs through BEL_PINs, and to SITEs through SITE_PINs. pblock : A Pblock is a collection of cells, and one or more rectangular areas or regions that specify the device resources contained by the Pblock. Pblocks are used during floorplanning placement to group related logic and assign it to a region of the target device. Example create_pblock Pblock_usbEngine add_cells_to_pblock [get_pblocks Pblock_usbEngine] [get_cells -quiet [listusbEngine1]] resize_pblock [get_pblocks Pblock_usbEngine] -add {SLICE_X8Y105:SLICE_X23Y149} resize_pblock [get_pblocks Pblock_usbEngine] -add {DSP48_X0Y42:DSP48_X1Y59} resize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB18_X0Y42:RAMB18_X1Y59} resize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB36_X0Y21:RAMB36_X1Y29} Device Resource Objects BEL : 1) leaf-level cells from the netlist design can be mapped onto bels on the target part 2) Bels are grouped in sites. 3) Each bel has bel_pins that map to pins on the cells. 4) get_bels -of [get_cells XX] , get_bels -of [get_nets XX] , and so on. BEL_PIN : 1) a pin or connection point on a BEL object. 2) BEL_PIN is a device object, associated with netlist objects such as the PIN on a CELL, which is the connection point for the NET. 3) get_bel_pins -of_objects [get_pins -of [get_cells XXX]] TILE SITE NODE WIRE PIP CONTAIN_ROUTING : The CONTAIN_ROUTING property restricts the routing of signals contained within a Pblock to use routing resources within the area defined by the Pblock. This prevents signals inside the Pblock from being routed outside the Pblock, and increases the reusability of the design. This is useful when you are trying to do advanced PR hacks. UG835 Vivado TCL Reference Guide aka. Vivado TCL Man Page. Read this with the above UG912. UG894 Vivado Using TCL scripting Get you started with Vivado TCL UG903 Using Constraints About Xilinx XDC files. You will need to understand UG912 first. Physical Constraints DONT_TOUCH . Prevent netlist optimizations. 1) prevent a net from being optimized away. 2) Prevent merging of manually replicated logic. Placement constraints Routing constraints Book: Practical Programming in Tcl and Tk Partial Reconfiguration Related UG909 Partial Reconfiguration Partition Pins Interface points called partition pins are automatically created within the Pblock ranges defined for the Reconfigurable Partition. These virtual I/O are established within interconnect tiles as the anchor points that remain consistent from one module to the next. In UltraScale or UltraScale+ designs, not all interface ports receive a partition pin . With the routing expansion feature, as explained in Expansion of CONTAIN_ROUTING Area, some interface nets are completely contained within the expanded region. When this happens, no partition pin is inserted; the entire net, including the source and all loads, is contained within the area captured by the partial bit file. Rather than pick an unnecessary intermediate point for the route, the entire net is rerouted, giving the Vivado tools the flexibility to pick an optimal solution. Exmaple set_property HD.PARTPIN_LOCS INT_R_X4Y153 [get_ports ] set_property HD.PARTPIN_RANGE SLICE_X4Y153:SLICE_X5Y157 [get_ports ] set_property HD.PARTPIN_RANGE {SLICE_Xx0Yx0:SLICE_Xx1Yy1 SLICE_XxNYyN:SLICE_XxMYyM} [get_pins /*] These pins can be manually relocated and locked. UG905 Hierarchical Design Add the CONTAIN_ROUTING property to all OOC Pblocks. Without this property, lock_design cannot lock the routing of an imported module because it cannot be guaranteed that there are no routing conflicts Some IPs \u00b6 UG947 has the sample code for the PR Controller IP It does not support simulation. Thus we can not probe any ICAP related signals. Ultrascale+ SEM does not have any useful ICAP usage signals in Simulation. xapp1230 has some TCL scripts to perform JTAG readback.","title":"Vivado Notes"},{"location":"fpga/vivado/#vivado-practice","text":"Version History Date Description Nov 5, 2019 More stuff Nov 4, 2019 Add UG903 Oct 31, 2019 Happy Halloween Sep 20, 2019 Created","title":"Vivado Practice"},{"location":"fpga/vivado/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"fpga/vivado/#partition-pins","text":"The partition pins are inserted by Vivado at the boundary of a PR region. PartPin is short for Partition Pins. PPLOC is short for Partpin LOC. Get the list of partition pins: get_pplocs - pins [ get_pins - hier * ] Partition pin (seems) map to a NODE: % report_property [ get_pplocs - pins [ get_pins XXX ]] % report_property [ get_pplocs - pins [ get_pins inst_count / count_out [ 0 ]]] INFO : [ Vivado 12 - 4841 ] Found PartPin: INT_X17Y790 / NN1_E_BEG3 Property Type Read-only Value BASE_CLOCK_REGION string true X0Y13 CLASS string true node","title":"Partition Pins"},{"location":"fpga/vivado/#pblocks","text":"","title":"Pblocks"},{"location":"fpga/vivado/#semantic-of-exclude_placement","text":"The document describe this as: Pblock property that prevents the placement of any logic not belonging to the Pblock inside the defined Pblock range. During my own simple experiment, I found that even Vivado will not place other logics into the Pblock, the routes of static region can still go across pblock.","title":"Semantic of EXCLUDE_PLACEMENT"},{"location":"fpga/vivado/#semantic-of-contain_routing","text":"References: UG909 and UG905. The contained routing requirement of RP Pblocks for UltraScale and UltraScale+ devices has been relaxed to allow for improved routing and timing results. Instead of routing being confined strictly to the resources owned by the Pblock, the routing footprint is expanded. Note that this option is enabled by default. When this option is enabled, 1) not all interface ports receive a partition pin, 2) the RP will use routing resources outside its confined area. This is annonying in some way. If this option is disabled, the implications are: 1) each interface port (per bit) receivces a partition pin, 2) RP will only resources confined to its pblocks, 3) the generated PR bitstream will be smaller, 4) hd_visual/ will not be generated. However, this option does not prevent routings from the static region from crossing RPs. This command is useful when you want to do some hacking about Partition Pins. Actually, you can also do this via GUI. set_param hd.routingContainmentAreaExpansion false But you wouldn\u2019t believe that: Static routing is still allowed to use resources inside of the Pblock. The implication is also obvious: all PR bitstreams and even blank bitstream will also have the static routing, if their targeted Pblocks happen to have static routing in the first place. This is also why we will need the static bitstream as the base to do PR bitstream generation.","title":"Semantic of CONTAIN_ROUTING"},{"location":"fpga/vivado/#clear-rm-and-lock-down-static","text":"These commands clear out the Reconfigurable Module logics from the whole design and then lock down the static region and static routing. (Reference: UG947) update_design - cell XXX - black_box lock_design - level routing","title":"Clear RM and Lock Down Static"},{"location":"fpga/vivado/#routing","text":"","title":"Routing"},{"location":"fpga/vivado/#get-the-routing-of-a-net","text":"set net [ get_nets XXX ] get_property ROUTE $net","title":"Get the routing of a net"},{"location":"fpga/vivado/#lock-the-routing-of-a-net","text":"We need to lock both the net and the connected cells. Reference is UG903. Following commands lock a route of a net. This net is already routed. You could run one by one. After execution, the route will become dashed (means locked). Replace the net name with your interested one. set net [ get_nets inst_count/count_out [ 0 ]] get_property ROUTE $net set_property FIXED_ROUTE [ get_property ROUTE $net ] $net set_property is_bel_fixed 1 [ get_cells XXX ] set_property is_loc_fixed 1 [ get_cells XXX ]","title":"Lock the routing of a net"},{"location":"fpga/vivado/#manual-routing","text":"A great GUI-based manual routing tutorial can be found at UG986 Lab 3 . The last step of manual routing, of course is to lock down the LOC and set FIXED_ROUTE . But how can we manually route an unrouted net? The difficulty is that we need to manually find out all the connection nodes/tiles etc.. This applies to LOC placement as well.","title":"Manual routing"},{"location":"fpga/vivado/#read-the-docs","text":"Basic UG912 Vivado Properties Reference Guide Excellent resource on explaining cell, net, pin, port, and so on. Differentiate Netlist Objects and Device Resource Objects . Netlist Objects pin : A pin is a point of logical connectivity on a primitive or hierarchical cell. A pin allows the contents of a cell to be abstracted away, and the logic simplified for ease-of-use. A pin is attached to a cell and can be connected to pins on other cells by a net. get_pins -of [get_cells XXX] . get_pins XXX port : A port is a special type of hierarchical pin, providing an external connection point at the top-level of a hierarchical design, or an internal connection point in a hierarchical cell or block module to connect the internal logic to the pins on the hierarchical cell. cell : A cell is an instance of a netlist logic object, which can either be a leaf-cell or a hierarchical cell. A leaf-cell is a primitive, or a primitive macro, with no further logic detail in the netlist. A hierarchical cell is a module or block that contains one or more additional levels of logic, and eventually concludes at leaf-cells. .. cells have PINs which are connected to NETs to define the external netlist\u2026 The CELL can be placed onto a BEL object in the case of basic logic such as flops, LUTs, and MUXes; or can be placed onto a SITE object in the case of larger logic cells such as BRAMs and DSPs. net : A net is a set of interconnected pins, ports, and wires. Every wire has a net name, which identifies it. Two or more wires can have the same net name. All wires sharing a common net name are part of a single NET, and all pins or ports connected to these wires are electrically connected. .. In the design netlist, a NET can be connected to the PIN of a CELL, or to a PORT. .. As the design is mapped onto the target Xilinx FPGA, the NET is mapped to routing resources such as WIREs, NODEs, and PIPs on the device, and is connected to BELs through BEL_PINs, and to SITEs through SITE_PINs. pblock : A Pblock is a collection of cells, and one or more rectangular areas or regions that specify the device resources contained by the Pblock. Pblocks are used during floorplanning placement to group related logic and assign it to a region of the target device. Example create_pblock Pblock_usbEngine add_cells_to_pblock [get_pblocks Pblock_usbEngine] [get_cells -quiet [listusbEngine1]] resize_pblock [get_pblocks Pblock_usbEngine] -add {SLICE_X8Y105:SLICE_X23Y149} resize_pblock [get_pblocks Pblock_usbEngine] -add {DSP48_X0Y42:DSP48_X1Y59} resize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB18_X0Y42:RAMB18_X1Y59} resize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB36_X0Y21:RAMB36_X1Y29} Device Resource Objects BEL : 1) leaf-level cells from the netlist design can be mapped onto bels on the target part 2) Bels are grouped in sites. 3) Each bel has bel_pins that map to pins on the cells. 4) get_bels -of [get_cells XX] , get_bels -of [get_nets XX] , and so on. BEL_PIN : 1) a pin or connection point on a BEL object. 2) BEL_PIN is a device object, associated with netlist objects such as the PIN on a CELL, which is the connection point for the NET. 3) get_bel_pins -of_objects [get_pins -of [get_cells XXX]] TILE SITE NODE WIRE PIP CONTAIN_ROUTING : The CONTAIN_ROUTING property restricts the routing of signals contained within a Pblock to use routing resources within the area defined by the Pblock. This prevents signals inside the Pblock from being routed outside the Pblock, and increases the reusability of the design. This is useful when you are trying to do advanced PR hacks. UG835 Vivado TCL Reference Guide aka. Vivado TCL Man Page. Read this with the above UG912. UG894 Vivado Using TCL scripting Get you started with Vivado TCL UG903 Using Constraints About Xilinx XDC files. You will need to understand UG912 first. Physical Constraints DONT_TOUCH . Prevent netlist optimizations. 1) prevent a net from being optimized away. 2) Prevent merging of manually replicated logic. Placement constraints Routing constraints Book: Practical Programming in Tcl and Tk Partial Reconfiguration Related UG909 Partial Reconfiguration Partition Pins Interface points called partition pins are automatically created within the Pblock ranges defined for the Reconfigurable Partition. These virtual I/O are established within interconnect tiles as the anchor points that remain consistent from one module to the next. In UltraScale or UltraScale+ designs, not all interface ports receive a partition pin . With the routing expansion feature, as explained in Expansion of CONTAIN_ROUTING Area, some interface nets are completely contained within the expanded region. When this happens, no partition pin is inserted; the entire net, including the source and all loads, is contained within the area captured by the partial bit file. Rather than pick an unnecessary intermediate point for the route, the entire net is rerouted, giving the Vivado tools the flexibility to pick an optimal solution. Exmaple set_property HD.PARTPIN_LOCS INT_R_X4Y153 [get_ports ] set_property HD.PARTPIN_RANGE SLICE_X4Y153:SLICE_X5Y157 [get_ports ] set_property HD.PARTPIN_RANGE {SLICE_Xx0Yx0:SLICE_Xx1Yy1 SLICE_XxNYyN:SLICE_XxMYyM} [get_pins /*] These pins can be manually relocated and locked. UG905 Hierarchical Design Add the CONTAIN_ROUTING property to all OOC Pblocks. Without this property, lock_design cannot lock the routing of an imported module because it cannot be guaranteed that there are no routing conflicts","title":"Read-the-docs"},{"location":"fpga/vivado/#some-ips","text":"UG947 has the sample code for the PR Controller IP It does not support simulation. Thus we can not probe any ICAP related signals. Ultrascale+ SEM does not have any useful ICAP usage signals in Simulation. xapp1230 has some TCL scripts to perform JTAG readback.","title":"Some IPs"},{"location":"general_log/0719/","text":"Jul 2019 \u00b6 0727 Sat \u00b6 bug fixed, we need to do the \u201cprep_new_page\u201d before return the page back into our lists. Obviously, pages in our lists are in \u201callocated\u201d state. Okay. Eval perf also. Looks like the direct invocation of cb_alloc_zero_page is not good.. Although the perf stat show that using pgadvance help to reduce the handlemmfault overhead by 15%, the actual runtime is the same.. weird. 0726 Friday \u00b6 Stuck at the BUG: bad mm counter and print_bad_pte . Issues identified: looks like I must use pcp lists? 0725 \u00b6 HA. I leaned about perf lock , which requies a kernel with CONFIG_LOCKDEP and CONFIG_LOCK_STAT . Not sure exactly how these options work, but they should insert some code inside each lock acquire and release. Anyway, with perf lock , we are able to know what specific locks are hot. This is fantastic. Note that normal perf record -e 'cycles:k' can tell use how much time is spent on spin_lock , but it does not say how much time each specific lock uses. Cool. Ah, side note, make nconfig is really powerful!! Even though I\u2019ve been playing with Linux for many years, I haven\u2019t actually played with linux config manually. Tried once. Our spinlock for pgadvance list shouldn\u2019t be the issue. Now disable it and reboot. 254 pg 252 nopg 37.140 0724 \u00b6 Bagel Day. First replot the figrue, adding an avg/95P/99P figure. Very long tails. Yeah! Then tune page advance. The key is to find a not busy CPU. I\u2019m first trying Round-Robin . Try TF Cifar. First run without pgadvance, see how the first 100 step performs in different runs. They seem to be very stable. 1 ) step = 100 ( 167 .432 sec ) 2 ) step = 100 ( 168 .021 sec ) step = 200 ( 167 .467 sec ) 3 ) step = 100 ( 167 .903 sec ) DAMN. Forgot to turn off huge page. Now use perf to make sure do_anonymous_page got enough cycles.. 22/23 down, no pgadvance, 163, 161, 161 22/23 down, no pgadvance, no perf, 159 0723 \u00b6 11:59pm End of day. Learned how to plot Violin. Very long tail, and does not scale well even with PCP opt! 9pm I\u2019m using my own benchmark to measure buddy allocator. I\u2019m testing order-0 alloc performance. Something weird happen during test. The 16 th line is very costly. And after that, suddenly the perf improves nealy 50%. I\u2019m reporting CPU cycles, CPU frequency change shouldn\u2019t matter, right? Linux has Per-CPU Pages (pcp), which is intended to optimize 0-order allocation. In my test case, each CPU keep 7 free pages. The patten is reflected in the measurement. Note that, the refill is sync. \"\"\" (latency in CPU cycles. 2.4GHz Xeon E5 v3) \"\"\" ... [ 1043.789257] idx=11956 order=0 latency=3128 [ 1043.789257] idx=11957 order=0 latency=376 [ 1043.789258] idx=11958 order=0 latency=376 [ 1043.789258] idx=11959 order=0 latency=368 [ 1043.789258] idx=11960 order=0 latency=376 [ 1043.789259] idx=11961 order=0 latency=400 [ 1043.789259] idx=11962 order=0 latency=384 [ 1043.789260] idx=11963 order=0 latency=3080 [ 1043.789260] idx=11964 order=0 latency=408 [ 1043.789260] idx=11965 order=0 latency=400 [ 1043.789261] idx=11966 order=0 latency=392 [ 1043.789261] idx=11967 order=0 latency=360 [ 1043.789262] idx=11968 order=0 latency=360 [ 1043.789262] idx=11969 order=0 latency=376 [ 1043.789262] idx=11970 order=0 latency=2992 [ 1043.789263] idx=11971 order=0 latency=29930 [ 1043.789263] idx=11972 order=0 latency=171 [ 1043.789264] idx=11973 order=0 latency=156 [ 1043.789264] idx=11974 order=0 latency=177 [ 1043.789264] idx=11975 order=0 latency=174 [ 1043.789265] idx=11976 order=0 latency=174 [ 1043.789265] idx=11977 order=0 latency=1419 [ 1043.789265] idx=11978 order=0 latency=156 [ 1043.789266] idx=11979 order=0 latency=174 [ 1043.789266] idx=11980 order=0 latency=171 [ 1043.789267] idx=11981 order=0 latency=171 [ 1043.789267] idx=11982 order=0 latency=171 [ 1043.789267] idx=11983 order=0 latency=156 [ 1043.789268] idx=11984 order=0 latency=1362 [ 1043.789268] idx=11985 order=0 latency=174 [ 1043.789269] idx=11986 order=0 latency=168 [ 1043.789269] idx=11987 order=0 latency=156 [ 1043.789269] idx=11988 order=0 latency=168 [ 1043.789270] idx=11989 order=0 latency=174 [ 1043.789270] idx=11990 order=0 latency=171 [ 1043.789270] idx=11991 order=0 latency=1266 ...","title":"Jul 2019"},{"location":"general_log/0719/#jul-2019","text":"","title":"Jul 2019"},{"location":"general_log/0719/#0727-sat","text":"bug fixed, we need to do the \u201cprep_new_page\u201d before return the page back into our lists. Obviously, pages in our lists are in \u201callocated\u201d state. Okay. Eval perf also. Looks like the direct invocation of cb_alloc_zero_page is not good.. Although the perf stat show that using pgadvance help to reduce the handlemmfault overhead by 15%, the actual runtime is the same.. weird.","title":"0727 Sat"},{"location":"general_log/0719/#0726-friday","text":"Stuck at the BUG: bad mm counter and print_bad_pte . Issues identified: looks like I must use pcp lists?","title":"0726 Friday"},{"location":"general_log/0719/#0725","text":"HA. I leaned about perf lock , which requies a kernel with CONFIG_LOCKDEP and CONFIG_LOCK_STAT . Not sure exactly how these options work, but they should insert some code inside each lock acquire and release. Anyway, with perf lock , we are able to know what specific locks are hot. This is fantastic. Note that normal perf record -e 'cycles:k' can tell use how much time is spent on spin_lock , but it does not say how much time each specific lock uses. Cool. Ah, side note, make nconfig is really powerful!! Even though I\u2019ve been playing with Linux for many years, I haven\u2019t actually played with linux config manually. Tried once. Our spinlock for pgadvance list shouldn\u2019t be the issue. Now disable it and reboot. 254 pg 252 nopg 37.140","title":"0725"},{"location":"general_log/0719/#0724","text":"Bagel Day. First replot the figrue, adding an avg/95P/99P figure. Very long tails. Yeah! Then tune page advance. The key is to find a not busy CPU. I\u2019m first trying Round-Robin . Try TF Cifar. First run without pgadvance, see how the first 100 step performs in different runs. They seem to be very stable. 1 ) step = 100 ( 167 .432 sec ) 2 ) step = 100 ( 168 .021 sec ) step = 200 ( 167 .467 sec ) 3 ) step = 100 ( 167 .903 sec ) DAMN. Forgot to turn off huge page. Now use perf to make sure do_anonymous_page got enough cycles.. 22/23 down, no pgadvance, 163, 161, 161 22/23 down, no pgadvance, no perf, 159","title":"0724"},{"location":"general_log/0719/#0723","text":"11:59pm End of day. Learned how to plot Violin. Very long tail, and does not scale well even with PCP opt! 9pm I\u2019m using my own benchmark to measure buddy allocator. I\u2019m testing order-0 alloc performance. Something weird happen during test. The 16 th line is very costly. And after that, suddenly the perf improves nealy 50%. I\u2019m reporting CPU cycles, CPU frequency change shouldn\u2019t matter, right? Linux has Per-CPU Pages (pcp), which is intended to optimize 0-order allocation. In my test case, each CPU keep 7 free pages. The patten is reflected in the measurement. Note that, the refill is sync. \"\"\" (latency in CPU cycles. 2.4GHz Xeon E5 v3) \"\"\" ... [ 1043.789257] idx=11956 order=0 latency=3128 [ 1043.789257] idx=11957 order=0 latency=376 [ 1043.789258] idx=11958 order=0 latency=376 [ 1043.789258] idx=11959 order=0 latency=368 [ 1043.789258] idx=11960 order=0 latency=376 [ 1043.789259] idx=11961 order=0 latency=400 [ 1043.789259] idx=11962 order=0 latency=384 [ 1043.789260] idx=11963 order=0 latency=3080 [ 1043.789260] idx=11964 order=0 latency=408 [ 1043.789260] idx=11965 order=0 latency=400 [ 1043.789261] idx=11966 order=0 latency=392 [ 1043.789261] idx=11967 order=0 latency=360 [ 1043.789262] idx=11968 order=0 latency=360 [ 1043.789262] idx=11969 order=0 latency=376 [ 1043.789262] idx=11970 order=0 latency=2992 [ 1043.789263] idx=11971 order=0 latency=29930 [ 1043.789263] idx=11972 order=0 latency=171 [ 1043.789264] idx=11973 order=0 latency=156 [ 1043.789264] idx=11974 order=0 latency=177 [ 1043.789264] idx=11975 order=0 latency=174 [ 1043.789265] idx=11976 order=0 latency=174 [ 1043.789265] idx=11977 order=0 latency=1419 [ 1043.789265] idx=11978 order=0 latency=156 [ 1043.789266] idx=11979 order=0 latency=174 [ 1043.789266] idx=11980 order=0 latency=171 [ 1043.789267] idx=11981 order=0 latency=171 [ 1043.789267] idx=11982 order=0 latency=171 [ 1043.789267] idx=11983 order=0 latency=156 [ 1043.789268] idx=11984 order=0 latency=1362 [ 1043.789268] idx=11985 order=0 latency=174 [ 1043.789269] idx=11986 order=0 latency=168 [ 1043.789269] idx=11987 order=0 latency=156 [ 1043.789269] idx=11988 order=0 latency=168 [ 1043.789270] idx=11989 order=0 latency=174 [ 1043.789270] idx=11990 order=0 latency=171 [ 1043.789270] idx=11991 order=0 latency=1266 ...","title":"0723"},{"location":"general_log/0819/","text":"Aug 2019 \u00b6 Aug 14 \u00b6 Back to sweet WL. Helping out for asplos submission. I was trying to run Octopus. Its cmake report that MPI_C is missing, so I run yum install openmpi-devel . However, this failed due to some broken dependency on rdma-core and others. It seems these packages have been updated by mlx-ofed.. what a mess.","title":"Aug 2019"},{"location":"general_log/0819/#aug-2019","text":"","title":"Aug 2019"},{"location":"general_log/0819/#aug-14","text":"Back to sweet WL. Helping out for asplos submission. I was trying to run Octopus. Its cmake report that MPI_C is missing, so I run yum install openmpi-devel . However, this failed due to some broken dependency on rdma-core and others. It seems these packages have been updated by mlx-ofed.. what a mess.","title":"Aug 14"},{"location":"general_log/0919/","text":"Sep 2019 \u00b6 Log range 0920 - 0930. 9/26/19 \u00b6 Check out PR today. create_pblock 9/25/19 \u00b6 Singularity and Helios 1 2 3 - I've been reading Singularity today. It has many insights on extension and isolation. Something we might be interested: 1) application has a manifest. We may want to have a similar one for each FPGA app. 2) Seal OS architecture, where the OS or app remain invariant after install. This is the nature of FPGA.. - Also, I think it's important to figure out a way to do IP sharing. The same thing is also beneficial for PR. - Scheduling: preemptive or non-preemptive.. Thoughts after reading Singularity papers - The contract-based channel is promising - The manifest-based program approach is also promising. Similarly, the AmoghOS has some Resource Vector associated with each FPGA application. I think it\u2019s valid and beneficial to attach such a spec with FPGA applications. I think I need to think more on the applications. Cannot wait till its too late! If an app is too big to fit into a FPGA, can we do \u201cbitstream\u201d swap? - App need to conform to some sort of model (e.g., msg-based) - Fast PR - Must be slow from app\u2019s point, but a solution., 0923 Monday \u00b6 Continue working on FPGA stuff. Let\u2019s focus on writing possible design ideas. I should also read some related work. 0921 Weekends \u00b6 Spent some time reading ATC papers, came across quite some interesting ones. Distributed actor runtime I came across actors many times recently. Like the iPipe, ST-Accel. There are some open-source frameworks. Erlang and akka. It\u2019s model that I should consider in the future SSD Related Alibaba has a study paper about SSD reliability in their datacenters. Amy Tai has an interesting paper, they enable distributed storage systems to run on high error rate SSDs. Traditionally, if an SSD has a high error rate, it will impact local file system perf thus higher level system perf. Their idea is neat: utilize the remote replicas to recover local SSD errors! Thus they could use those SSDs! File system on SSD study. Paper from Toronto. I haven\u2019t read it yet. 0920 Fri \u00b6 Well.. I should continue on this. We moved to UCSD recently. Everything is setup except desktop and server stuff. Started using F1 recently. Porting our code from VCU108 to VCU118. The migration between boards and between different vivado versions is a REAL headache. (VCU108 -> VCU118 && 2018.2 -> 2018.3) So for those TCL scripts generated by vivado, i found it will use hardcoded IP version. Upgrading vivado means possibly updated IP versions, thus broken TCL scripts. I\u2019ve found a way to workaround. But if the IP interface changed, it has to be modified manually. Many things left on the table - Merge LegoOS code - Think about and finish design doc - Tons of papers to read Life wise: sea is nearby, although UCSD gym sucks, it has jiu jitsu courses. Let\u2019s do the work. Try make PCIe work first. Checking out xtp444, the VCU118 PCIe reference design. Okay. Finished patching the XDC file, basically went through the example designs and check couple design docs, same old shit. Synthesis can pass. Implementation failed because Disk is full (?!). Anyway, next step is: - Resize Disk size - Run implementation, check it can pass - Run simulation, functionality check of RDM!","title":"Sep 2019"},{"location":"general_log/0919/#sep-2019","text":"Log range 0920 - 0930.","title":"Sep 2019"},{"location":"general_log/0919/#92619","text":"Check out PR today. create_pblock","title":"9/26/19"},{"location":"general_log/0919/#92519","text":"Singularity and Helios 1 2 3 - I've been reading Singularity today. It has many insights on extension and isolation. Something we might be interested: 1) application has a manifest. We may want to have a similar one for each FPGA app. 2) Seal OS architecture, where the OS or app remain invariant after install. This is the nature of FPGA.. - Also, I think it's important to figure out a way to do IP sharing. The same thing is also beneficial for PR. - Scheduling: preemptive or non-preemptive.. Thoughts after reading Singularity papers - The contract-based channel is promising - The manifest-based program approach is also promising. Similarly, the AmoghOS has some Resource Vector associated with each FPGA application. I think it\u2019s valid and beneficial to attach such a spec with FPGA applications. I think I need to think more on the applications. Cannot wait till its too late! If an app is too big to fit into a FPGA, can we do \u201cbitstream\u201d swap? - App need to conform to some sort of model (e.g., msg-based) - Fast PR - Must be slow from app\u2019s point, but a solution.,","title":"9/25/19"},{"location":"general_log/0919/#0923-monday","text":"Continue working on FPGA stuff. Let\u2019s focus on writing possible design ideas. I should also read some related work.","title":"0923 Monday"},{"location":"general_log/0919/#0921-weekends","text":"Spent some time reading ATC papers, came across quite some interesting ones. Distributed actor runtime I came across actors many times recently. Like the iPipe, ST-Accel. There are some open-source frameworks. Erlang and akka. It\u2019s model that I should consider in the future SSD Related Alibaba has a study paper about SSD reliability in their datacenters. Amy Tai has an interesting paper, they enable distributed storage systems to run on high error rate SSDs. Traditionally, if an SSD has a high error rate, it will impact local file system perf thus higher level system perf. Their idea is neat: utilize the remote replicas to recover local SSD errors! Thus they could use those SSDs! File system on SSD study. Paper from Toronto. I haven\u2019t read it yet.","title":"0921 Weekends"},{"location":"general_log/0919/#0920-fri","text":"Well.. I should continue on this. We moved to UCSD recently. Everything is setup except desktop and server stuff. Started using F1 recently. Porting our code from VCU108 to VCU118. The migration between boards and between different vivado versions is a REAL headache. (VCU108 -> VCU118 && 2018.2 -> 2018.3) So for those TCL scripts generated by vivado, i found it will use hardcoded IP version. Upgrading vivado means possibly updated IP versions, thus broken TCL scripts. I\u2019ve found a way to workaround. But if the IP interface changed, it has to be modified manually. Many things left on the table - Merge LegoOS code - Think about and finish design doc - Tons of papers to read Life wise: sea is nearby, although UCSD gym sucks, it has jiu jitsu courses. Let\u2019s do the work. Try make PCIe work first. Checking out xtp444, the VCU118 PCIe reference design. Okay. Finished patching the XDC file, basically went through the example designs and check couple design docs, same old shit. Synthesis can pass. Implementation failed because Disk is full (?!). Anyway, next step is: - Resize Disk size - Run implementation, check it can pass - Run simulation, functionality check of RDM!","title":"0920 Fri"},{"location":"general_log/1019/","text":"Oct 2019 \u00b6 Writing a research journal here has its advantages: simple and version-control. But I\u2019m moving to Google Docs.","title":"Oct 2019"},{"location":"general_log/1019/#oct-2019","text":"Writing a research journal here has its advantages: simple and version-control. But I\u2019m moving to Google Docs.","title":"Oct 2019"},{"location":"lego/driver/ib/","text":"Infiniband Subsystem \u00b6 Current Status \u00b6 Lego\u2019s IB stack is ported based on linux-3.11.1 . We ported: ib_core mlx4_ib mlx4_core Lego does not support uverbs. At the time of writing, Lego IB stack has only been tested on Mellanox Technologies MT27500 Family [ConnectX-3] . Random summary \u00b6 The stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly. Top down ib_core \u00b6 IB core code is in driver/infiniband/core , which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on. In IB, each port\u2019s QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD. There are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see ib_mad_completion_handler() ). A lot layers, huh? ib_mad_completion_handler() : we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c mlx4_ib and mlx4_core \u00b6 mlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation. A lot IB verbs are ultimately translated into fw.c __mlx4_cmd() , which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don\u2019t really have a working IRQ subsystem, so we use polling instead. I\u2019m still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled. Init Sequence \u00b6 Init PCI subsystem, build data structures Core IB layer register ib_client mlx4_init() : register PCI driver, provide a callback __mlx4_init_one() : initialize the hardware itself, register interrupt handler. mlx4_ib_init() : allocate a ib_device, and register, which will callback through all ib_client registered at step 1. \u2013 Yizhou Shan Created: Aug 20, 2018 Last Updated: Aug 20, 2018","title":"Infiniband"},{"location":"lego/driver/ib/#infiniband-subsystem","text":"","title":"Infiniband Subsystem"},{"location":"lego/driver/ib/#current-status","text":"Lego\u2019s IB stack is ported based on linux-3.11.1 . We ported: ib_core mlx4_ib mlx4_core Lego does not support uverbs. At the time of writing, Lego IB stack has only been tested on Mellanox Technologies MT27500 Family [ConnectX-3] .","title":"Current Status"},{"location":"lego/driver/ib/#random-summary","text":"The stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly. Top down","title":"Random summary"},{"location":"lego/driver/ib/#ib_core","text":"IB core code is in driver/infiniband/core , which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on. In IB, each port\u2019s QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD. There are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see ib_mad_completion_handler() ). A lot layers, huh? ib_mad_completion_handler() : we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c","title":"ib_core"},{"location":"lego/driver/ib/#mlx4_ib-and-mlx4_core","text":"mlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation. A lot IB verbs are ultimately translated into fw.c __mlx4_cmd() , which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don\u2019t really have a working IRQ subsystem, so we use polling instead. I\u2019m still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled.","title":"mlx4_ib and mlx4_core"},{"location":"lego/driver/ib/#init-sequence","text":"Init PCI subsystem, build data structures Core IB layer register ib_client mlx4_init() : register PCI driver, provide a callback __mlx4_init_one() : initialize the hardware itself, register interrupt handler. mlx4_ib_init() : allocate a ib_device, and register, which will callback through all ib_client registered at step 1. \u2013 Yizhou Shan Created: Aug 20, 2018 Last Updated: Aug 20, 2018","title":"Init Sequence"},{"location":"lego/driver/pci/","text":"PCI Subsystem \u00b6 What we have ported so far \u00b6 PCI data structures such as pci_dev , pci_bus , and so on. Mechanism to scan bus and build data structures during boot. Performed by pci_scan_root_bus() , and most code is in driver/pci/probe.c Unfinished business \u00b6 Ways to go through all PCI device. pci_init_capabilities() : for each PCI device pci_fixup_device() : a lot quicks, maybe not useful pcie_aspm_init_link_state() : PCIe link state pci_iov_bus_range : all SR-IOV support \u2013 Yizhou Shan Created: July 5, 2018 Last Updated: July 5, 2018","title":"PCI"},{"location":"lego/driver/pci/#pci-subsystem","text":"","title":"PCI Subsystem"},{"location":"lego/driver/pci/#what-we-have-ported-so-far","text":"PCI data structures such as pci_dev , pci_bus , and so on. Mechanism to scan bus and build data structures during boot. Performed by pci_scan_root_bus() , and most code is in driver/pci/probe.c","title":"What we have ported so far"},{"location":"lego/driver/pci/#unfinished-business","text":"Ways to go through all PCI device. pci_init_capabilities() : for each PCI device pci_fixup_device() : a lot quicks, maybe not useful pcie_aspm_init_link_state() : PCIe link state pci_iov_bus_range : all SR-IOV support \u2013 Yizhou Shan Created: July 5, 2018 Last Updated: July 5, 2018","title":"Unfinished business"},{"location":"lego/kernel/boot/","text":"Notes on GRUB2 and Boot Sequence \u00b6 Version History Date Description Mar 31, 2020 Copied from https://github.com/lastweek/source-grub2 . About GRUB2 \u00b6 GRUB2: https://www.gnu.org/software/grub/manual/grub/grub.html#Introduction Source code: https://github.com/lastweek/source-grub2 linux v.s. linux16 \u00b6 An interesting thing is that there are two ways to load an kernel image in grub.cfg , either linux vmlinuz-3.10.0 or linux16 vmlinuz-3.10.0 . They have different effects, but not sure what are those differences. I remember only the linux16 one works for me, but not remembering why either. At least on CentOS 7, it\u2019s all linux16. The linux16 and initrd16 in grub-core/loader/i386/pc/linux.c : GRUB_MOD_INIT ( linux16 ) { cmd_linux = grub_register_command ( \"linux16\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd16\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; } The linux and initrd in grub-core/loader/i386/linux.c : static grub_command_t cmd_linux , cmd_initrd ; GRUB_MOD_INIT ( linux ) { cmd_linux = grub_register_command ( \"linux\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; } Boot Protocol and Sequence \u00b6 This was written for https://github.com/lastweek/source-grub2 . I just copied it here. Linux (x86) has a boot protocol, described by https://www.kernel.org/doc/html/latest/x86/boot.html . Essentially, it is a contiguous memory region, just like a big C struct : some fields are filled by kernel duing compile time ( arch/x86/boot/tools/build.c and some in code), some fields are filled by GRUB2 during boot time to tell kernel some important addresses, e.g., kernel parameters, ramdisk locations etc. GRUB2 code follows the protocol, and you can partially tell from the grub_cmd_linux() function. Last time I working on this was late 2016, I truly spent a lot investigating how GRUB and linux boot works. I will try to document a bit, if my memory serves: In the Linux kernel, file arch/x86/boot/header.S is the first file got run after GRUB2. This file is a bit complicated but not hard to understand! It has 3 parts. For the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot. It the kernel was loaded by a bootloader like GRUB2, the first part will never execute. The bootload will directly jump to the second part. This is part of the boot protocol. For the second part, it lists all the fields described by the boot protocol. And finally the third part is real-mode instructions that got run after the GRUB2 jumo. The starting function is called start_of_setup , which will do some stack checking, and then jump to C code in arch/x86/boot/main.c . arch/x86/boot/main.c runs on real-mode, it will do some setup and jump to protected-mode (32-bit). It is running after BIOS but before the actual Linux kernel. Thus this piece of code must rely on BIOS to do stuff, which makes it very unique. The major task of the setup code is to prepare the struct boot_params , which has all the boot information, some of them were extracted from the header.S . The struct boot_params will be passed down and used by many kernel subsystems later on. The final jump happens in arch/x86/boot/pmjump.S # # Jump to protected-mode kernel, 0x100000 # which is the compressed/head_$(BITS).o # jmp *% eax Then, we are in arch/x86/boot/compressed/head_64.S . Above pmjump jumps to startup_32 , it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point startup_64 . And finally, we are in 64-bit. The final jump will go to arch/x86/kernel/head_64.S . We are close! Now we are in arch/x86/kernel/head_64.S . We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to arch/x86/kernel/head64.c , the C code! I guess an interesting part is secondary_startup_64 . This code is actually run by non-booting CPUs, or secondary CPUs. After the major boot CPU is up and running (already within start_kernel() ), I believe its the smp_init() that will send IPI wakeup interrupts to all present secondary CPUs. The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in arch/x86/realmode/rm/trampoline.S ! arch/x86/realmode is interesting. It uses piggyback technique. All the real-mode and 32bit code are in arch/x86/realmode/rm/* , a special linker script is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty! Hooray, C world. We are in arch/x86/kernel/head64.c . The starting function is x86_64_start_kernel ! And the end is the start_kernel , the one in init/main.c . In all, there are a lot jumps after GRUB2 load the kernel, and its a long road before we can reach start_kernel() . It probably should not be this complex, but the x86 architecture really makes it worse. Happy hacking!","title":"GRUB2 and Boot"},{"location":"lego/kernel/boot/#notes-on-grub2-and-boot-sequence","text":"Version History Date Description Mar 31, 2020 Copied from https://github.com/lastweek/source-grub2 .","title":"Notes on GRUB2 and Boot Sequence"},{"location":"lego/kernel/boot/#about-grub2","text":"GRUB2: https://www.gnu.org/software/grub/manual/grub/grub.html#Introduction Source code: https://github.com/lastweek/source-grub2","title":"About GRUB2"},{"location":"lego/kernel/boot/#linux-vs-linux16","text":"An interesting thing is that there are two ways to load an kernel image in grub.cfg , either linux vmlinuz-3.10.0 or linux16 vmlinuz-3.10.0 . They have different effects, but not sure what are those differences. I remember only the linux16 one works for me, but not remembering why either. At least on CentOS 7, it\u2019s all linux16. The linux16 and initrd16 in grub-core/loader/i386/pc/linux.c : GRUB_MOD_INIT ( linux16 ) { cmd_linux = grub_register_command ( \"linux16\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd16\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; } The linux and initrd in grub-core/loader/i386/linux.c : static grub_command_t cmd_linux , cmd_initrd ; GRUB_MOD_INIT ( linux ) { cmd_linux = grub_register_command ( \"linux\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; }","title":"linux v.s. linux16"},{"location":"lego/kernel/boot/#boot-protocol-and-sequence","text":"This was written for https://github.com/lastweek/source-grub2 . I just copied it here. Linux (x86) has a boot protocol, described by https://www.kernel.org/doc/html/latest/x86/boot.html . Essentially, it is a contiguous memory region, just like a big C struct : some fields are filled by kernel duing compile time ( arch/x86/boot/tools/build.c and some in code), some fields are filled by GRUB2 during boot time to tell kernel some important addresses, e.g., kernel parameters, ramdisk locations etc. GRUB2 code follows the protocol, and you can partially tell from the grub_cmd_linux() function. Last time I working on this was late 2016, I truly spent a lot investigating how GRUB and linux boot works. I will try to document a bit, if my memory serves: In the Linux kernel, file arch/x86/boot/header.S is the first file got run after GRUB2. This file is a bit complicated but not hard to understand! It has 3 parts. For the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot. It the kernel was loaded by a bootloader like GRUB2, the first part will never execute. The bootload will directly jump to the second part. This is part of the boot protocol. For the second part, it lists all the fields described by the boot protocol. And finally the third part is real-mode instructions that got run after the GRUB2 jumo. The starting function is called start_of_setup , which will do some stack checking, and then jump to C code in arch/x86/boot/main.c . arch/x86/boot/main.c runs on real-mode, it will do some setup and jump to protected-mode (32-bit). It is running after BIOS but before the actual Linux kernel. Thus this piece of code must rely on BIOS to do stuff, which makes it very unique. The major task of the setup code is to prepare the struct boot_params , which has all the boot information, some of them were extracted from the header.S . The struct boot_params will be passed down and used by many kernel subsystems later on. The final jump happens in arch/x86/boot/pmjump.S # # Jump to protected-mode kernel, 0x100000 # which is the compressed/head_$(BITS).o # jmp *% eax Then, we are in arch/x86/boot/compressed/head_64.S . Above pmjump jumps to startup_32 , it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point startup_64 . And finally, we are in 64-bit. The final jump will go to arch/x86/kernel/head_64.S . We are close! Now we are in arch/x86/kernel/head_64.S . We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to arch/x86/kernel/head64.c , the C code! I guess an interesting part is secondary_startup_64 . This code is actually run by non-booting CPUs, or secondary CPUs. After the major boot CPU is up and running (already within start_kernel() ), I believe its the smp_init() that will send IPI wakeup interrupts to all present secondary CPUs. The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in arch/x86/realmode/rm/trampoline.S ! arch/x86/realmode is interesting. It uses piggyback technique. All the real-mode and 32bit code are in arch/x86/realmode/rm/* , a special linker script is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty! Hooray, C world. We are in arch/x86/kernel/head64.c . The starting function is x86_64_start_kernel ! And the end is the start_kernel , the one in init/main.c . In all, there are a lot jumps after GRUB2 load the kernel, and its a long road before we can reach start_kernel() . It probably should not be this complex, but the x86 architecture really makes it worse. Happy hacking!","title":"Boot Protocol and Sequence"},{"location":"lego/kernel/debug/","text":"Debug Facility in Lego \u00b6 Lego provides several handy debug helpers to ease our coding pain. We category them by layers, namely 1) Core Kernel , the lowest level of Lego, which is shared by all managers. 2) Processor Manager , which controls processor components. 3) Memory Manager , which controls memory components. Core Kernel \u00b6 void dump_pte ( pte_t * ptep , const char * reason ); void dump_page ( struct page * page , const char * reason ); These two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache. void ptdump_walk_pgd_level ( pgd_t * pgd ); This debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables. void show_state_filter ( unsigned long state_filter , bool print_rq ); void sched_show_task ( struct task_struct * p ); void sysrq_sched_debug_show ( void ); This set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu runqueue . Use this set of functions if you are developing code related to scheduler. Processor Manager \u00b6 void dump_pcache_meta ( struct pcache_meta * pcm , const char * reason ); void dump_pcache_victim ( struct pcache_victim_meta * victim , const char * reason ); void dump_pcache_rmap ( struct pcache_rmap * rmap , const char * reason ); void dump_pcache_line ( struct pcache_meta * pcm , const char * reason ); These functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code. Memory Manager \u00b6 void dump_lego_mm ( const struct lego_mm_struct * mm ); void dump_vma ( const struct vm_area_struct * vma ); These two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.","title":"Debug"},{"location":"lego/kernel/debug/#debug-facility-in-lego","text":"Lego provides several handy debug helpers to ease our coding pain. We category them by layers, namely 1) Core Kernel , the lowest level of Lego, which is shared by all managers. 2) Processor Manager , which controls processor components. 3) Memory Manager , which controls memory components.","title":"Debug Facility in Lego"},{"location":"lego/kernel/debug/#core-kernel","text":"void dump_pte ( pte_t * ptep , const char * reason ); void dump_page ( struct page * page , const char * reason ); These two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache. void ptdump_walk_pgd_level ( pgd_t * pgd ); This debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables. void show_state_filter ( unsigned long state_filter , bool print_rq ); void sched_show_task ( struct task_struct * p ); void sysrq_sched_debug_show ( void ); This set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu runqueue . Use this set of functions if you are developing code related to scheduler.","title":"Core Kernel"},{"location":"lego/kernel/debug/#processor-manager","text":"void dump_pcache_meta ( struct pcache_meta * pcm , const char * reason ); void dump_pcache_victim ( struct pcache_victim_meta * victim , const char * reason ); void dump_pcache_rmap ( struct pcache_rmap * rmap , const char * reason ); void dump_pcache_line ( struct pcache_meta * pcm , const char * reason ); These functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code.","title":"Processor Manager"},{"location":"lego/kernel/debug/#memory-manager","text":"void dump_lego_mm ( const struct lego_mm_struct * mm ); void dump_vma ( const struct vm_area_struct * vma ); These two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.","title":"Memory Manager"},{"location":"lego/kernel/fpu/","text":"x86 Floating Point Unit \u00b6 This is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson. FPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the strcmp function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm. I would say, without Ingo Molnar\u2019s x86 FPU code rewrite , there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don\u2019t quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art. Anyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts. Boot \u00b6 FPU detection and init happen during early boot. You should know the struct fpu is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since struct fpu is part of task_struct , that implies task_struct is dynamically-sized too. Apparently, cpu_init() will also callback to init its local FPU. Context Switch \u00b6 FPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software\u2019s duty to save and restore FPU context properly. FPU context is saved in struct fpu . Thus whenever we switch task, we also need to switch FPU context: __visible struct task_struct * __switch_to ( struct task_struct * prev_p , struct task_struct * next_p ) { .. fpu_switch = switch_fpu_prepare ( prev_fpu , next_fpu , cpu ); .. switch_fpu_finish ( next_fpu , fpu_switch ); .. } SYSCALL \u00b6 fork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread. execve(): When execve() is called, the FPU context will be cleared. exit(): When a thread exit,, FPU will do cleanup based on if eagerfpu or lazyfpu is used. Exceptions \u00b6 Like the device not available exception, which may be triggered if lazyfpu is used. Also, do_simd_exception and do_coprocessor_error , which are some math related exceptions. Signal \u00b6 Kernel needs to setup a sigframe for user level signal handlers. sigframe is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to sigframe in stack. Thoughts \u00b6 I\u2019ve been debugging this FPU introduced bugs for over a month. And during this month, I\u2019m always not sure if it is FPU\u2019s bug, or some other code that corrupts memory. So I\u2019m lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don\u2019t think I need all of it. One stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I\u2019m able to move to next stage. Compatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers. \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 23, 2018","title":"x86 FPU"},{"location":"lego/kernel/fpu/#x86-floating-point-unit","text":"This is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson. FPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the strcmp function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm. I would say, without Ingo Molnar\u2019s x86 FPU code rewrite , there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don\u2019t quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art. Anyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts.","title":"x86 Floating Point Unit"},{"location":"lego/kernel/fpu/#boot","text":"FPU detection and init happen during early boot. You should know the struct fpu is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since struct fpu is part of task_struct , that implies task_struct is dynamically-sized too. Apparently, cpu_init() will also callback to init its local FPU.","title":"Boot"},{"location":"lego/kernel/fpu/#context-switch","text":"FPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software\u2019s duty to save and restore FPU context properly. FPU context is saved in struct fpu . Thus whenever we switch task, we also need to switch FPU context: __visible struct task_struct * __switch_to ( struct task_struct * prev_p , struct task_struct * next_p ) { .. fpu_switch = switch_fpu_prepare ( prev_fpu , next_fpu , cpu ); .. switch_fpu_finish ( next_fpu , fpu_switch ); .. }","title":"Context Switch"},{"location":"lego/kernel/fpu/#syscall","text":"fork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread. execve(): When execve() is called, the FPU context will be cleared. exit(): When a thread exit,, FPU will do cleanup based on if eagerfpu or lazyfpu is used.","title":"SYSCALL"},{"location":"lego/kernel/fpu/#exceptions","text":"Like the device not available exception, which may be triggered if lazyfpu is used. Also, do_simd_exception and do_coprocessor_error , which are some math related exceptions.","title":"Exceptions"},{"location":"lego/kernel/fpu/#signal","text":"Kernel needs to setup a sigframe for user level signal handlers. sigframe is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to sigframe in stack.","title":"Signal"},{"location":"lego/kernel/fpu/#thoughts","text":"I\u2019ve been debugging this FPU introduced bugs for over a month. And during this month, I\u2019m always not sure if it is FPU\u2019s bug, or some other code that corrupts memory. So I\u2019m lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don\u2019t think I need all of it. One stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I\u2019m able to move to next stage. Compatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers. \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 23, 2018","title":"Thoughts"},{"location":"lego/kernel/grub/","text":"Use GRUB2 to boot Lego \u00b6 Last Updated: 02/02/2018 This document explains: 1) how Lego itself is written to pretend as a Linux kernel, 2) how to boot Lego kernel with GRUB2, 3) GRUB2 configurations specific to Lego. How Lego pretend as a Linux kernel \u00b6 asdsad How to config GRUB2 for Lego \u00b6 asdsa","title":"GRUB"},{"location":"lego/kernel/grub/#use-grub2-to-boot-lego","text":"Last Updated: 02/02/2018 This document explains: 1) how Lego itself is written to pretend as a Linux kernel, 2) how to boot Lego kernel with GRUB2, 3) GRUB2 configurations specific to Lego.","title":"Use GRUB2 to boot Lego"},{"location":"lego/kernel/grub/#how-lego-pretend-as-a-linux-kernel","text":"asdsad","title":"How Lego pretend as a Linux kernel"},{"location":"lego/kernel/grub/#how-to-config-grub2-for-lego","text":"asdsa","title":"How to config GRUB2 for Lego"},{"location":"lego/kernel/irq/","text":"IRQ \u00b6 IRQ is majorly ported based on linux-4.4 . The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I\u2019m not so familiar with this stuff. This technique decision has pros and cons. The whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now. Init \u00b6 The first part of initialization is trap_init() at early setup_arch() . The second major entry point is irq_init() at start_kernel() . This irq_init() is actually a combination of linux\u2019s: early_irq_init() : 1) setup irq_desc[] array, and then call arch_early_irq_init() , which will register two IRQ domains (x86_vector_domain, msi_domain). init_IRQ() : is actually a callback to low-level x86 interrupt setup. It mainly setup the desc\u2019s data/chip etc, and register all different handlers. In Lego, you will be able to find all the functionalitis are moved into arch_irq_init() . And, to this point, we have a complete setup. The third (and last) entry point is smp_prepare_cpus() : smp_prepare_cpus() -> apic_bsp_setup() -> setup_local_APIC() -> setup_IO_APIC() -> x86_init.timers.setup_percpu_clockev() IRQ Domain \u00b6 We should have at least 2 or 3 IRQ domains: x86_vector x86_msi x86_ioapic-N (each ioapic has one) The first two guys are created during arch_irq_init() . While the latter ioapic ones are created during setup_IO_APIC() . All of them are allocated eventually by __irq_domain_add() , and linked at LIST_HEAD(irq_domain_list) . So.... Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs. However, this IRQ number MAY not have a identical mapping to hardware\u2019s own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That\u2019s the purpose of having linear_revmap and revmap_tree within each domain, it is used to translate hwirq to IRQ. Why two different data structures? linear_revmap is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don\u2019t want to waste memory, that\u2019s how we want to use trees. These two can be used together. If we fail to insert into linear_revmap , we insert into tree. During search time, we need to look up both. By default, x86_vector and x86_msi use radix tree only. x86_ioapic-N uses a mix of linear and radix tree. To dump all IRQ domains, call dump_irq_domain_list() , which give you something like this: [ 118.308544 ] name mapped linear - max direct - max devtree - node [ 118.316114 ] x86_ioapic - 2 24 24 0 [ 118.322707 ] x86_ioapic - 1 24 24 0 [ 118.329299 ] x86_ioapic - 0 24 24 0 [ 118.335893 ] x86_msi 25 0 0 [ 118.342486 ] * x86_vector 40 0 0 [ 118.349078 ] irq hwirq chip name chip data active type domain [ 118.358775 ] 1 0x00001 IO - APIC 0xffff88107fcae000 LINEAR x86_ioapic - 0 [ 118.368858 ] 3 0x00003 IO - APIC 0xffff88107fc8f000 LINEAR x86_ioapic - 0 [ 118.378940 ] 4 0x00004 IO - APIC 0xffff88107fc6e000 LINEAR x86_ioapic - 0 [ 118.389025 ] 5 0x00005 IO - APIC 0xffff88107fc6f000 LINEAR x86_ioapic - 0 [ 118.399109 ] 6 0x00006 IO - APIC 0xffff88107fc4e000 LINEAR x86_ioapic - 0 [ 118.409192 ] 7 0x00007 IO - APIC 0xffff88107fc4f000 LINEAR x86_ioapic - 0 [ 118.419276 ] 8 0x00008 IO - APIC 0xffff88107fc2e000 LINEAR x86_ioapic - 0 [ 118.429358 ] 9 0x00009 IO - APIC 0xffff88107fc2f000 LINEAR x86_ioapic - 0 [ 118.439442 ] 10 0x0000a IO - APIC 0xffff88107fc0e000 LINEAR x86_ioapic - 0 [ 118.449525 ] 11 0x0000b IO - APIC 0xffff88107fc0f000 LINEAR x86_ioapic - 0 [ 118.459609 ] 12 0x0000c IO - APIC 0xffff88107fff0000 LINEAR x86_ioapic - 0 [ 118.469692 ] 13 0x0000d IO - APIC 0xffff88107fff1000 LINEAR x86_ioapic - 0 [ 118.479776 ] 14 0x0000e IO - APIC 0xffff88107fff2000 LINEAR x86_ioapic - 0 [ 118.489860 ] 15 0x0000f IO - APIC 0xffff88107fff3000 LINEAR x86_ioapic - 0 [ 118.499943 ] 24 0x300000 PCI - MSI ( null ) * RADIX x86_msi [ 118.509833 ] 25 0x300001 PCI - MSI ( null ) * RADIX x86_msi [ 118.519722 ] 26 0x300002 PCI - MSI ( null ) * RADIX x86_msi [ 118.529612 ] 27 0x300003 PCI - MSI ( null ) * RADIX x86_msi [ 118.539501 ] 28 0x300004 PCI - MSI ( null ) RADIX x86_msi Aug 20, 2018 \u00b6 Well, I\u2019ve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff. These days, I was verifying our IB code and tried to add back mlx4en\u2019s interrupt handler, somehow, there is no interrupt after request_irq() . Two possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work. \u2013 Last Updated: Aug 28, 2018","title":"IRQ"},{"location":"lego/kernel/irq/#irq","text":"IRQ is majorly ported based on linux-4.4 . The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I\u2019m not so familiar with this stuff. This technique decision has pros and cons. The whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now.","title":"IRQ"},{"location":"lego/kernel/irq/#init","text":"The first part of initialization is trap_init() at early setup_arch() . The second major entry point is irq_init() at start_kernel() . This irq_init() is actually a combination of linux\u2019s: early_irq_init() : 1) setup irq_desc[] array, and then call arch_early_irq_init() , which will register two IRQ domains (x86_vector_domain, msi_domain). init_IRQ() : is actually a callback to low-level x86 interrupt setup. It mainly setup the desc\u2019s data/chip etc, and register all different handlers. In Lego, you will be able to find all the functionalitis are moved into arch_irq_init() . And, to this point, we have a complete setup. The third (and last) entry point is smp_prepare_cpus() : smp_prepare_cpus() -> apic_bsp_setup() -> setup_local_APIC() -> setup_IO_APIC() -> x86_init.timers.setup_percpu_clockev()","title":"Init"},{"location":"lego/kernel/irq/#irq-domain","text":"We should have at least 2 or 3 IRQ domains: x86_vector x86_msi x86_ioapic-N (each ioapic has one) The first two guys are created during arch_irq_init() . While the latter ioapic ones are created during setup_IO_APIC() . All of them are allocated eventually by __irq_domain_add() , and linked at LIST_HEAD(irq_domain_list) . So.... Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs. However, this IRQ number MAY not have a identical mapping to hardware\u2019s own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That\u2019s the purpose of having linear_revmap and revmap_tree within each domain, it is used to translate hwirq to IRQ. Why two different data structures? linear_revmap is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don\u2019t want to waste memory, that\u2019s how we want to use trees. These two can be used together. If we fail to insert into linear_revmap , we insert into tree. During search time, we need to look up both. By default, x86_vector and x86_msi use radix tree only. x86_ioapic-N uses a mix of linear and radix tree. To dump all IRQ domains, call dump_irq_domain_list() , which give you something like this: [ 118.308544 ] name mapped linear - max direct - max devtree - node [ 118.316114 ] x86_ioapic - 2 24 24 0 [ 118.322707 ] x86_ioapic - 1 24 24 0 [ 118.329299 ] x86_ioapic - 0 24 24 0 [ 118.335893 ] x86_msi 25 0 0 [ 118.342486 ] * x86_vector 40 0 0 [ 118.349078 ] irq hwirq chip name chip data active type domain [ 118.358775 ] 1 0x00001 IO - APIC 0xffff88107fcae000 LINEAR x86_ioapic - 0 [ 118.368858 ] 3 0x00003 IO - APIC 0xffff88107fc8f000 LINEAR x86_ioapic - 0 [ 118.378940 ] 4 0x00004 IO - APIC 0xffff88107fc6e000 LINEAR x86_ioapic - 0 [ 118.389025 ] 5 0x00005 IO - APIC 0xffff88107fc6f000 LINEAR x86_ioapic - 0 [ 118.399109 ] 6 0x00006 IO - APIC 0xffff88107fc4e000 LINEAR x86_ioapic - 0 [ 118.409192 ] 7 0x00007 IO - APIC 0xffff88107fc4f000 LINEAR x86_ioapic - 0 [ 118.419276 ] 8 0x00008 IO - APIC 0xffff88107fc2e000 LINEAR x86_ioapic - 0 [ 118.429358 ] 9 0x00009 IO - APIC 0xffff88107fc2f000 LINEAR x86_ioapic - 0 [ 118.439442 ] 10 0x0000a IO - APIC 0xffff88107fc0e000 LINEAR x86_ioapic - 0 [ 118.449525 ] 11 0x0000b IO - APIC 0xffff88107fc0f000 LINEAR x86_ioapic - 0 [ 118.459609 ] 12 0x0000c IO - APIC 0xffff88107fff0000 LINEAR x86_ioapic - 0 [ 118.469692 ] 13 0x0000d IO - APIC 0xffff88107fff1000 LINEAR x86_ioapic - 0 [ 118.479776 ] 14 0x0000e IO - APIC 0xffff88107fff2000 LINEAR x86_ioapic - 0 [ 118.489860 ] 15 0x0000f IO - APIC 0xffff88107fff3000 LINEAR x86_ioapic - 0 [ 118.499943 ] 24 0x300000 PCI - MSI ( null ) * RADIX x86_msi [ 118.509833 ] 25 0x300001 PCI - MSI ( null ) * RADIX x86_msi [ 118.519722 ] 26 0x300002 PCI - MSI ( null ) * RADIX x86_msi [ 118.529612 ] 27 0x300003 PCI - MSI ( null ) * RADIX x86_msi [ 118.539501 ] 28 0x300004 PCI - MSI ( null ) RADIX x86_msi","title":"IRQ Domain"},{"location":"lego/kernel/irq/#aug-20-2018","text":"Well, I\u2019ve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff. These days, I was verifying our IB code and tried to add back mlx4en\u2019s interrupt handler, somehow, there is no interrupt after request_irq() . Two possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work. \u2013 Last Updated: Aug 28, 2018","title":"Aug 20, 2018"},{"location":"lego/kernel/kconfig/","text":"Lego Kconfig \u00b6 Network \u00b6 Enable CONFIG_INFINIBAND Enable CONFIG_FIT Set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT : boot time connection timeout Set CONFIG_FIT_NR_NODES : number of Lego nodes in this run Set CONFIG_FIT_LOCAL_ID : current node id In net/lego/fit_machine.c , modify the lego_cluster_hostnames array to match the machines you are using. Set CONFIG_DEFAULT_MEM_NODE in processor manager Set CONFIG_DEFAULT_STORAGE_NODE if you are running with storage component. Network configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect. Processor \u00b6 Enable CONFIG_COMP_PROCESSOR open .config remove line # CONFIG_COMP_PROCESSOR is not set close .config do make , you will see Configure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW) , select Y Choose default configuration for all new config options Enable CONFIG_USE_RAMFS if you are not using storage components Memory \u00b6 Enable CONFIG_COMP_MEMORY open .config remove line # CONFIG_COMP_MEMORY is not set close .config do make , you will see Configure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW) , select Y Choose default configuration for all new config options Enable CONFIG_USE_RAMFS if you are not using storage components Set CONFIG_RAMFS_OBJECT_FILE : points to static-linked ELF file that you want to execute. tips: you can put your test code under usr/ directory, and a simple make will compile everything under. Run without Storage Component \u00b6 To run Lego just with one processor component and one memory component, you need to: Enable CONFIG_USE_RAMFS at both sides. And in memory side, you need to set the CONFIG_RAMFS_OBJECT_FILE , which points to the ELF binary you want to test. make sure CONFIG_DEFAULT_MEM_NODE at processor component is pointing to memory component\u2019s node id. A typical code snippet and configuration would be: static const char * lego_cluster_hostnames [ CONFIG_FIT_NR_NODES ] = { [ 0 ] = \"wuklab00\" , [ 1 ] = \"wuklab01\" , }; wuklab00 Processor # # Lego Processor Component Configurations # CONFIG_COMP_PROCESSOR=y CONFIG_CHECKPOINT=y CONFIG_MEMMAP_MEMBLOCK_RESERVED=y # CONFIG_PCACHE_EVICT_RANDOM is not set # CONFIG_PCACHE_EVICT_FIFO is not set CONFIG_PCACHE_EVICT_LRU=y CONFIG_PCACHE_EVICT_GENERIC_SWEEP=y # CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set # CONFIG_PCACHE_EVICTION_PERSET_LIST is not set CONFIG_PCACHE_EVICTION_VICTIM=y CONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8 CONFIG_PCACHE_PREFETCH=y # # Processor DEBUG Options # # # Lego Memory Component Configurations # # CONFIG_COMP_MEMORY is not set # # DRAM Cache Options # CONFIG_PCACHE_LINE_SIZE_SHIFT=12 CONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3 # # General Manager Config/Debug Options # CONFIG_DEFAULT_MEM_NODE=1 CONFIG_DEFAULT_STORAGE_NODE=2 CONFIG_USE_RAMFS=y # # Networking # # CONFIG_LWIP is not set CONFIG_FIT=y # CONFIG_FIT_DEBUG is not set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30 CONFIG_FIT_NR_NODES=2 CONFIG_FIT_LOCAL_ID=0 wuklab01 Memory # # Lego Memory Component Configurations # CONFIG_COMP_MEMORY=y # # Memory DEBUG Options # # CONFIG_MEM_PREFETCH is not set # # DRAM Cache Options # CONFIG_PCACHE_LINE_SIZE_SHIFT=12 CONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3 # # General Manager Config/Debug Options # CONFIG_DEFAULT_MEM_NODE=1 CONFIG_DEFAULT_STORAGE_NODE=2 CONFIG_USE_RAMFS=y CONFIG_RAMFS_OBJECT_FILE=\"usr/pcache_conflict.o\" # # Networking # # CONFIG_LWIP is not set CONFIG_FIT=y # CONFIG_FIT_DEBUG is not set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30 CONFIG_FIT_NR_NODES=2 CONFIG_FIT_LOCAL_ID=1","title":"Kconfig"},{"location":"lego/kernel/kconfig/#lego-kconfig","text":"","title":"Lego Kconfig"},{"location":"lego/kernel/kconfig/#network","text":"Enable CONFIG_INFINIBAND Enable CONFIG_FIT Set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT : boot time connection timeout Set CONFIG_FIT_NR_NODES : number of Lego nodes in this run Set CONFIG_FIT_LOCAL_ID : current node id In net/lego/fit_machine.c , modify the lego_cluster_hostnames array to match the machines you are using. Set CONFIG_DEFAULT_MEM_NODE in processor manager Set CONFIG_DEFAULT_STORAGE_NODE if you are running with storage component. Network configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect.","title":"Network"},{"location":"lego/kernel/kconfig/#processor","text":"Enable CONFIG_COMP_PROCESSOR open .config remove line # CONFIG_COMP_PROCESSOR is not set close .config do make , you will see Configure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW) , select Y Choose default configuration for all new config options Enable CONFIG_USE_RAMFS if you are not using storage components","title":"Processor"},{"location":"lego/kernel/kconfig/#memory","text":"Enable CONFIG_COMP_MEMORY open .config remove line # CONFIG_COMP_MEMORY is not set close .config do make , you will see Configure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW) , select Y Choose default configuration for all new config options Enable CONFIG_USE_RAMFS if you are not using storage components Set CONFIG_RAMFS_OBJECT_FILE : points to static-linked ELF file that you want to execute. tips: you can put your test code under usr/ directory, and a simple make will compile everything under.","title":"Memory"},{"location":"lego/kernel/kconfig/#run-without-storage-component","text":"To run Lego just with one processor component and one memory component, you need to: Enable CONFIG_USE_RAMFS at both sides. And in memory side, you need to set the CONFIG_RAMFS_OBJECT_FILE , which points to the ELF binary you want to test. make sure CONFIG_DEFAULT_MEM_NODE at processor component is pointing to memory component\u2019s node id. A typical code snippet and configuration would be: static const char * lego_cluster_hostnames [ CONFIG_FIT_NR_NODES ] = { [ 0 ] = \"wuklab00\" , [ 1 ] = \"wuklab01\" , }; wuklab00 Processor # # Lego Processor Component Configurations # CONFIG_COMP_PROCESSOR=y CONFIG_CHECKPOINT=y CONFIG_MEMMAP_MEMBLOCK_RESERVED=y # CONFIG_PCACHE_EVICT_RANDOM is not set # CONFIG_PCACHE_EVICT_FIFO is not set CONFIG_PCACHE_EVICT_LRU=y CONFIG_PCACHE_EVICT_GENERIC_SWEEP=y # CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set # CONFIG_PCACHE_EVICTION_PERSET_LIST is not set CONFIG_PCACHE_EVICTION_VICTIM=y CONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8 CONFIG_PCACHE_PREFETCH=y # # Processor DEBUG Options # # # Lego Memory Component Configurations # # CONFIG_COMP_MEMORY is not set # # DRAM Cache Options # CONFIG_PCACHE_LINE_SIZE_SHIFT=12 CONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3 # # General Manager Config/Debug Options # CONFIG_DEFAULT_MEM_NODE=1 CONFIG_DEFAULT_STORAGE_NODE=2 CONFIG_USE_RAMFS=y # # Networking # # CONFIG_LWIP is not set CONFIG_FIT=y # CONFIG_FIT_DEBUG is not set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30 CONFIG_FIT_NR_NODES=2 CONFIG_FIT_LOCAL_ID=0 wuklab01 Memory # # Lego Memory Component Configurations # CONFIG_COMP_MEMORY=y # # Memory DEBUG Options # # CONFIG_MEM_PREFETCH is not set # # DRAM Cache Options # CONFIG_PCACHE_LINE_SIZE_SHIFT=12 CONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3 # # General Manager Config/Debug Options # CONFIG_DEFAULT_MEM_NODE=1 CONFIG_DEFAULT_STORAGE_NODE=2 CONFIG_USE_RAMFS=y CONFIG_RAMFS_OBJECT_FILE=\"usr/pcache_conflict.o\" # # Networking # # CONFIG_LWIP is not set CONFIG_FIT=y # CONFIG_FIT_DEBUG is not set CONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30 CONFIG_FIT_NR_NODES=2 CONFIG_FIT_LOCAL_ID=1","title":"Run without Storage Component"},{"location":"lego/kernel/loader/","text":"Lego Program Loader \u00b6 This document explains the high-level workflow of Lego\u2019s program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended. Status \u00b6 Formats Supported ELF (static-linked) ELF (dynamic-linked) Overall \u00b6 In order to support different executable formats, Lego has a virtual loader layer above all specific formats, which is quite similar to virtual file system . In Lego, execve() is divided into two parts: 1) syscall hook at processor side, 2) real loader at memory side. Combined together, they provide the same semantic of execve() as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds. Lego\u2019s Loader \u00b6 Lego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start. Entry Point \u00b6 So the normal entry point is do_execve() . Above that, it can be invoked by syscall from user space, or from kernel space by calling do_execve() directly. There are not too many places that will call do_execve within kernel. One notable case is how kernel starts the pid 1 user program. This happens after kernel finished all initialization. The code is: static int run_init_process ( const char * init_filename ) { argv_init [ 0 ] = init_filename ; return do_execve ( init_filename , argv_init , envp_init ); } Memory Manager\u2019s Job \u00b6 Memory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass start_ip and start_stack back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand. Load ld-linux \u00b6 For dynamically-linked images, kernel ELF loader needs to load the ld-linux.so as well. It will first try to map the ld-linux.so into this process\u2019s virtual address space. Furthermore, the first user instruction that will run is no longer __libc_main_start , kernel will transfer the kernel to ld-linux.so instead. Thus, for a normal user program, ld-linux.so will load all the shared libraries before running glibc. static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... /* Dynamically-linked */ if ( elf_interpreter ) { unsigned long interp_map_addr = 0 ; elf_entry = load_elf_interp ( tsk , & loc -> interp_elf_ex , interpreter , & interp_map_addr , load_bias , interp_elf_phdata ); if ( ! IS_ERR (( void * ) elf_entry )) { /* * load_elf_interp() returns relocation * adjustment */ interp_load_addr = elf_entry ; elf_entry += loc -> interp_elf_ex . e_entry ; } if ( BAD_ADDR ( elf_entry )) { retval = IS_ERR (( void * ) elf_entry ) ? ( int ) elf_entry : - EINVAL ; goto out_free_dentry ; } reloc_func_desc = interp_load_addr ; put_lego_file ( interpreter ); kfree ( elf_interpreter ); } else { /* Statically-linked */ /* * e_entry is the VA to which the system first transfers control * Not the start_code! Normally, it is the <_start> function. */ elf_entry = loc -> elf_ex . e_entry ; if ( BAD_ADDR ( elf_entry )) { retval = - EINVAL ; goto out_free_dentry ; } } ... } Processor Manager\u2019s Job \u00b6 It needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run flush_old_exec() , and setup_new_exec() . Destroy old context: flush_old_exec() \u00b6 Zap other threads \u00b6 de_thread is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A exec starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader. Switch to new address space \u00b6 We also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache: static int exec_mmap ( void ) { struct mm_struct * new_mm ; struct mm_struct * old_mm ; struct task_struct * tsk ; new_mm = mm_alloc (); if ( ! new_mm ) return - ENOMEM ; tsk = current ; old_mm = current -> mm ; mm_release ( tsk , old_mm ); task_lock ( tsk ); tsk -> mm = new_mm ; tsk -> active_mm = new_mm ; activate_mm ( old_mm , new_mm ); task_unlock ( tsk ); if ( old_mm ) mmput ( old_mm ); return 0 ; } Clear Architecture-Specific state \u00b6 This is performed by flush_thread() , which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array: void flush_thread ( void ) { struct task_struct * tsk = current ; memset ( tsk -> thread . tls_array , 0 , sizeof ( tsk -> thread . tls_array )); fpu__clear ( & tsk -> thread . fpu ); } Setup new context: setup_new_exec() \u00b6 Lego\u2019s setup_new_exec() is quite different from Linux\u2019s default implementation. Lego moves several functions to memory component, like the arch_pick_mmap_layout stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff: static void setup_new_exec ( const char * filename ) { /* This is the point of no return */ current -> sas_ss_sp = current -> sas_ss_size = 0 ; set_task_comm ( current , kbasename ( filename )); flush_signal_handlers ( current , 0 ); } Change return frame in stack \u00b6 We do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail). /** * start_thread - Starting a new user thread * @regs: pointer to pt_regs * @new_ip: the first instruction IP of user thread * @new_sp: the new stack pointer of user thread */ void start_thread ( struct pt_regs * regs , unsigned long new_ip , unsigned long new_sp ) { loadsegment ( fs , 0 ); loadsegment ( es , 0 ); loadsegment ( ds , 0 ); load_gs_index ( 0 ); regs -> ip = new_ip ; regs -> sp = new_sp ; regs -> cs = __USER_CS ; regs -> ss = __USER_DS ; regs -> flags = X86_EFLAGS_IF ; } If calling execve() from userspace, the return frame is saved in the stack, we can simply do start_thread above, and merge to syscall return path. However, if calling execve() from a kernel thread, things changed. As you can see, all forked threads will run from ret_from_fork when it wakes for the first time. If it is a kernel thread, it jumps to line 23 , to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an execve() , and return frame has been changed by start_thread() . So we jump to line 16 to let it merge to syscall return path. /* * A newly forked process directly context switches into this address. * * rax: prev task we switched from * rbx: kernel thread func (NULL for user thread) * r12: kernel thread arg */ ENTRY ( ret_from_fork ) movq %rax , %rdi call schedule_tail /* rdi: 'prev' task parameter */ testq %rbx , %rbx /* from kernel_thread? */ jnz 1 f /* kernel threads are uncommon */ 2: movq %rsp , %rdi call syscall_return_slowpath /* return with IRQs disabled */ SWAPGS /* switch to user gs.base */ jmp restore_regs_and_iret 1: /* kernel thread */ movq %r12 , %rdi call * %rbx /* * A kernel thread is allowed to return here after successfully * calling do_execve(). Exit to userspace to complete the execve() * syscall: */ movq $0 , RAX ( %rsp ) jmp 2 b END ( ret_from_fork ) This is such a typical control flow hijacking. :-) Features \u00b6 This section lists various features, or behaviors and Lego\u2019s program loader. Virtual Address Space Range \u00b6 User\u2019s virtual address falls into this range: [sysctl_mmap_min_addr, TASK_SIZE) By default, unsigned long sysctl_mmap_min_addr = PAGE_SIZE ; /* * User space process size. 47bits minus one guard page. The guard * page is necessary on Intel CPUs: if a SYSCALL instruction is at * the highest possible canonical userspace address, then that * syscall will enter the kernel with a non-canonical return * address, and SYSRET will explode dangerously. We avoid this * particular problem by preventing anything from being mapped * at the maximum canonical address. */ #define TASK_SIZE ((1UL << 47) - PAGE_SIZE) Essentially: [0x1000, 0x7ffffffff000) Pre-Populated .bss and .brk \u00b6 The heap vma created at loading time is a combination of .bss and .brk segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just .bss pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by vm_brk() . int vm_brk ( struct lego_task_struct * tsk , unsigned long start , unsigned long len ) { int ret ; struct lego_mm_struct * mm = tsk -> mm ; if ( down_write_killable ( & mm -> mmap_sem )) return - EINTR ; ret = do_brk ( tsk , start , len ); up_write ( & mm -> mmap_sem ); /* Prepopulate brk pages */ if ( ! ret ) lego_mm_populate ( mm , start , len ); return ret ; } Un-Populated stack \u00b6 Stack vma is manually expanded to 32 pages + pages for argv info by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be: 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] The code to expand stack is done when ELF loader tries to finalize the stack vma, by calling setup_arg_pages() : int setup_arg_pages ( struct lego_task_struct * tsk , struct lego_binprm * bprm , unsigned long stack_top , int executable_stack ) { ... /* * 32*4k (or 2*64k) pages */ stack_expand = 131072UL ; stack_size = vma -> vm_end - vma -> vm_start ; stack_base = vma -> vm_start - stack_expand ; mm -> start_stack = bprm -> p ; ret = expand_stack ( vma , stack_base ); ... } Un-Populated .text and .data \u00b6 In essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data\u2019s on-demand paging, we can prefault them in the future. Disabled Randomized Top of Stack \u00b6 Lego currently does not randomize the stack top. The stack vma is allocated by bprm_mm_init() at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just TASK_SIZE : static int __bprm_mm_init ( struct lego_binprm * bprm ) { ... vma -> vm_end = TASK_SIZE ; ... } ( managers / memory / loader / elf . c ) Top of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer\u2019s setup_arg_pages() function, which is used to finalize the top of stack: int setup_arg_pages ( struct lego_task_struct * tsk , struct lego_binprm * bprm , unsigned long stack_top , int executable_stack ); So, to actually randomize the top of stack, you can simply do the following: static unsigned long randomize_stack_top ( unsigned long stack_top ) { unsigned long random_variable = 0 ; if (( current -> flags & PF_RANDOMIZE ) && ! ( current -> personality & ADDR_NO_RANDOMIZE )) { random_variable = get_random_long (); random_variable &= STACK_RND_MASK ; random_variable <<= PAGE_SHIFT ; } #ifdef CONFIG_STACK_GROWSUP return PAGE_ALIGN ( stack_top ) + random_variable ; #else return PAGE_ALIGN ( stack_top ) - random_variable ; #endif } static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... retval = setup_arg_pages ( bprm , randomize_stack_top ( TASK_SIZE ), executable_stack ); ... } However, current Lego disables randomization by passing TASK_SIZE : static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... retval = setup_arg_pages ( tsk , bprm , TASK_SIZE , executable_stack ); ... } ( managers / memory / loader / elf . c ) No vDSO \u00b6 Currently, Lego does not have vDSO support. There are not too many syscalls mapped in the vDSO, for x86-64 : clock_gettime getcpu gettimeofday time The reason to add it back is simple: if those syscalls are used a lot and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO must be mapped at processor side, mapped in our emulated pgtable. Below is the original part where loader maps vDSO: static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES /* * TODO: vdso * x86 can map vdso vma here */ #endif ... } managers / memory / loader / elf . c For lego, we should move it to processor right before start_thread() : int do_execve ( const char * filename , const char * const * argv , const char * const * envp ) { ... /* Should be here */ start_thread ( regs , new_ip , new_sp ); ... } Besides, don\u2019t forget to report the vDSO address in the aux vector: static int create_elf_tables ( struct lego_task_struct * tsk , struct lego_binprm * bprm , struct elfhdr * exec , unsigned long load_addr , unsigned long interp_load_addr , unsigned long * argv_len , unsigned long * envp_len ) { ... #ifdef ARCH_DLINFO /* * ARCH_DLINFO must come first so PPC can do its special alignment of * AUXV. * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in * ARCH_DLINFO changes */ ARCH_DLINFO ; #endif ... } \u2013 Yizhou Shan Created: Feb 16, 2018 Last Updated: Feb 27, 2018","title":"Program Loader"},{"location":"lego/kernel/loader/#lego-program-loader","text":"This document explains the high-level workflow of Lego\u2019s program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended.","title":"Lego Program Loader"},{"location":"lego/kernel/loader/#status","text":"Formats Supported ELF (static-linked) ELF (dynamic-linked)","title":"Status"},{"location":"lego/kernel/loader/#overall","text":"In order to support different executable formats, Lego has a virtual loader layer above all specific formats, which is quite similar to virtual file system . In Lego, execve() is divided into two parts: 1) syscall hook at processor side, 2) real loader at memory side. Combined together, they provide the same semantic of execve() as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds.","title":"Overall"},{"location":"lego/kernel/loader/#legos-loader","text":"Lego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start.","title":"Lego's Loader"},{"location":"lego/kernel/loader/#entry-point","text":"So the normal entry point is do_execve() . Above that, it can be invoked by syscall from user space, or from kernel space by calling do_execve() directly. There are not too many places that will call do_execve within kernel. One notable case is how kernel starts the pid 1 user program. This happens after kernel finished all initialization. The code is: static int run_init_process ( const char * init_filename ) { argv_init [ 0 ] = init_filename ; return do_execve ( init_filename , argv_init , envp_init ); }","title":"Entry Point"},{"location":"lego/kernel/loader/#memory-managers-job","text":"Memory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass start_ip and start_stack back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand.","title":"Memory Manager's Job"},{"location":"lego/kernel/loader/#load-ld-linux","text":"For dynamically-linked images, kernel ELF loader needs to load the ld-linux.so as well. It will first try to map the ld-linux.so into this process\u2019s virtual address space. Furthermore, the first user instruction that will run is no longer __libc_main_start , kernel will transfer the kernel to ld-linux.so instead. Thus, for a normal user program, ld-linux.so will load all the shared libraries before running glibc. static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... /* Dynamically-linked */ if ( elf_interpreter ) { unsigned long interp_map_addr = 0 ; elf_entry = load_elf_interp ( tsk , & loc -> interp_elf_ex , interpreter , & interp_map_addr , load_bias , interp_elf_phdata ); if ( ! IS_ERR (( void * ) elf_entry )) { /* * load_elf_interp() returns relocation * adjustment */ interp_load_addr = elf_entry ; elf_entry += loc -> interp_elf_ex . e_entry ; } if ( BAD_ADDR ( elf_entry )) { retval = IS_ERR (( void * ) elf_entry ) ? ( int ) elf_entry : - EINVAL ; goto out_free_dentry ; } reloc_func_desc = interp_load_addr ; put_lego_file ( interpreter ); kfree ( elf_interpreter ); } else { /* Statically-linked */ /* * e_entry is the VA to which the system first transfers control * Not the start_code! Normally, it is the <_start> function. */ elf_entry = loc -> elf_ex . e_entry ; if ( BAD_ADDR ( elf_entry )) { retval = - EINVAL ; goto out_free_dentry ; } } ... }","title":"Load ld-linux"},{"location":"lego/kernel/loader/#processor-managers-job","text":"It needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run flush_old_exec() , and setup_new_exec() .","title":"Processor Manager's Job"},{"location":"lego/kernel/loader/#destroy-old-context-flush_old_exec","text":"","title":"Destroy old context: flush_old_exec()"},{"location":"lego/kernel/loader/#zap-other-threads","text":"de_thread is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A exec starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader.","title":"Zap other threads"},{"location":"lego/kernel/loader/#switch-to-new-address-space","text":"We also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache: static int exec_mmap ( void ) { struct mm_struct * new_mm ; struct mm_struct * old_mm ; struct task_struct * tsk ; new_mm = mm_alloc (); if ( ! new_mm ) return - ENOMEM ; tsk = current ; old_mm = current -> mm ; mm_release ( tsk , old_mm ); task_lock ( tsk ); tsk -> mm = new_mm ; tsk -> active_mm = new_mm ; activate_mm ( old_mm , new_mm ); task_unlock ( tsk ); if ( old_mm ) mmput ( old_mm ); return 0 ; }","title":"Switch to new address space"},{"location":"lego/kernel/loader/#clear-architecture-specific-state","text":"This is performed by flush_thread() , which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array: void flush_thread ( void ) { struct task_struct * tsk = current ; memset ( tsk -> thread . tls_array , 0 , sizeof ( tsk -> thread . tls_array )); fpu__clear ( & tsk -> thread . fpu ); }","title":"Clear Architecture-Specific state"},{"location":"lego/kernel/loader/#setup-new-context-setup_new_exec","text":"Lego\u2019s setup_new_exec() is quite different from Linux\u2019s default implementation. Lego moves several functions to memory component, like the arch_pick_mmap_layout stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff: static void setup_new_exec ( const char * filename ) { /* This is the point of no return */ current -> sas_ss_sp = current -> sas_ss_size = 0 ; set_task_comm ( current , kbasename ( filename )); flush_signal_handlers ( current , 0 ); }","title":"Setup new context: setup_new_exec()"},{"location":"lego/kernel/loader/#change-return-frame-in-stack","text":"We do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail). /** * start_thread - Starting a new user thread * @regs: pointer to pt_regs * @new_ip: the first instruction IP of user thread * @new_sp: the new stack pointer of user thread */ void start_thread ( struct pt_regs * regs , unsigned long new_ip , unsigned long new_sp ) { loadsegment ( fs , 0 ); loadsegment ( es , 0 ); loadsegment ( ds , 0 ); load_gs_index ( 0 ); regs -> ip = new_ip ; regs -> sp = new_sp ; regs -> cs = __USER_CS ; regs -> ss = __USER_DS ; regs -> flags = X86_EFLAGS_IF ; } If calling execve() from userspace, the return frame is saved in the stack, we can simply do start_thread above, and merge to syscall return path. However, if calling execve() from a kernel thread, things changed. As you can see, all forked threads will run from ret_from_fork when it wakes for the first time. If it is a kernel thread, it jumps to line 23 , to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an execve() , and return frame has been changed by start_thread() . So we jump to line 16 to let it merge to syscall return path. /* * A newly forked process directly context switches into this address. * * rax: prev task we switched from * rbx: kernel thread func (NULL for user thread) * r12: kernel thread arg */ ENTRY ( ret_from_fork ) movq %rax , %rdi call schedule_tail /* rdi: 'prev' task parameter */ testq %rbx , %rbx /* from kernel_thread? */ jnz 1 f /* kernel threads are uncommon */ 2: movq %rsp , %rdi call syscall_return_slowpath /* return with IRQs disabled */ SWAPGS /* switch to user gs.base */ jmp restore_regs_and_iret 1: /* kernel thread */ movq %r12 , %rdi call * %rbx /* * A kernel thread is allowed to return here after successfully * calling do_execve(). Exit to userspace to complete the execve() * syscall: */ movq $0 , RAX ( %rsp ) jmp 2 b END ( ret_from_fork ) This is such a typical control flow hijacking. :-)","title":"Change return frame in stack"},{"location":"lego/kernel/loader/#features","text":"This section lists various features, or behaviors and Lego\u2019s program loader.","title":"Features"},{"location":"lego/kernel/loader/#virtual-address-space-range","text":"User\u2019s virtual address falls into this range: [sysctl_mmap_min_addr, TASK_SIZE) By default, unsigned long sysctl_mmap_min_addr = PAGE_SIZE ; /* * User space process size. 47bits minus one guard page. The guard * page is necessary on Intel CPUs: if a SYSCALL instruction is at * the highest possible canonical userspace address, then that * syscall will enter the kernel with a non-canonical return * address, and SYSRET will explode dangerously. We avoid this * particular problem by preventing anything from being mapped * at the maximum canonical address. */ #define TASK_SIZE ((1UL << 47) - PAGE_SIZE) Essentially: [0x1000, 0x7ffffffff000)","title":"Virtual Address Space Range"},{"location":"lego/kernel/loader/#pre-populated-bss-and-brk","text":"The heap vma created at loading time is a combination of .bss and .brk segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just .bss pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by vm_brk() . int vm_brk ( struct lego_task_struct * tsk , unsigned long start , unsigned long len ) { int ret ; struct lego_mm_struct * mm = tsk -> mm ; if ( down_write_killable ( & mm -> mmap_sem )) return - EINTR ; ret = do_brk ( tsk , start , len ); up_write ( & mm -> mmap_sem ); /* Prepopulate brk pages */ if ( ! ret ) lego_mm_populate ( mm , start , len ); return ret ; }","title":"Pre-Populated .bss and .brk"},{"location":"lego/kernel/loader/#un-populated-stack","text":"Stack vma is manually expanded to 32 pages + pages for argv info by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be: 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] The code to expand stack is done when ELF loader tries to finalize the stack vma, by calling setup_arg_pages() : int setup_arg_pages ( struct lego_task_struct * tsk , struct lego_binprm * bprm , unsigned long stack_top , int executable_stack ) { ... /* * 32*4k (or 2*64k) pages */ stack_expand = 131072UL ; stack_size = vma -> vm_end - vma -> vm_start ; stack_base = vma -> vm_start - stack_expand ; mm -> start_stack = bprm -> p ; ret = expand_stack ( vma , stack_base ); ... }","title":"Un-Populated stack"},{"location":"lego/kernel/loader/#un-populated-text-and-data","text":"In essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data\u2019s on-demand paging, we can prefault them in the future.","title":"Un-Populated .text and .data"},{"location":"lego/kernel/loader/#disabled-randomized-top-of-stack","text":"Lego currently does not randomize the stack top. The stack vma is allocated by bprm_mm_init() at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just TASK_SIZE : static int __bprm_mm_init ( struct lego_binprm * bprm ) { ... vma -> vm_end = TASK_SIZE ; ... } ( managers / memory / loader / elf . c ) Top of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer\u2019s setup_arg_pages() function, which is used to finalize the top of stack: int setup_arg_pages ( struct lego_task_struct * tsk , struct lego_binprm * bprm , unsigned long stack_top , int executable_stack ); So, to actually randomize the top of stack, you can simply do the following: static unsigned long randomize_stack_top ( unsigned long stack_top ) { unsigned long random_variable = 0 ; if (( current -> flags & PF_RANDOMIZE ) && ! ( current -> personality & ADDR_NO_RANDOMIZE )) { random_variable = get_random_long (); random_variable &= STACK_RND_MASK ; random_variable <<= PAGE_SHIFT ; } #ifdef CONFIG_STACK_GROWSUP return PAGE_ALIGN ( stack_top ) + random_variable ; #else return PAGE_ALIGN ( stack_top ) - random_variable ; #endif } static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... retval = setup_arg_pages ( bprm , randomize_stack_top ( TASK_SIZE ), executable_stack ); ... } However, current Lego disables randomization by passing TASK_SIZE : static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... retval = setup_arg_pages ( tsk , bprm , TASK_SIZE , executable_stack ); ... } ( managers / memory / loader / elf . c )","title":"Disabled Randomized Top of Stack"},{"location":"lego/kernel/loader/#no-vdso","text":"Currently, Lego does not have vDSO support. There are not too many syscalls mapped in the vDSO, for x86-64 : clock_gettime getcpu gettimeofday time The reason to add it back is simple: if those syscalls are used a lot and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO must be mapped at processor side, mapped in our emulated pgtable. Below is the original part where loader maps vDSO: static int load_elf_binary ( struct lego_task_struct * tsk , struct lego_binprm * bprm , u64 * new_ip , u64 * new_sp , unsigned long * argv_len , unsigned long * envp_len ) { ... #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES /* * TODO: vdso * x86 can map vdso vma here */ #endif ... } managers / memory / loader / elf . c For lego, we should move it to processor right before start_thread() : int do_execve ( const char * filename , const char * const * argv , const char * const * envp ) { ... /* Should be here */ start_thread ( regs , new_ip , new_sp ); ... } Besides, don\u2019t forget to report the vDSO address in the aux vector: static int create_elf_tables ( struct lego_task_struct * tsk , struct lego_binprm * bprm , struct elfhdr * exec , unsigned long load_addr , unsigned long interp_load_addr , unsigned long * argv_len , unsigned long * envp_len ) { ... #ifdef ARCH_DLINFO /* * ARCH_DLINFO must come first so PPC can do its special alignment of * AUXV. * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in * ARCH_DLINFO changes */ ARCH_DLINFO ; #endif ... } \u2013 Yizhou Shan Created: Feb 16, 2018 Last Updated: Feb 27, 2018","title":"No vDSO"},{"location":"lego/kernel/net_thpool/","text":"Thread Pool Model for Handling Network Requests \u00b6 Passive : whenever a network request comes in, callback to thpool. Active : thpool keep polling if there is new network requests queued. Previously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One ib_send_reply RRT drops from ~20us to a normal ~6us for a TensorFlow run. Never thought this could make such a big difference (~3x slowdown)! Dark network! \u2013 Yizhou Shan Created: April 29, 2018 Last Updated: April 29, 2018","title":"Network Thpool"},{"location":"lego/kernel/net_thpool/#thread-pool-model-for-handling-network-requests","text":"Passive : whenever a network request comes in, callback to thpool. Active : thpool keep polling if there is new network requests queued. Previously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One ib_send_reply RRT drops from ~20us to a normal ~6us for a TensorFlow run. Never thought this could make such a big difference (~3x slowdown)! Dark network! \u2013 Yizhou Shan Created: April 29, 2018 Last Updated: April 29, 2018","title":"Thread Pool Model for Handling Network Requests"},{"location":"lego/kernel/pagefault_disable/","text":"The story of pagefault_disable/enable \u00b6 pagefault_disable() is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from user virtual address . Please note the difference between user virtual address and user mode fault . The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3). If pgfault is disabled, then do_page_fault() function will NOT try to solve the pgfault by calling into pcache , instead, it will go straight to fixup code (in no_context()). This function is not intended to be used standalone. Normally, we do 1) pagefault_disable() , 2) then use some functions that have fixup code, 3) then pagefault_enable() . (The fixup code is another magic inside kernel. We will cover it in another document.) Currently in Lego, this is only used by futex , which needs something like atomic_cmpxchg() with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network. However, do note the difference with uaccess family functions. Most uaccess functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a SEGFAULT , pgfault code will go into fixup code. And that, my friend, is where uaccess returns -EFAULT to caller. \u2013 Yizhou Shan Feb 01, 2018","title":"Disable pgfault"},{"location":"lego/kernel/pagefault_disable/#the-story-of-pagefault_disableenable","text":"pagefault_disable() is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from user virtual address . Please note the difference between user virtual address and user mode fault . The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3). If pgfault is disabled, then do_page_fault() function will NOT try to solve the pgfault by calling into pcache , instead, it will go straight to fixup code (in no_context()). This function is not intended to be used standalone. Normally, we do 1) pagefault_disable() , 2) then use some functions that have fixup code, 3) then pagefault_enable() . (The fixup code is another magic inside kernel. We will cover it in another document.) Currently in Lego, this is only used by futex , which needs something like atomic_cmpxchg() with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network. However, do note the difference with uaccess family functions. Most uaccess functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a SEGFAULT , pgfault code will go into fixup code. And that, my friend, is where uaccess returns -EFAULT to caller. \u2013 Yizhou Shan Feb 01, 2018","title":"The story of pagefault_disable/enable"},{"location":"lego/kernel/profile/","text":"Lego Profilers \u00b6 Lego has three runtime profilers in kernel: strace heatmap profile points Combined together, they can provide the following information. Sweet, huh? [ 1017.047366 ] Kernel strace [ 1017.050276 ] Task : 20 : 20 nr_accumulated_threads : 46 [ 1017.055837 ] % time seconds usecs / call calls errors syscall [ 1017.063213 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.071648 ] 98.16 33.839597842 1879978 18 0 sys_futex [ 1017.079406 ] 0.26 0.260143997 260144 1 0 sys_execve [ 1017.087260 ] 0.18 0.185456860 7133 26 0 sys_write [ 1017.095017 ] 0.50 0.050189546 913 55 0 sys_munmap [ 1017.102870 ] 0.25 0.025223661 255 99 0 sys_mmap [ 1017.110531 ] 0.50 0.000505134 12 45 0 sys_clone [ 1017.118288 ] 0.20 0.000202327 26 8 0 sys_read [ 1017.125947 ] 0.14 0.000144065 17 9 0 sys_open [ 1017.133608 ] 0.67 0.000067251 7 11 0 sys_brk [ 1017.141171 ] 0.30 0.000030361 7 5 0 sys_newfstat [ 1017.149219 ] 0.64 0.000006410 1 9 0 sys_close [ 1017.156976 ] 0.48 0.000004842 1 45 0 sys_madvise [ 1017.164927 ] 0.34 0.000003443 1 47 0 sys_set_robust_list [ 1017.173653 ] 0.21 0.000002137 1 52 0 sys_mprotect [ 1017.181702 ] 0.71 0.000000717 1 4 0 sys_gettimeofday [ 1017.190137 ] 0.60 0.000000608 1 3 0 sys_time [ 1017.197797 ] 0.51 0.000000513 1 2 0 sys_getrlimit [ 1017.205942 ] 0.49 0.000000498 1 2 0 sys_rt_sigprocmask [ 1017.214572 ] 0.46 0.000000469 1 4 0 sys_rt_sigaction [ 1017.223008 ] 0.45 0.000000453 1 2 0 sys_arch_prctl [ 1017.231249 ] 0.27 0.000000272 1 2 0 sys_newuname [ 1017.239298 ] 0.13 0.000000135 1 2 0 sys_set_tid_address [ 1017.248025 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.256460 ] 100.00 34.361581541 451 0 total [ 1017.263830 ] [ 1017.308295 ] [ 1017.309754 ] Kernel Heatmap ( top # 10 ) [ 1017.313731 ] Address Function NR % [ 1017.321294 ] ---------------- -------------------- ---------- --------- [ 1017.328858 ] ffffffff8101a600 cpu_idle 112082 73.11 [ 1017.336421 ] ffffffff810666f0 __schedule 19192 12.95 [ 1017.343983 ] ffffffff8104f500 mlx4_ib_poll_cq 5551 3.99 [ 1017.351546 ] ffffffff8103bf50 delay_tsc 5393 3.83 [ 1017.359110 ] ffffffff81034a10 victim_flush_async 3766 2.72 [ 1017.366673 ] ffffffff8102b220 slob_alloc . constpro 1992 1.47 [ 1017.374235 ] ffffffff810668d0 schedule 1519 0.15 [ 1017.381800 ] ffffffff810648f0 fit_send_reply_with 956 0.95 [ 1017.389362 ] ffffffff81062370 ibapi_send_reply_ti 307 0.30 [ 1017.396925 ] ffffffff8105a0d0 ib_mad_completion_h 232 0.23 [ 1017.404487 ] ---------------- -------------------- ---------- --------- [ 1017.412052 ] 151994 100.00 [ 1017.419613 ] [ 1017.421267 ] [ 1017.422911 ] Kernel Profile Points [ 1017.426594 ] status name total nr avg . ns [ 1017.436292 ] ------- -------------------- ---------------- ---------------- ---------------- [ 1017.445988 ] off flush_tlb_others 0.000153470 55 2791 [ 1017.455685 ] off pcache_cache_miss 16.147020152 274698 58781 [ 1017.465381 ] ------- -------------------- ---------------- ---------------- ----------------","title":"Profile"},{"location":"lego/kernel/profile/#lego-profilers","text":"Lego has three runtime profilers in kernel: strace heatmap profile points Combined together, they can provide the following information. Sweet, huh? [ 1017.047366 ] Kernel strace [ 1017.050276 ] Task : 20 : 20 nr_accumulated_threads : 46 [ 1017.055837 ] % time seconds usecs / call calls errors syscall [ 1017.063213 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.071648 ] 98.16 33.839597842 1879978 18 0 sys_futex [ 1017.079406 ] 0.26 0.260143997 260144 1 0 sys_execve [ 1017.087260 ] 0.18 0.185456860 7133 26 0 sys_write [ 1017.095017 ] 0.50 0.050189546 913 55 0 sys_munmap [ 1017.102870 ] 0.25 0.025223661 255 99 0 sys_mmap [ 1017.110531 ] 0.50 0.000505134 12 45 0 sys_clone [ 1017.118288 ] 0.20 0.000202327 26 8 0 sys_read [ 1017.125947 ] 0.14 0.000144065 17 9 0 sys_open [ 1017.133608 ] 0.67 0.000067251 7 11 0 sys_brk [ 1017.141171 ] 0.30 0.000030361 7 5 0 sys_newfstat [ 1017.149219 ] 0.64 0.000006410 1 9 0 sys_close [ 1017.156976 ] 0.48 0.000004842 1 45 0 sys_madvise [ 1017.164927 ] 0.34 0.000003443 1 47 0 sys_set_robust_list [ 1017.173653 ] 0.21 0.000002137 1 52 0 sys_mprotect [ 1017.181702 ] 0.71 0.000000717 1 4 0 sys_gettimeofday [ 1017.190137 ] 0.60 0.000000608 1 3 0 sys_time [ 1017.197797 ] 0.51 0.000000513 1 2 0 sys_getrlimit [ 1017.205942 ] 0.49 0.000000498 1 2 0 sys_rt_sigprocmask [ 1017.214572 ] 0.46 0.000000469 1 4 0 sys_rt_sigaction [ 1017.223008 ] 0.45 0.000000453 1 2 0 sys_arch_prctl [ 1017.231249 ] 0.27 0.000000272 1 2 0 sys_newuname [ 1017.239298 ] 0.13 0.000000135 1 2 0 sys_set_tid_address [ 1017.248025 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.256460 ] 100.00 34.361581541 451 0 total [ 1017.263830 ] [ 1017.308295 ] [ 1017.309754 ] Kernel Heatmap ( top # 10 ) [ 1017.313731 ] Address Function NR % [ 1017.321294 ] ---------------- -------------------- ---------- --------- [ 1017.328858 ] ffffffff8101a600 cpu_idle 112082 73.11 [ 1017.336421 ] ffffffff810666f0 __schedule 19192 12.95 [ 1017.343983 ] ffffffff8104f500 mlx4_ib_poll_cq 5551 3.99 [ 1017.351546 ] ffffffff8103bf50 delay_tsc 5393 3.83 [ 1017.359110 ] ffffffff81034a10 victim_flush_async 3766 2.72 [ 1017.366673 ] ffffffff8102b220 slob_alloc . constpro 1992 1.47 [ 1017.374235 ] ffffffff810668d0 schedule 1519 0.15 [ 1017.381800 ] ffffffff810648f0 fit_send_reply_with 956 0.95 [ 1017.389362 ] ffffffff81062370 ibapi_send_reply_ti 307 0.30 [ 1017.396925 ] ffffffff8105a0d0 ib_mad_completion_h 232 0.23 [ 1017.404487 ] ---------------- -------------------- ---------- --------- [ 1017.412052 ] 151994 100.00 [ 1017.419613 ] [ 1017.421267 ] [ 1017.422911 ] Kernel Profile Points [ 1017.426594 ] status name total nr avg . ns [ 1017.436292 ] ------- -------------------- ---------------- ---------------- ---------------- [ 1017.445988 ] off flush_tlb_others 0.000153470 55 2791 [ 1017.455685 ] off pcache_cache_miss 16.147020152 274698 58781 [ 1017.465381 ] ------- -------------------- ---------------- ---------------- ----------------","title":"Lego Profilers"},{"location":"lego/kernel/profile_heatmap/","text":"Lego Profile Kernel Heatmap \u00b6 To get a sense of what is the hottest function within kernel, Lego adds a counter based heatmap. It is the same with Linux\u2019s /proc/profile . Mechanism \u00b6 General idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter. However, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap. Thus, kernel\u2019s solutions are: Coarse granularity : maintain a counter for each 1<<prof_shift bytes. Update counter on timer interrupt tick , which is a constant stable entry. Supported Features \u00b6 Currently, we only support CPU_PROFILING , which profile on each timer interrupt tick. We could also add SCHED_PROFILING , or SLEEP_PROFILING . But we are fine with current setting. Of course, we also have a simple dump function void print_profile_heatmap_nr(int nr) , which is similar to userspace tool readprofile . Example Output \u00b6 Workload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out cpu_idle() ) [ 1017.309754 ] Kernel Heatmap ( top # 10 ) [ 1017.313731 ] Address Function NR % [ 1017.321294 ] ---------------- -------------------- ---------- --------- [ 1017.328858 ] ffffffff8101a600 cpu_idle 112082 73.11 [ 1017.336421 ] ffffffff810666f0 __schedule 19192 12.95 [ 1017.343983 ] ffffffff8104f500 mlx4_ib_poll_cq 5551 3.99 [ 1017.351546 ] ffffffff8103bf50 delay_tsc 5393 3.83 [ 1017.359110 ] ffffffff81034a10 victim_flush_async 3766 2.72 [ 1017.366673 ] ffffffff8102b220 slob_alloc . constpro 1992 1.47 [ 1017.374235 ] ffffffff810668d0 schedule 1519 0.15 [ 1017.381800 ] ffffffff810648f0 fit_send_reply_with 956 0.95 [ 1017.389362 ] ffffffff81062370 ibapi_send_reply_ti 307 0.30 [ 1017.396925 ] ffffffff8105a0d0 ib_mad_completion_h 232 0.23 [ 1017.404487 ] ---------------- -------------------- ---------- --------- [ 1017.412052 ] 151994 100.00 [ 1017.419613 ] \u2013 Yizhou Shan Created: April 06, 2018 Last Updated: April 06, 2018","title":"Profile heatmap"},{"location":"lego/kernel/profile_heatmap/#lego-profile-kernel-heatmap","text":"To get a sense of what is the hottest function within kernel, Lego adds a counter based heatmap. It is the same with Linux\u2019s /proc/profile .","title":"Lego Profile Kernel Heatmap"},{"location":"lego/kernel/profile_heatmap/#mechanism","text":"General idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter. However, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap. Thus, kernel\u2019s solutions are: Coarse granularity : maintain a counter for each 1<<prof_shift bytes. Update counter on timer interrupt tick , which is a constant stable entry.","title":"Mechanism"},{"location":"lego/kernel/profile_heatmap/#supported-features","text":"Currently, we only support CPU_PROFILING , which profile on each timer interrupt tick. We could also add SCHED_PROFILING , or SLEEP_PROFILING . But we are fine with current setting. Of course, we also have a simple dump function void print_profile_heatmap_nr(int nr) , which is similar to userspace tool readprofile .","title":"Supported Features"},{"location":"lego/kernel/profile_heatmap/#example-output","text":"Workload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out cpu_idle() ) [ 1017.309754 ] Kernel Heatmap ( top # 10 ) [ 1017.313731 ] Address Function NR % [ 1017.321294 ] ---------------- -------------------- ---------- --------- [ 1017.328858 ] ffffffff8101a600 cpu_idle 112082 73.11 [ 1017.336421 ] ffffffff810666f0 __schedule 19192 12.95 [ 1017.343983 ] ffffffff8104f500 mlx4_ib_poll_cq 5551 3.99 [ 1017.351546 ] ffffffff8103bf50 delay_tsc 5393 3.83 [ 1017.359110 ] ffffffff81034a10 victim_flush_async 3766 2.72 [ 1017.366673 ] ffffffff8102b220 slob_alloc . constpro 1992 1.47 [ 1017.374235 ] ffffffff810668d0 schedule 1519 0.15 [ 1017.381800 ] ffffffff810648f0 fit_send_reply_with 956 0.95 [ 1017.389362 ] ffffffff81062370 ibapi_send_reply_ti 307 0.30 [ 1017.396925 ] ffffffff8105a0d0 ib_mad_completion_h 232 0.23 [ 1017.404487 ] ---------------- -------------------- ---------- --------- [ 1017.412052 ] 151994 100.00 [ 1017.419613 ] \u2013 Yizhou Shan Created: April 06, 2018 Last Updated: April 06, 2018","title":"Example Output"},{"location":"lego/kernel/profile_points/","text":"Lego Profile Points \u00b6 Lego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore. Example \u00b6 To trace TLB shootdown cost. DEFINE_PROFILE_POINT ( flush_tlb_others ) void flush_tlb_others ( const struct cpumask * cpumask , struct mm_struct * mm , unsigned long start , unsigned long end ) { struct flush_tlb_info info ; PROFILE_POINT_TIME ( flush_tlb_others ) if ( end == 0 ) end = start + PAGE_SIZE ; info . flush_mm = mm ; info . flush_start = start ; info . flush_end = end ; profile_point_start ( flush_tlb_others ); smp_call_function_many ( cpumask , flush_tlb_func , & info , 1 ); profile_point_leave ( flush_tlb_others ); } Explanation: DEFINE_PROFILE_POINT() will define a local structure, that contains the profile point name, number of invoked times, and total execution time. PROFILE_POINT_TIME() will define a stack local variable, to save the starting time. profile_point_start() will save the current time in nanosecond, while profile_point_leave() will calculate the execution of this run, and update the global counters defined by DEFINE_PROFILE_POINT() . System-wide profile points will be printed together if you invoke print_profile_points() : [ 1017.422911 ] Kernel Profile Points [ 1017.426594 ] status name total nr avg . ns [ 1017.436292 ] ------- -------------------- ---------------- ---------------- ---------------- [ 1017.445988 ] off flush_tlb_others 0.000153470 55 2791 [ 1017.455685 ] off pcache_cache_miss 16.147020152 274698 58781 [ 1017.465381 ] ------- -------------------- ---------------- ---------------- ---------------- Mechanism \u00b6 Once again, the profile points are aggregated by linker script. Each profile point will be in a special section .profile.point . The linker will merge them into one section, and export the starting and ending address of this section. Part I. Annotate. #define __profile_point __section(.profile.point) #define DEFINE_PROFILE_POINT(name) \\ struct profile_point _PP_NAME(name) __profile_point = { ... ... }; Part II. Link script merge. . = ALIGN ( L1_CACHE_BYTES ); . profile . point : AT ( ADDR (. profile . point ) - LOAD_OFFSET ) { __sprofilepoint = .; * (. profile . point ) __eprofilepoint = .; } Part III. Walk through. void print_profile_points ( void ) { struct profile_point * pp ; for ( pp = __sprofilepoint ; pp < __eprofilepoint ; pp ++ ) { print_profile_point ( pp ); ... } I really love the linker script. ;-) \u2013 Yizhou Shan Created: April 06, 2018 Last Updated: April 06, 2018","title":"Profile points"},{"location":"lego/kernel/profile_points/#lego-profile-points","text":"Lego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore.","title":"Lego Profile Points"},{"location":"lego/kernel/profile_points/#example","text":"To trace TLB shootdown cost. DEFINE_PROFILE_POINT ( flush_tlb_others ) void flush_tlb_others ( const struct cpumask * cpumask , struct mm_struct * mm , unsigned long start , unsigned long end ) { struct flush_tlb_info info ; PROFILE_POINT_TIME ( flush_tlb_others ) if ( end == 0 ) end = start + PAGE_SIZE ; info . flush_mm = mm ; info . flush_start = start ; info . flush_end = end ; profile_point_start ( flush_tlb_others ); smp_call_function_many ( cpumask , flush_tlb_func , & info , 1 ); profile_point_leave ( flush_tlb_others ); } Explanation: DEFINE_PROFILE_POINT() will define a local structure, that contains the profile point name, number of invoked times, and total execution time. PROFILE_POINT_TIME() will define a stack local variable, to save the starting time. profile_point_start() will save the current time in nanosecond, while profile_point_leave() will calculate the execution of this run, and update the global counters defined by DEFINE_PROFILE_POINT() . System-wide profile points will be printed together if you invoke print_profile_points() : [ 1017.422911 ] Kernel Profile Points [ 1017.426594 ] status name total nr avg . ns [ 1017.436292 ] ------- -------------------- ---------------- ---------------- ---------------- [ 1017.445988 ] off flush_tlb_others 0.000153470 55 2791 [ 1017.455685 ] off pcache_cache_miss 16.147020152 274698 58781 [ 1017.465381 ] ------- -------------------- ---------------- ---------------- ----------------","title":"Example"},{"location":"lego/kernel/profile_points/#mechanism","text":"Once again, the profile points are aggregated by linker script. Each profile point will be in a special section .profile.point . The linker will merge them into one section, and export the starting and ending address of this section. Part I. Annotate. #define __profile_point __section(.profile.point) #define DEFINE_PROFILE_POINT(name) \\ struct profile_point _PP_NAME(name) __profile_point = { ... ... }; Part II. Link script merge. . = ALIGN ( L1_CACHE_BYTES ); . profile . point : AT ( ADDR (. profile . point ) - LOAD_OFFSET ) { __sprofilepoint = .; * (. profile . point ) __eprofilepoint = .; } Part III. Walk through. void print_profile_points ( void ) { struct profile_point * pp ; for ( pp = __sprofilepoint ; pp < __eprofilepoint ; pp ++ ) { print_profile_point ( pp ); ... } I really love the linker script. ;-) \u2013 Yizhou Shan Created: April 06, 2018 Last Updated: April 06, 2018","title":"Mechanism"},{"location":"lego/kernel/profile_strace/","text":"Lego Profile strace \u00b6 Lego has a built-in kernel-version syscall tracer, similar to strace utility in the user space. Below we will just call our Lego\u2019s syscall tracer as strace for simplicity. Design \u00b6 There are essentially three important metrics to track for each syscall number of times invoked number of times error happened total execution, or per-call latency Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons: Performance: set of counters are atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications. Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores. Below is our simple design, where each thread has a struct strace_info , which include a set of counters for each syscall. All strace_info within a process are chained together by a doubly-linked list. When we want to look at the strace statistic numbers, we need to accumulate counters from all threads within a process, including those dead threads. We do the accumulate when the last thread of this process is going to exit. The benefit of doubly-linked strace_info is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if task_struct is reaped, strace_info is still there and linked. For example, assume thread_3 has a SIGSEGV, and did a zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing. Details \u00b6 There are essentially three hooks in core kernel: syscall : before and after sys_call_table fork/clone : create strace_info for each thread do_exit() : when group_dead(signal->live==1), accumulate Example Output \u00b6 [ 1017.047366 ] Kernel strace [ 1017.050276 ] Task : 20 : 20 nr_accumulated_threads : 46 [ 1017.055837 ] % time seconds usecs / call calls errors syscall [ 1017.063213 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.071648 ] 98.16 33.839597842 1879978 18 0 sys_futex [ 1017.079406 ] 0.26 0.260143997 260144 1 0 sys_execve [ 1017.087260 ] 0.18 0.185456860 7133 26 0 sys_write [ 1017.095017 ] 0.50 0.050189546 913 55 0 sys_munmap [ 1017.102870 ] 0.25 0.025223661 255 99 0 sys_mmap [ 1017.110531 ] 0.50 0.000505134 12 45 0 sys_clone [ 1017.118288 ] 0.20 0.000202327 26 8 0 sys_read [ 1017.125947 ] 0.14 0.000144065 17 9 0 sys_open [ 1017.133608 ] 0.67 0.000067251 7 11 0 sys_brk [ 1017.141171 ] 0.30 0.000030361 7 5 0 sys_newfstat [ 1017.149219 ] 0.64 0.000006410 1 9 0 sys_close [ 1017.156976 ] 0.48 0.000004842 1 45 0 sys_madvise [ 1017.164927 ] 0.34 0.000003443 1 47 0 sys_set_robust_list [ 1017.173653 ] 0.21 0.000002137 1 52 0 sys_mprotect [ 1017.181702 ] 0.71 0.000000717 1 4 0 sys_gettimeofday [ 1017.190137 ] 0.60 0.000000608 1 3 0 sys_time [ 1017.197797 ] 0.51 0.000000513 1 2 0 sys_getrlimit [ 1017.205942 ] 0.49 0.000000498 1 2 0 sys_rt_sigprocmask [ 1017.214572 ] 0.46 0.000000469 1 4 0 sys_rt_sigaction [ 1017.223008 ] 0.45 0.000000453 1 2 0 sys_arch_prctl [ 1017.231249 ] 0.27 0.000000272 1 2 0 sys_newuname [ 1017.239298 ] 0.13 0.000000135 1 2 0 sys_set_tid_address [ 1017.248025 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.256460 ] 100.00 34.361581541 451 0 total \u2013 Yizhou Shan Created: April 05, 2018 Last Updated: April 05, 2018","title":"strace"},{"location":"lego/kernel/profile_strace/#lego-profile-strace","text":"Lego has a built-in kernel-version syscall tracer, similar to strace utility in the user space. Below we will just call our Lego\u2019s syscall tracer as strace for simplicity.","title":"Lego Profile strace"},{"location":"lego/kernel/profile_strace/#design","text":"There are essentially three important metrics to track for each syscall number of times invoked number of times error happened total execution, or per-call latency Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons: Performance: set of counters are atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications. Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores. Below is our simple design, where each thread has a struct strace_info , which include a set of counters for each syscall. All strace_info within a process are chained together by a doubly-linked list. When we want to look at the strace statistic numbers, we need to accumulate counters from all threads within a process, including those dead threads. We do the accumulate when the last thread of this process is going to exit. The benefit of doubly-linked strace_info is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if task_struct is reaped, strace_info is still there and linked. For example, assume thread_3 has a SIGSEGV, and did a zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.","title":"Design"},{"location":"lego/kernel/profile_strace/#details","text":"There are essentially three hooks in core kernel: syscall : before and after sys_call_table fork/clone : create strace_info for each thread do_exit() : when group_dead(signal->live==1), accumulate","title":"Details"},{"location":"lego/kernel/profile_strace/#example-output","text":"[ 1017.047366 ] Kernel strace [ 1017.050276 ] Task : 20 : 20 nr_accumulated_threads : 46 [ 1017.055837 ] % time seconds usecs / call calls errors syscall [ 1017.063213 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.071648 ] 98.16 33.839597842 1879978 18 0 sys_futex [ 1017.079406 ] 0.26 0.260143997 260144 1 0 sys_execve [ 1017.087260 ] 0.18 0.185456860 7133 26 0 sys_write [ 1017.095017 ] 0.50 0.050189546 913 55 0 sys_munmap [ 1017.102870 ] 0.25 0.025223661 255 99 0 sys_mmap [ 1017.110531 ] 0.50 0.000505134 12 45 0 sys_clone [ 1017.118288 ] 0.20 0.000202327 26 8 0 sys_read [ 1017.125947 ] 0.14 0.000144065 17 9 0 sys_open [ 1017.133608 ] 0.67 0.000067251 7 11 0 sys_brk [ 1017.141171 ] 0.30 0.000030361 7 5 0 sys_newfstat [ 1017.149219 ] 0.64 0.000006410 1 9 0 sys_close [ 1017.156976 ] 0.48 0.000004842 1 45 0 sys_madvise [ 1017.164927 ] 0.34 0.000003443 1 47 0 sys_set_robust_list [ 1017.173653 ] 0.21 0.000002137 1 52 0 sys_mprotect [ 1017.181702 ] 0.71 0.000000717 1 4 0 sys_gettimeofday [ 1017.190137 ] 0.60 0.000000608 1 3 0 sys_time [ 1017.197797 ] 0.51 0.000000513 1 2 0 sys_getrlimit [ 1017.205942 ] 0.49 0.000000498 1 2 0 sys_rt_sigprocmask [ 1017.214572 ] 0.46 0.000000469 1 4 0 sys_rt_sigaction [ 1017.223008 ] 0.45 0.000000453 1 2 0 sys_arch_prctl [ 1017.231249 ] 0.27 0.000000272 1 2 0 sys_newuname [ 1017.239298 ] 0.13 0.000000135 1 2 0 sys_set_tid_address [ 1017.248025 ] ------ -------------- ----------- --------- --------- ---------------- [ 1017.256460 ] 100.00 34.361581541 451 0 total \u2013 Yizhou Shan Created: April 05, 2018 Last Updated: April 05, 2018","title":"Example Output"},{"location":"lego/kernel/stop_machine/","text":"The highest priority thread in kernel \u00b6 This document is about migration/N kernel threads, stop_sched schdueling class, and the interesting source file kernel/stop_machine.c . Background on kernel scheduler design is recommended. Scheduler uses the following code to pick the next runnable task: static inline struct task_struct * pick_next_task ( struct rq * rq , struct task_struct * prev ) { struct task_struct * p ; const struct sched_class * class ; again : for_each_class ( class ) { p = class -> pick_next_task ( rq , prev ); if ( p ) { if ( unlikely ( p == RETRY_TASK )) goto again ; return p ; } } BUG (); } while the class is linked together as: #define sched_class_highest (&stop_sched_class) #define for_each_class(class) \\ for ( class = sched_class_highest ; class ; class = class -> next ) Clearly, the highest priority class is stop_sched_class . Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like migration/0 when you do ps aux in Linux. And yes, these kernel threads are the only users. These threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls sched_setaffinity() to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a running task out, it can only move queued task out). Then, scheduler has to ask migration/0 for help. Once there is a job enqueued, migration/0 will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast. migration code is defined in kernel/stop_machine.c . They are created during early boot. They use the smpboot_register_percpu_thread to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through kthread_create . Since Lego does not support cpu hotplug, and this cpu_stop_init is called after SMP is initialized, so Lego has slight different initialiaztion: void __init cpu_stop_init ( void ) { unsigned int cpu ; for_each_possible_cpu ( cpu ) { struct cpu_stopper * stopper = & per_cpu ( cpu_stopper , cpu ); spin_lock_init ( & stopper -> lock ); INIT_LIST_HEAD ( & stopper -> works ); } BUG_ON ( smpboot_register_percpu_thread ( & cpu_stop_threads )); /* * smpboot_create_threads use kthread_create_on_cpu() to * create new threads. And they are parked, too. * Since we call this function after smp_init(), all CPUs * are already online, thus we need to unpark them manually. */ for_each_online_cpu ( cpu ) stop_machine_unpark ( cpu ); Internally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.): static void cpu_stopper_thread ( unsigned int cpu ) { struct cpu_stopper * stopper = & per_cpu ( cpu_stopper , cpu ); struct cpu_stop_work * work ; repeat : work = NULL ; spin_lock_irq ( & stopper -> lock ); if ( ! list_empty ( & stopper -> works )) { work = list_first_entry ( & stopper -> works , struct cpu_stop_work , list ); list_del_init ( & work -> list ); } spin_unlock_irq ( & stopper -> lock ); if ( work ) { ... ret = fn ( arg ); ... goto repeat ; } } It has several interesting public APIs that are quite similar to smp_call_functions , but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU. int stop_one_cpu ( unsigned int cpu , cpu_stop_fn_t fn , void * arg ); int stop_cpus ( const struct cpumask * cpumask , cpu_stop_fn_t fn , void * arg ); int try_stop_cpus ( const struct cpumask * cpumask , cpu_stop_fn_t fn , void * arg ); They are used only when there are some very urgent things to do. So, please use with caution. \u2013 Yizhou Shan Created: Feb 12, 2018 Last Updated: Feb 12, 2018","title":"Stop machine"},{"location":"lego/kernel/stop_machine/#the-highest-priority-thread-in-kernel","text":"This document is about migration/N kernel threads, stop_sched schdueling class, and the interesting source file kernel/stop_machine.c . Background on kernel scheduler design is recommended. Scheduler uses the following code to pick the next runnable task: static inline struct task_struct * pick_next_task ( struct rq * rq , struct task_struct * prev ) { struct task_struct * p ; const struct sched_class * class ; again : for_each_class ( class ) { p = class -> pick_next_task ( rq , prev ); if ( p ) { if ( unlikely ( p == RETRY_TASK )) goto again ; return p ; } } BUG (); } while the class is linked together as: #define sched_class_highest (&stop_sched_class) #define for_each_class(class) \\ for ( class = sched_class_highest ; class ; class = class -> next ) Clearly, the highest priority class is stop_sched_class . Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like migration/0 when you do ps aux in Linux. And yes, these kernel threads are the only users. These threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls sched_setaffinity() to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a running task out, it can only move queued task out). Then, scheduler has to ask migration/0 for help. Once there is a job enqueued, migration/0 will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast. migration code is defined in kernel/stop_machine.c . They are created during early boot. They use the smpboot_register_percpu_thread to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through kthread_create . Since Lego does not support cpu hotplug, and this cpu_stop_init is called after SMP is initialized, so Lego has slight different initialiaztion: void __init cpu_stop_init ( void ) { unsigned int cpu ; for_each_possible_cpu ( cpu ) { struct cpu_stopper * stopper = & per_cpu ( cpu_stopper , cpu ); spin_lock_init ( & stopper -> lock ); INIT_LIST_HEAD ( & stopper -> works ); } BUG_ON ( smpboot_register_percpu_thread ( & cpu_stop_threads )); /* * smpboot_create_threads use kthread_create_on_cpu() to * create new threads. And they are parked, too. * Since we call this function after smp_init(), all CPUs * are already online, thus we need to unpark them manually. */ for_each_online_cpu ( cpu ) stop_machine_unpark ( cpu ); Internally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.): static void cpu_stopper_thread ( unsigned int cpu ) { struct cpu_stopper * stopper = & per_cpu ( cpu_stopper , cpu ); struct cpu_stop_work * work ; repeat : work = NULL ; spin_lock_irq ( & stopper -> lock ); if ( ! list_empty ( & stopper -> works )) { work = list_first_entry ( & stopper -> works , struct cpu_stop_work , list ); list_del_init ( & work -> list ); } spin_unlock_irq ( & stopper -> lock ); if ( work ) { ... ret = fn ( arg ); ... goto repeat ; } } It has several interesting public APIs that are quite similar to smp_call_functions , but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU. int stop_one_cpu ( unsigned int cpu , cpu_stop_fn_t fn , void * arg ); int stop_cpus ( const struct cpumask * cpumask , cpu_stop_fn_t fn , void * arg ); int try_stop_cpus ( const struct cpumask * cpumask , cpu_stop_fn_t fn , void * arg ); They are used only when there are some very urgent things to do. So, please use with caution. \u2013 Yizhou Shan Created: Feb 12, 2018 Last Updated: Feb 12, 2018","title":"The highest priority thread in kernel"},{"location":"lego/kernel/tlbflush/","text":"TLB Flush \u00b6 \u2013 Yizhou Shan Created: March 01, 2018 Last Updated: March 01, 2018","title":"TLB Flush"},{"location":"lego/kernel/tlbflush/#tlb-flush","text":"\u2013 Yizhou Shan Created: March 01, 2018 Last Updated: March 01, 2018","title":"TLB Flush"},{"location":"lego/kernel/trampoline/","text":"How trampoline works in Lego \u00b6 What is trampoline code? \u00b6 Trampoline code is used by BSP to boot other secondary CPUs. At startup, BSP wakeup secondary CPUs by sending a APIC INIT command, which carry the [start_ip] where the secondary CPUs should start to run. The trampoline code is the code starting from [start_ip] . Used by the secondary CPU to jump from 16-bit realmode to 64-bit code (the first instruction of 64-bit code will be in arch/x86/kernel/head_64.S ). Where is the trampoline source code? \u00b6 The source files are all in arch/x86/realmode/ . There are two parts: 1) arch/x86/realmode/rm/trampoline.S : which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..). 2) arch/x86/realmode/piggy.S : Since the trampoline code can not to linked into kernel image directly. So we have to piggyback the trampoline.bin binary code into a section, which is described by trampoline_start and trampoline_end . So the kernel can address the trampoline code via these two symbols. The compile flow is: arch/x86/realmode/rm/trmapoline.S -> CC__ arch/x86/realmode/rm/trmapoline.o -> LD arch/x86/realmode/rm/trampoline -> OBJCOPY arch/x86/realmode/rm/trampoline.bin -> This bin goes into piggy.o -> piggy.o goes into vmImage What happened at runtime? \u00b6 The setup code was loaded by GRUB below 1MB. Inside arch/x86/boot/main.c , we will save the cs() into the boot_params and pass it to kernel. In setup_arch() , we will copy the trampoline.bin code to the cs() address reported by boot_param . This means we will override setup code, which is okay. At last, we wake up the secondary CPUs inside smp_init() . Compare with Linux \u00b6 I vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities. \u2013 Yizhou Shan Mar 3, 2017","title":"Trampoline"},{"location":"lego/kernel/trampoline/#how-trampoline-works-in-lego","text":"","title":"How trampoline works in Lego"},{"location":"lego/kernel/trampoline/#what-is-trampoline-code","text":"Trampoline code is used by BSP to boot other secondary CPUs. At startup, BSP wakeup secondary CPUs by sending a APIC INIT command, which carry the [start_ip] where the secondary CPUs should start to run. The trampoline code is the code starting from [start_ip] . Used by the secondary CPU to jump from 16-bit realmode to 64-bit code (the first instruction of 64-bit code will be in arch/x86/kernel/head_64.S ).","title":"What is trampoline code?"},{"location":"lego/kernel/trampoline/#where-is-the-trampoline-source-code","text":"The source files are all in arch/x86/realmode/ . There are two parts: 1) arch/x86/realmode/rm/trampoline.S : which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..). 2) arch/x86/realmode/piggy.S : Since the trampoline code can not to linked into kernel image directly. So we have to piggyback the trampoline.bin binary code into a section, which is described by trampoline_start and trampoline_end . So the kernel can address the trampoline code via these two symbols. The compile flow is: arch/x86/realmode/rm/trmapoline.S -> CC__ arch/x86/realmode/rm/trmapoline.o -> LD arch/x86/realmode/rm/trampoline -> OBJCOPY arch/x86/realmode/rm/trampoline.bin -> This bin goes into piggy.o -> piggy.o goes into vmImage","title":"Where is the trampoline source code?"},{"location":"lego/kernel/trampoline/#what-happened-at-runtime","text":"The setup code was loaded by GRUB below 1MB. Inside arch/x86/boot/main.c , we will save the cs() into the boot_params and pass it to kernel. In setup_arch() , we will copy the trampoline.bin code to the cs() address reported by boot_param . This means we will override setup code, which is okay. At last, we wake up the secondary CPUs inside smp_init() .","title":"What happened at runtime?"},{"location":"lego/kernel/trampoline/#compare-with-linux","text":"I vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities. \u2013 Yizhou Shan Mar 3, 2017","title":"Compare with Linux"},{"location":"lego/kernel/vDSO/","text":"vDSO and vsyscall \u00b6 We have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features. References: https://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html \u2013 Yizhou Shan Created: March 01, 2018 Last Updated: March 01, 2018","title":"vDSO"},{"location":"lego/kernel/vDSO/#vdso-and-vsyscall","text":"We have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features. References: https://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html \u2013 Yizhou Shan Created: March 01, 2018 Last Updated: March 01, 2018","title":"vDSO and vsyscall"},{"location":"lego/kernel/vfs/","text":"Processor Manager\u2019s Virtual File System \u00b6 Lego processor manager has an virtual file system layer to accommodate the famous legacy Everything is a file philosophy. But we implement this in a very dirty way. Cover later. \u2013 Yizhou Shan Created: Feb 20, 2018 Last Updated: Feb 20, 2018","title":"Virtual File System"},{"location":"lego/kernel/vfs/#processor-managers-virtual-file-system","text":"Lego processor manager has an virtual file system layer to accommodate the famous legacy Everything is a file philosophy. But we implement this in a very dirty way. Cover later. \u2013 Yizhou Shan Created: Feb 20, 2018 Last Updated: Feb 20, 2018","title":"Processor Manager's Virtual File System"},{"location":"lego/kernel/vm/","text":"Process Virtual Memory \u00b6 Limits \u00b6 Max Number of VMAs \u00b6 By default, the maximum number of VMAs is: 65530 . It is defined by the following variable: #define MAPCOUNT_ELF_CORE_MARGIN (5) #define DEFAULT_MAX_MAP_COUNT (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN) int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT ; Facts \u00b6 munmap can split vma \u00b6 munmap can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that, munmap can create hole for both anonymous vma and file-backed vma. msync() is not atomic \u00b6 During msync() , pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure. msync() need concurrency control \u00b6 With a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under msync() . This implies that application need to handle concurrency by themselves, e.g., rwlocks. \u2013 Yizhou Shan Created: Feb 19, 2018 Last Updated: Feb 19, 2018","title":"Process Virtual Memory"},{"location":"lego/kernel/vm/#process-virtual-memory","text":"","title":"Process Virtual Memory"},{"location":"lego/kernel/vm/#limits","text":"","title":"Limits"},{"location":"lego/kernel/vm/#max-number-of-vmas","text":"By default, the maximum number of VMAs is: 65530 . It is defined by the following variable: #define MAPCOUNT_ELF_CORE_MARGIN (5) #define DEFAULT_MAX_MAP_COUNT (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN) int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT ;","title":"Max Number of VMAs"},{"location":"lego/kernel/vm/#facts","text":"","title":"Facts"},{"location":"lego/kernel/vm/#munmap-can-split-vma","text":"munmap can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that, munmap can create hole for both anonymous vma and file-backed vma.","title":"munmap can split vma"},{"location":"lego/kernel/vm/#msync-is-not-atomic","text":"During msync() , pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.","title":"msync() is not atomic"},{"location":"lego/kernel/vm/#msync-need-concurrency-control","text":"With a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under msync() . This implies that application need to handle concurrency by themselves, e.g., rwlocks. \u2013 Yizhou Shan Created: Feb 19, 2018 Last Updated: Feb 19, 2018","title":"msync() need concurrency control"},{"location":"lego/log/TODO/","text":"TODO \u00b6 Last Updated: July 18, 2018 Planned \u00b6 Try fully-associative pcache, to see how many conflict misses can be removed (got the idea from HPCA18 google search paper) kmem_cache TSC deadline mode (one-shot tick) . What is the performance comparison with periodic mode? batched TLB flush __unhash_process() : in exit, release pid etc. de_thread() : in exec, change pid etc. posix timers : used by exit(), wait() and others. Functions like posix_cpu_timers_exit_group . vDSO : if later we find applications are using gettimeofday , time , and getcpu a lot, and it truly hurt performance, then we should consider adding this in the processor side. (Check Processor Loader document for code that needs to be patched). (02/27/18) VA randomization : our loader does not add any randomization. For security reasons, we probably want to add this. VM Organization : multiple vm choice at M side, on a per-vma basis. fork: dup free pool : duplicate the free VA pool at both P and M. pcache : send each page\u2019s type back. something like PcacheAnon, PcacheFile. So the pcache_evict/do_exit routine can be optimized. mm alloc : don\u2019t use the kmalloc to get a new mm_struct. This is a hot data structure, use get_free_page instead maybe. Like task_struct. fork_dup_pcache : have real vm_flags to guide write-protect. Get vm ranges from memory to optimize the duplication. Currently, all pages will be downgraded to read-only. P side mm sem : check if we need the sem in P side. pgfault need read, fork and others need W. Even though M side also serialize this, but out ops are divided. mprotect : it is empty now. We assume applications are well-written. But does any of them rely on this COW feature? CPU_NO_HZ : disable timer for some cores, to reduce the overhead of timer interrupts. This is named CPU_NO_HZ and some similar Kconfigs. SYSCALL : compared with linux, we are always using the slow path, which pass all arguments. We should consider optimize this. OS-intensive applications may hurt. IB : reply is a sg list. Esp benefit pcache. Finished \u00b6 - vsyscall : mostly emulation-","title":"TODO"},{"location":"lego/log/TODO/#todo","text":"Last Updated: July 18, 2018","title":"TODO"},{"location":"lego/log/TODO/#planned","text":"Try fully-associative pcache, to see how many conflict misses can be removed (got the idea from HPCA18 google search paper) kmem_cache TSC deadline mode (one-shot tick) . What is the performance comparison with periodic mode? batched TLB flush __unhash_process() : in exit, release pid etc. de_thread() : in exec, change pid etc. posix timers : used by exit(), wait() and others. Functions like posix_cpu_timers_exit_group . vDSO : if later we find applications are using gettimeofday , time , and getcpu a lot, and it truly hurt performance, then we should consider adding this in the processor side. (Check Processor Loader document for code that needs to be patched). (02/27/18) VA randomization : our loader does not add any randomization. For security reasons, we probably want to add this. VM Organization : multiple vm choice at M side, on a per-vma basis. fork: dup free pool : duplicate the free VA pool at both P and M. pcache : send each page\u2019s type back. something like PcacheAnon, PcacheFile. So the pcache_evict/do_exit routine can be optimized. mm alloc : don\u2019t use the kmalloc to get a new mm_struct. This is a hot data structure, use get_free_page instead maybe. Like task_struct. fork_dup_pcache : have real vm_flags to guide write-protect. Get vm ranges from memory to optimize the duplication. Currently, all pages will be downgraded to read-only. P side mm sem : check if we need the sem in P side. pgfault need read, fork and others need W. Even though M side also serialize this, but out ops are divided. mprotect : it is empty now. We assume applications are well-written. But does any of them rely on this COW feature? CPU_NO_HZ : disable timer for some cores, to reduce the overhead of timer interrupts. This is named CPU_NO_HZ and some similar Kconfigs. SYSCALL : compared with linux, we are always using the slow path, which pass all arguments. We should consider optimize this. OS-intensive applications may hurt. IB : reply is a sg list. Esp benefit pcache.","title":"Planned"},{"location":"lego/log/TODO/#finished","text":"- vsyscall : mostly emulation-","title":"Finished"},{"location":"lego/log/log-02-2018/","text":"Feb 2018 \u00b6 02/28 Wed \u00b6 patch fork, and cow handler debug pcache, while running python hello world add vDSO, gettimeofday So, it is end of the day. After adding wp handler, I now have the whole picture of pcache activities, and the interactions between them. The reclaim, zap, move, copy, add, operations needs to be carefully synchronized. Also the refcount etc. I feel the ground rule is we need to make sure a PCM that a function is currently using, can not suddenly become invalid due to other operations. This has to be synced by: refcount, lock, flags. Oh well, mm is hard with SMP, but also fun. We are very close to have a fully working OS. I did not have time to look into the python hello world bug issue. It is a very serious one. It may also rule out some root bugs. 02/27 Tue \u00b6 Spent two days on CS527 source project, implemented a small SSHD and SSD client. And we have to inject exactly five bugs, or vulnerabilities into the systems. Lol, it is really hard to intentionally plant BUGs! Anyway, back to Lego. Since others are having a hard time compile program statically, I will try to add dynamic loader today. The interpreter: /lib64/ld-linux-x86-64.so.2 . Linux seq.c maps (no randomization): 00400000-00401000 r-xp 00000000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00600000-00601000 r--p 00000000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00601000-00602000 rw-p 00001000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00602000-00604000 rw-p 00000000 00:00 0 [heap] 7ffff7a18000-7ffff7bd0000 r-xp 00000000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7bd0000-7ffff7dd0000 ---p 001b8000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd0000-7ffff7dd4000 r--p 001b8000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd4000-7ffff7dd6000 rw-p 001bc000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd6000-7ffff7ddb000 rw-p 00000000 00:00 0 7ffff7ddb000-7ffff7dfc000 r-xp 00000000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7fde000-7ffff7fe1000 rw-p 00000000 00:00 0 7ffff7ff9000-7ffff7ffa000 rw-p 00000000 00:00 0 7ffff7ffa000-7ffff7ffc000 r-xp 00000000 00:00 0 [vdso] 7ffff7ffc000-7ffff7ffd000 r--p 00021000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7ffd000-7ffff7ffe000 rw-p 00022000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] lego after loading 00400000-00401000 r-xp 00000000 /root/ys/LegoOS/usr/a.out 00600000-00602000 rw-p 00000000 /root/ys/LegoOS/usr/a.out 00602000-00604000 rw-p 00000000 [heap] 7ffff7ddb000-7ffff7dfc000 r-xp 00000000 /lib64/ld-linux-x86-64.so.2 7ffff7ffc000-7ffff7ffe000 rw-p 00021000 /lib64/ld-linux-x86-64.so.2 7ffff7ffe000-7ffff7fff000 rw-p 00000000 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] [ 2066.379224] **** Finish dump final mm [ 2066.426023] handle_p2m_execve(): reply_status: OKAY, new_ip: 0x7ffff7ddc170, new_sp: 0x7fffffffede0 [ 2066.628949] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:150 vaddr:0x7ffff7ddc170 [ 2066.732034] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:150 vaddr:0x7ffff7ddc170 [ 2066.934947] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:51 vaddr:0x7fffffffedd8 [ 2067.036978] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:51 vaddr:0x7fffffffedd8 [ 2067.238842] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ffce00 [ 2067.340880] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ffce00 [ 2067.542747] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:51 vaddr:0x7ffff7ffd9a8 [ 2067.644774] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:51 vaddr:0x7ffff7ffd9a8 [ 2067.846640] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ddb8e0 [ 2067.948679] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ddb8e0 [ 2068.355424] ------------[ cut here ]------------ [ 2068.408568] WARNING: CPU: 4 PID: 31 at managers/memory/handle_pcache/fault.c:54 handle_p2m_pcache_miss+0x29d/0x380 [ 2068.532327] src_nid:0,pid:32,vaddr:0x7ffff7e0e000 [ 2068.588487] CPU: 4 PID: 31 Comm: mc-manager 4.0.0-lego-ys+ #100 [ 2068.659207] Stack: [root@wuklab13: lib64] $ ll ld-* -rwxr-xr-x 1 root root 164112 Nov 30 13:53 ld-2.17.so lrwxrwxrwx 1 root root 10 Jan 8 12:34 ld-linux-x86-64.so.2 -> ld-2.17.so [root@wuklab13: lib64] It turns out there is a bug in mmap code: forgot to increment the file ref count when a file-backed vma is created. Some put_file in loader accidentally free the ld-linux file. Bug fixed, dyloader works like a charm. 02/24 Sat \u00b6 Well. PhDs do not have weekends. Anyway, it is Saturday after all, relaxed a little bit. I was looking into the pcache issue. Also added our own kernel version strace. 02/23 Fri \u00b6 Solved FPU BUG \u00b6 current is fine. I should not compare the old implementation with the new per-cpu current. I forgot that the kernel stack is switched in the __switch_to_asm . This means in __switch_to() , we are actually using the next_p \u2018s kernel stack. So there is small time frame, where current_thread_info() points to next_p , while current_task is still prev_p . Since interrupts are disabled during context switch, we are good with this mismatch. Rule out current, the only thing left is fpu__copy warning, which happens during copy_process() . One weird thing is this function has been called multiple times before it showed a warning. System itself use this function to create a lot background threads, which are fine. Only when it was triggered by sys_clone then we have the warning: [ 3213.055639 ] CPU : 6 PID : 17 sys_clone + 0x0 / 0x30 [ 3213.056584 ] new task_struct : ffff88083e4c9838 [ 3213.057530 ] arch_dup_task_struct cpu6 dst : ffff88083e4c9838 17 word_count - seq src : ffff88083e457838 17 word_count - seq [ 3213.059536 ] TRAP do_general_protection in CPU6 , error_code : 0 current : ffff88083e457838 17 word_count - seq [ 3213.061289 ] fixup_exception pid ( 17 ) cpu ( 6 ) insn : 0xffffffff81009a21 ( fpu__copy + 0x81 / 0x260 ) fixup : 0xffffffff8105d9b2 ( __fixup_text_start + 0xc2 / 0x322 ) handler : ex_handler_default + 0x0 / 0x20 [ 3213.064114 ] ------------ [ cut here ] ------------ [ 3213.065040 ] WARNING : CPU : 6 PID : 17 at . / arch / x86 / include / asm / fpu / internal . h : 354 fpu__copy + 0xc3 / 0x260 [ 3213.066760 ] CPU : 6 PID : 17 Comm : word_count - seq 4.0.0 - lego + # 6 [ 3213.067855 ] Stack : [ 3213.068424 ] ffff88083e4c7dd0 ffffffff810124b5 ffff88083e4c9bf8 ffff88083e4c9c38 [ 3213.070133 ] ffff88083e4c9838 00007ff ff7ffd700 ffff88083e4c7de0 ffffffff8101258f [ 3213.071775 ] ffff88083e4c7e08 ffffffff81009a63 ffff88083e457838 ffff88083e4c9838 [ 3213.073419 ] ffff88083e457838 ffff88083e4c7e40 ffffffff81000ebb ffff88083e457838 [ 3213.075057 ] ffff880800000011 ffff88083e457a68 00000000003 d0f00 ffff88083e457838 [ 3213.076703 ] Call Trace : [ 3213.077295 ] < TSK > [ 3213.077828 ] [ < ffffffff810124c1 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 3213.078855 ] [ < ffffffff8101258f > ] warn_slowpath_null + 0xf / 0x20 [ 3213.081653 ] [ < ffffffff81009a63 > ] fpu__copy + 0xc3 / 0x260 [ 3213.082543 ] [ < ffffffff81000ebb > ] arch_dup_task_struct + 0x7b / 0x90 [ 3213.083667 ] [ < ffffffff8101d32e > ] copy_process + 0x14e / 0x10e0 [ 3213.084618 ] [ < ffffffff8103a3c6 > ] ? n_tty_write + 0x166 / 0x3c0 [ 3213.085564 ] [ < ffffffff8101e2e6 > ] do_fork + 0x26 / 0x140 [ 3213.086439 ] [ < ffffffff8101e4a0 > ] ? sys_vfork + 0x40 / 0x40 [ 3213.087333 ] [ < ffffffff8101e4a0 > ] ? sys_vfork + 0x40 / 0x40 [ 3213.088232 ] [ < ffffffff8101e4c9 > ] sys_clone + 0x29 / 0x30 [ 3213.089109 ] [ < ffffffff8100e719 > ] do_syscall_64 + 0x69 / 0xf0 [ 3213.090030 ] [ < ffffffff8100d5ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3213.091078 ] < EOT > [ 3213.091580 ] --- [ end trace 0000000000000000 ] --- [ 3213.093250 ] TRAP do_general_protection in CPU7 , error_code : 0 current : ffff88083fd0f008 0 swapper / 7 [ 3213.096526 ] fixup_exception pid ( 0 ) cpu ( 7 ) insn : 0xffffffff81000c62 ( __switch_to + 0x452 / 0x630 ) fixup : 0xffffffff8105d922 ( __fixup_text_start + 0x32 / 0x322 ) handler : ex_handler_default + 0x0 / 0x20 [ 3213.101241 ] ------------ [ cut here ] ------------ [ 3213.103285 ] WARNING : CPU : 7 PID : 0 at . / arch / x86 / include / asm / fpu / internal . h : 369 __switch_to + 0x47e / 0x630 So, dig into fpu__copy() , find out why it fails at this certain point. Glad I have something to dig into. The instruction leads to GP is: ffffffff8100b0f5 : 48 0f ae 27 xsave64 ( % rdi ) which is generated by: #define XSTATE_XSAVE(st, lmask, hmask, err) \\ asm volatile(ALTERNATIVE_2(XSAVE, \\ XSAVEOPT, X86_FEATURE_XSAVEOPT, \\ XSAVES, X86_FEATURE_XSAVES) \\ \"\\n\" \\ \"xor %[err], %[err]\\n\" \\ \"3:\\n\" \\ \".pushsection .fixup,\\\"ax\\\"\\n\" \\ \"4: movl $-2, %[err]\\n\" \\ \"jmp 3b\\n\" \\ \".popsection\\n\" \\ _ASM_EXTABLE(661b, 4b) \\ : [err] \"=r\" (err) \\ : \"D\" (st), \"m\" (*st), \"a\" (lmask), \"d\" (hmask) \\ : \"memory\") static inline void copy_xregs_to_kernel ( struct xregs_state * xstate ) { u64 mask = - 1 ; u32 lmask = mask ; u32 hmask = mask >> 32 ; int err ; WARN_ON ( ! alternatives_patched ); XSTATE_XSAVE ( xstate , lmask , hmask , err ); /* We should never fault when copying to a kernel buffer: */ WARN_ON_FPU ( err ); } From SDM on XSAVE : Use of a destination operand not aligned to 64-byte boundary (in either 64-bit or 32-bit modes) results in a general-protection (#GP) exception. In 64-bit mode, the upper 32 bits of RDX and RAX are ignored. %rdi is struct xregs_state *xstate in above code. Thus, check if xstate if 64-bytes aligned. Of course, it is not: [10894.999997] copy_xregs_to_kernel CPU6 xstate: ffff88083e4c8c38 Hehe. Criminal identified. But why? The xstate structure is already marked as __attribute__(aliged 64) in the code. It is the task_struct , which is NOT 0x40 aligned. But god why? Because we currently use kmalloc to allocate new task_struct, whose minimum alignment is 8 bytes . Anyway, use __alloc_pages instead. Such an deeply hidden bug. Took me almost a month to find out. IB \u00b6 Seen this during boot (at both P and M, although lego continue running correctly): [ 54017.712533 ] *** NodeID Hostname LID QPN [ 54017.770776 ] *** ------------------------------------- [ 54017.834220 ] *** 0 wuklab12 13 72 [ 54017.892462 ] *** 1 wuklab14 16 72 <--- [ 54017.955906 ] *** 2 wuklab16 20 74 [ 54018.014149 ] *** [ 54074.552844 ] *** Start establish connection ( mynodeid : 1 ) [ 54102.554407 ] ib_process_mad mad_ifc fails [ 54130.960691 ] *** recvpollcq runs on CPU2 [ 54131.070918 ] *** Successfully built QP for node 0 [ LID : 13 QPN : 72 ] [ 54131.152936 ] *** Successfully built QP for node 2 [ LID : 20 QPN : 74 ] [ 54161.228245 ] *** FIT layer ready to go ! [ 54161.272034 ] *** Another one: [ 1966.930409 ] *** [ 1966.951210 ] *** FIT_initial_timeout_s : 30 [ 1967.002168 ] *** FIT_local_id : 0 [ 1967.052087 ] *** [ 1967.072887 ] *** NodeID Hostname LID QPN [ 1967.131126 ] *** ------------------------------------- [ 1967.194567 ] *** 0 wuklab12 13 72 <--- [ 1967.258005 ] *** 1 wuklab14 16 72 [ 1967.316244 ] *** 2 wuklab16 20 74 [ 1967.374484 ] *** [ 2032.926448 ] *** Start establish connection ( mynodeid : 0 ) [ 2032.996068 ] Fail to modify qp [ 6 ] [ 2033.032572 ] Fail to do client_init_ctx [ 2033.077287 ] client_establish_conn : ctx ( null ) fail to init_interface [ 2033.164646 ] ibapi_establish_conn : ctx ( null ) fail to init_interface [ 2033.250967 ] *** [ 2035.620167 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000004 [ 2035.713763 ] IP : [ < ffffffff8105c589 > ] client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0 [ 2035.812562 ] PGD 0 [ 2035.836482 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 2035.884321 ] CPU : 0 PID : 1 Comm : kernel_init 4.0.0 - lego - ys + # 253 [ 2035.955041 ] RIP : 0010 : [ < ffffffff8105c589 > ] [ < ffffffff8105c589 > ] client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0 ... [ 2037.313267 ] < TSK > [ 2037.336146 ] [ < ffffffff8105a377 > ] ibapi_send_reply_timeout + 0x57 / 0x70 [ 2037.411025 ] [ < ffffffff81033d24 > ] ? net_send_reply_timeout + 0x94 / 0x132 [ 2037.486944 ] [ < ffffffff81033d24 > ] net_send_reply_timeout + 0x94 / 0x132 pcache \u00b6 Running word_count-pthread, with 100MB dataset, finally got some reasonable bug: [ 54211.243181 ] pcache_evict_line () : pset : ffff88207f86e3c0 , for uva : 0x7ffff1b8f000 [ 54211.385654 ] pcache : ffff88207f86e3a8 mapcount : 8 refcount : 0 flags :() [ 54211.510447 ] pcache dumped because : PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm )) [ 54212.080336 ] BUG : failure at managers / processor / pcache / evict . c : 240 / pcache_evict_line () ! [ 54212.664785 ] Kernel Panic - not syncing : BUG ! [ 54212.715742 ] CPU : 8 PID : 81 Comm : word_count - pthr 4.0.0 - lego - ys + # 252 ... [ 54213.391706 ] < TSK > [ 54213.414584 ] [ < ffffffff81024180 > ] panic + 0xc2 / 0xeb [ 54213.524818 ] [ < ffffffff8101b81c > ] ? task_tick_rt + 0x2c / 0xd0 [ 54213.589295 ] [ < ffffffff81018f75 > ] ? scheduler_tick + 0x55 / 0x60 [ 54213.655850 ] [ < ffffffff81016625 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 54213.728647 ] [ < ffffffff81006634 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 54213.801443 ] [ < ffffffff8100e22a > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 54213.879439 ] [ < ffffffff8101256d > ] ? printk + 0x11d / 0x1b0 [ 54214.103027 ] [ < ffffffff8102ecf4 > ] pcache_evict_line + 0x134 / 0x220 [ 54214.172703 ] [ < ffffffff8102c6ae > ] pcache_alloc + 0x22e / 0x2e0 [ 54214.237179 ] [ < ffffffff8102be0a > ] common_do_fill_page + 0x2a / 0x1f0 [ 54214.307895 ] [ < ffffffff8102baf0 > ] ? move_page_tables + 0x4c0 / 0x4c0 [ 54214.378612 ] [ < ffffffff8102c172 > ] pcache_handle_fault + 0x1a2 / 0x3a0 [ 54214.450367 ] [ < ffffffff8100fc02 > ] do_page_fault + 0xa2 / 0x1a0 [ 54214.514843 ] [ < ffffffff8100d85f > ] page_fault + 0x1f / 0x30 [ 54214.575161 ] [ < ffffffff81034842 > ] ? copy_user_enhanced_fast_string + 0x2 / 0x10 [ 54214.657316 ] [ < ffffffff81032368 > ] ? seq_read + 0x248 / 0x360 [ 54214.719714 ] [ < ffffffff810307af > ] sys_read + 0x3f / 0xc0 [ 54214.777949 ] [ < ffffffff81030770 > ] ? sweep_pset_lru + 0x220 / 0x220 [ 54214.846587 ] [ < ffffffff8100e619 > ] do_syscall_64 + 0x69 / 0xf0 [ 54214.910022 ] [ < ffffffff8100d4ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 54214.985939 ] < EOT > Another one: [ 735.393244 ] pcache_evict_line () : pset : ffff88207f86e3c0 , for uva : 0x7ffff1b8fd90 [ 735.537804 ] pcache : ffff88207f86e3a8 mapcount : 8 refcount : 0 flags :() [ 735.663642 ] pcache dumped because : PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm )) Do note this happens after computation. This happens when phoenix create a lot threads to sort the results. Both bug happen to the same set, same user page. The pcache is clearly corrupted: mapcount:8, refcount:0, flags:(). Come back after dinner. Remember to check altenative, cause the XSAVE above should be XSAVEOPT. Make sure it does not override other memory. Also, check linker script. Do not forget to link any sections. Another several bug logs in wuklab13 and wuklab15: 022318-* . I\u2019m really tired today after fixing the FPU bug. But I\u2019m also pretty confident pcache is something I\u2019m able to debug. Even thought it is hard in SMP case. Anyway, I gonna call for the day. 02/22 Thur \u00b6 context switch fpu signal compat check, all good. make current use percpu current_task, so all code in Lego is consistent. checked entry_SYSCALL-64 again, which looks good to me. The only concern is rsp_scratch and current_top_of_stack , which are per-cpu variables. If these per-cpu is setup wrong, then we are doomed. Also check if per-cpu is all cleared up? try big syscall lock does x86 has to use different kernel stacks? Interrupt is using different stack in Linux, has to do so??? check current is correct. compare with old implementation. First of all, FPU is definitely functional for now. Since I replaced the current macro today, I add some code to check if this current matches our old implementation: static __always_inline struct task_struct *get_current(void) { return this_cpu_read_stable(current_task); } //#define current get_current() #define current \\ ({ \\ struct task_struct *old = current_thread_info()->task; \\ struct task_struct *new = get_current(); \\ \\ if (old != new) { \\ printk(\"%s:%d() cpu:%d old:%pS %d %s new:%pS %d %s\\n\", \\ __func__, __LINE__, smp_processor_id(), old, old->pid, old->comm, \\ new, new->pid, new->comm); \\ BUG(); \\ } \\ get_current(); \\ }) Combined with some FPU warning, it is now like this: [ 3273.748819 ] CPU : 5 PID : 32 sys_clone + 0x0 / 0x30 [ 3273.800808 ] alloc_task_struct_node : size : 740 ffff88107e831838 [ 3273.869451 ] arch_dup_task_struct () CPU5 current : 32 new : ffff88107e831838 old : ffff88107e827838 32 [ 3273.975533 ] ------------ [ cut here ] ------------ [ 3274.030651 ] WARNING : CPU : 5 PID : 32 at . / arch / x86 / include / asm / fpu / internal . h : 354 fpu__copy + 0xe2 / 0x310 [ 3274.140895 ] CPU : 5 PID : 32 Comm : word_count - pthr 4.0.0 - lego - ys - gdbe6dbe - dirty # 249 [ 3274.231377 ] Stack : [ 3274.255298 ] ffff88107e82fd68 ffffffff81016dbf 00000000ff ffffff 0000000000000000 [ 3274.342659 ] 00000000ff ffffff 0000000000000000 ffff88107e831bf8 ffff88107e831c38 [ 3274.430021 ] ffff88107e831838 000000207f e64000 ffff88107e82fd78 ffffffff810170af [ 3274.517382 ] ffff88107e82fdc0 ffffffff8100b052 0000000000000020 ffff88107e831838 [ 3274.604745 ] ffff88107e827838 ffff88107e827838 ffff88107e831838 ffff88107e827838 [ 3274.692106 ] Call Trace : [ 3274.721229 ] < TSK > [ 3274.744109 ] [ < ffffffff81016dd8 > ] __warn . constprop .0 + 0xe8 / 0x3b0 [ 3274.813790 ] [ < ffffffff810170af > ] warn_slowpath_null + 0xf / 0x20 [ 3274.881391 ] [ < ffffffff8100b052 > ] fpu__copy + 0xe2 / 0x310 [ 3274.941713 ] [ < ffffffff810012e4 > ] arch_dup_task_struct + 0x84 / 0x120 [ 3275.013475 ] [ < ffffffff81022c10 > ] copy_process + 0x160 / 0x1e60 [ 3275.078996 ] [ < ffffffff81024936 > ] do_fork + 0x26 / 0x140 [ 3275.137238 ] [ < ffffffff81024af0 > ] ? sys_vfork + 0x40 / 0x40 [ 3275.198599 ] [ < ffffffff81024af0 > ] ? sys_vfork + 0x40 / 0x40 [ 3275.259960 ] [ < ffffffff81024b19 > ] sys_clone + 0x29 / 0x30 [ 3275.319242 ] [ < ffffffff81012314 > ] do_syscall_64 + 0x84 / 0x240 [ 3275.383723 ] [ < ffffffff8101106c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3275.459645 ] < EOT > [ 3275.482526 ] --- [ end trace 0000000000000000 ] --- [ 3275.537648 ] wake_up_new_task CPU5 task : ffff88107e831838 , dest_cpu : 6 current : 32 [ 3275.623970 ] SMP IPI : reschedule_interrupt () CPU ( 6 ) PID ( 0 ) [ 3275.739412 ] do_general_protection : 186 () cpu : 6 old : 0xffff88107e831838 33 word_count - pthr new : 0xffff88107fcaf008 0 swapper / 6 [ 3275.871493 ] ------------ [ cut here ] ------------ [ 3275.926614 ] BUG : failure at arch / x86 / kernel / traps . c : 186 / do_general_protection () ! [ 3276.015018 ] Kernel Panic - not syncing : BUG ! [ 3276.065978 ] panic : 107 () cpu : 6 old : 0xffff88107e831838 33 word_count - pthr new : 0xffff88107fcaf008 0 swapper / 6 Based on the switch code: __switch_to ( struct task_struct * prev_p , struct task_struct * next_p ) { this_cpu_write ( current_task , next_p ); /* Reload sp0 This changes current_thread_info(). */ load_sp0 ( tss , next ); } Based on log line 30, load_sp0() already happened, which means this_cpu_write(..) happened too. If this_cpu_write(..) happened, then log line 30\u2019s new should have been updated to 0xffff88107e831838 . Something wrong with percpu? 02/21 Wed \u00b6 irq_regs, old code, check signal frame, and fpu hook together Done in_interrupt() , it is empty, TODO check arch/x86/Makefile, it introduce a lot FPU flags. added more than 4K lines today. Damn FPU. Ugh go home sleep. 02/20 Tue Cloudy \u00b6 Not too many Sunny days recently. Well, continue yesterday\u2019s work. I don\u2019t think I can easily find out why so many /proc/memoinfo open happened. Instead, I\u2019m trying to enable the flush_thread in P\u2019s exec code. During the way, I found some issue related to __ARCH_HAS_SA_RESTORER in signal code. I need to check if these x86 macros are defined, but lego does not port them. Well, it turns out flush_thread does not make too much difference. Next I\u2019m going to try to disable exit_thread , which uses fpu__drop() . Hmm, disable exit_thread also does not work. 02/19 Mon Rainy \u00b6 It is another week. I can not deny I\u2019m a little tired about the bug. Tried so many possible solutions, but none of them work. Well, today I first need to test the vma changes (pgoff and anon_vma) thing. Especially the vma merge and split. This morning I fixed a bug in kernel_init process: make kernel_init able to run all possible CPUs. Because the first user process is forked from kernel_init, it is quite important that it gets the right cpu affinity: static int kernel_init ( void * unused ) { ... set_cpus_allowed_ptr ( current , cpu_possible_mask ); ... } Well, interestingly, the unmodified word_count-pthread succeed with 50MB dataset\u2026 with or without any DEBUG option! Amazing! I need to find out why the cpus_allowed becomes 0 at the beginning of kernel_init. Because init_task actually has: . cpus_allowed = CPU_MASK_ALL , . nr_cpus_allowed = NR_CPUS , Things to do next: check why the cpus_allowed changed check why word_count-pthread open /dev/../cpu so many times. Anything wrong with our copy_files , or open, close? here is an idea, to verify if FPU code is correct, run some scientific benchmarks. Okay, findings: cpus_allowd is fine, it is reset inside sched_init() , when it tries make the init_task as the idle thread. Thus it is reasonable to set cpus_allowed again at kernel_init thread. And it should NOTHING to do with the bug. about the second, check the following log: [ 11838.364543 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 11838.422886 ] STDOUT : --- [ ] --- [ 11838.463445 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_50MB . txt , flags : 0 , mode : 900 [ 11838.619460 ] SYSC_open ( cpu5 pid : 32 ) : fd : 3 [ 11838.667406 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / sys / devices / system / cpu / online , flags : 80000 , mode : 0 [ 11838.773351 ] SYSC_open ( cpu5 pid : 32 ) : fd : 4 [ 11838.821239 ] seq_file : dest_uva : 00007ff fffffc8d0 , nr_chars : 5 string : [ 0 - 23 ] [ 11838.913791 ] SYSC_close ( cpu5 pid : 32 ) : fd : 4 [ 11838.962622 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 11840.223255 ] STDOUT : --- [ Word Count : Computation Completed 1.555581 sec ] --- [ 11840.309678 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / sys / devices / system / cpu / online , flags : 80000 , mode : 0 [ 11840.415754 ] SYSC_open ( cpu5 pid : 32 ) : fd : 4 [ 11840.463593 ] seq_file : dest_uva : 00007ff fffffc8a0 , nr_chars : 5 string : [ 0 - 23 ] [ 11840.556147 ] SYSC_close ( cpu5 pid : 32 ) : fd : 4 [ 11840.605024 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 11840.677821 ] STDOUT : --- [ THe number of processors is 24 \u00f4 ] --- [ 11840.753769 ] SYSC_open ( cpu7 pid : 80 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11840.844212 ] SYSC_open ( cpu19 pid : 92 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11840.935728 ] SYSC_open ( cpu7 pid : 80 ) : fd : 4 [ 11840.983567 ] SYSC_open ( cpu19 pid : 92 ) : fd : 5 [ 11841.032444 ] seq_file : dest_uva : 00007ff ff444c000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11841.305953 ] seq_file : dest_uva : 00007ff ff444b000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11841.579460 ] SYSC_close ( cpu7 pid : 80 ) : fd : 4 [ 11841.628339 ] SYSC_close ( cpu19 pid : 92 ) : fd : 5 [ 11841.678257 ] SYSC_close () : [ 4 ] -> [ / proc / meminfo ] [ 11841.733375 ] SYSC_close () : [ 5 ] -> [ / proc / meminfo ] [ 11841.788493 ] SYSC_open ( cpu18 pid : 91 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11841.880008 ] SYSC_open ( cpu6 pid : 102 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11841.971523 ] SYSC_open ( cpu12 pid : 85 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.063040 ] SYSC_open ( cpu0 pid : 97 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.153516 ] SYSC_open ( cpu14 pid : 87 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.245032 ] SYSC_open ( cpu16 pid : 89 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.336548 ] SYSC_open ( cpu4 pid : 100 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.428064 ] SYSC_open ( cpu16 pid : 89 ) : fd : 9 [ 11842.476942 ] SYSC_open ( cpu4 pid : 100 ) : fd : 10 [ 11842.526860 ] seq_file : dest_uva : 00007ff ff444c000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11842.800368 ] seq_file : dest_uva : 00007ff ff444b000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11843.073877 ] SYSC_close ( cpu16 pid : 89 ) : fd : 9 However, in a normal Linux exeution: strace - C - o strace_2 . / word_count - pthread . / word_count_datafiles / word_50MB . txt % time seconds usecs / call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 86.41 0.052074 1736 30 futex 6.89 0.004151 67 62 munmap 2.47 0.001490 17 88 mmap 2.12 0.001278 14 93 clone 1.51 0.000912 14 64 mprotect 0.19 0.000117 7 16 write 0.15 0.000092 46 2 open $ cat strace_2 | grep open open ( \"./word_count_datafiles/word_50MB.txt\" , O_RDONLY ) = 3 open ( \"/sys/devices/system/cpu/online\" , O_RDONLY | O_CLOEXEC ) = 4 It opened the /proc/meminfo for way too many times. In the normal Linux execution, this should not happen. Is it because our meminfo is faked, so glibs is complaining? But why it does not open meminfo while running in Linux? Or does our entry assembly messed up some stuff in stack, so the return path changed? oh, about the FPU. It reminds our flush_thread function actually has an issue before. When I enabled this function during loading in P, the P will crash. Within flush_thread , there is a fpu_clear !!! So, check this tomorrow! (12:00am, need to go home) 02/18 Sun Sunny \u00b6 It is a nice day. Yesterday I\u2019ve changed one line of code in mmap code path: change anonymous vma\u2019s pgoff from some value to 0. The result is I got several succeed work-count-pthread(bind to one core) testing. However, it still fail with unmodified word-count-pthread. It brings me to inspect pgoff manipulation code and all mmap.c code. We ported everything from linux without almost zero modification. That means we ported all those useless anon_vma and pgoff code, which is used a lot by vma_merge, vma_split code. The thing is: our memory manager, our vma code do not need such anon_vma structure, and do not maintain pgoff. Thus, I\u2019m a little bit worried linux code may doing some crazy behind our back: mess vma and pages, then pcache miss gets some wrong pages Well. Lego does not use anon_vma , and pgoff should only be used by file-backed vma. So, I decided to remove anon_vma from our code, and make sure pgoff is used properly. Of course, the goal is to make vma_merge, split, copy, do the things we intended. Lesson learned. 02/17 Sat Snowy \u00b6 Fixed the bss bug. It comes from loader. We did not implement the lego_clear_user function, so some part of bss is non-zero. Bad news is word_count-pthread still fail at same fpu instruction. Have to look into memory code more. This is actually a fun debugging story. We should always add TODO or XXX or some warnings to unfinished code, no matter what. Lesson learned. 02/16 Fri Cloudy \u00b6 Yilun found a major loader bug yesterday: the .bss section variables are not 0, in the iozone benchmark. I did not encounter this issue before with simple test program. This is pretty serious. 02/15 Thur Rainy \u00b6 Today is Chinese New Year. Line 7 and 8 show the uva belong to the same page. Need to revisit get_arg_pages etc functions. [ 108.393991] handle_p2m_execve(): pid:22,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 108.395255] argc[0] (len: 65): /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 108.396329] argc[1] (len: 82): /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt [ 108.397530] envc[0] (len: 7): HOME=/ [ 108.398069] envc[1] (len: 11): TERM=linux [ 108.398640] __bprm_mm_init vma: ffff88083effe6b8 [ 108.399226] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffefed [ 108.399949] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffef94 Well, this is 100% fine. I wrote this loader code long time ago and need some time to pickup. So, after I read the loader code, especially the copy_strings function, I found this is okay. Because copy_strings will be invoked three times, so the faultin_page basically will be invoked at least three times. That is why it went to that pte fault handling code. Although actually I think copy_strings should not use faultin_page , instead, it should use get_user_pages , which will walk through the pgtable first, then went to handle_lego_mm_fault . 02/14 Wed Rainy \u00b6 Hmm, tried to make kmalloc behave as kzalloc, and bind all threads to one core, still gave the same old bug: 42731a: f3 0f 6f 16 movdqu (%rsi),%xmm2 [93182.657376] word_count-pthr[85] general protection ip:42731a sp:7fffe3ffed28 error:0 [93182.747959] CPU: 8 PID: 85 Comm: word_count-pthr 4.0.0-lego+ #170 [93182.820758] RIP: 0033:[<000000000042731a>] [<000000000042731a>] 0x42731a [93182.901878] RSP: 002b:00007fffe3ffed28 EFLAGS: 00010283 [93182.965317] RAX: 000000000000001f RBX: 00007ffff001b010 RCX: 0000000000000005 [93183.050596] RDX: 0000000000000000 RSI: 5345485355420045 RDI: 00007ffff294791f [93183.135876] RBP: 00007ffff294791f R08: 000000000000ffff R09: 0000000000000008 [93183.221156] R10: fffffffffffff048 R11: 00000000004acfc0 R12: 0000000000001cde [93183.306435] R13: 00000000006e4a8c R14: 0000000000001cd7 R15: 0000000000001cda [93183.391716] FS: 00007fffe3fff700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000 [93183.488434] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [93183.557075] CR2: 00007ffff27a4000 CR3: 000000107e924000 CR4: 00000000000406a0 427377: 66 0f 6f 17 movdqa (%rdi),%xmm2 [93180.527248] word_count-pthr[93]: segfault at 0x0 ip 0000000000427377 sp 00007fffdfff6d28 error 4 [93180.630314] CPU: 8 PID: 93 Comm: word_count-pthr 4.0.0-lego+ #170 [93180.703114] RIP: 0033:[<0000000000427377>] [<0000000000427377>] 0x427377 [93180.784234] RSP: 002b:00007fffdfff6d28 EFLAGS: 00010297 [93180.847674] RAX: 0000000000000000 RBX: 000000000073c4c0 RCX: 000000000000000d [93180.932953] RDX: 000000000000ffff RSI: 00007ffff4999070 RDI: 0000000000000000 [93181.018233] RBP: 00007ffff499907d R08: 000000000000ffff R09: 0000000000000000 [93181.103513] R10: 0000000000427760 R11: 00007ffff49982c0 R12: 0000000000000118 [93181.188791] R13: 00000000006e4aac R14: 0000000000000116 R15: 0000000000000117 [93181.274072] FS: 00007fffdfff7700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000 [93181.370790] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [93181.439430] CR2: 0000000000000000 CR3: 000000107e924000 CR4: 00000000000406a0 Tried several ways to ensure memory safety. It still failed even if I enabled all of them. So, I guess the memory safety is ensured? Still some other things? force alloc_pages to use __GFP_ZERO make kmalloc behave as kzalloc make kfree empty I also suspect munmap may free extra wrong pgtable entries. Although I\u2019ve went through all the code and checked, but in addition to the above things, I\u2019m going to: make munmap dummy (no p2m_munmap, return 0 directly) Failed. Next, I\u2019m going to: add checksum for every page transferred across network. add warning for unnormal cases Bang! I found something while running P+M: [ 115.727597 ] Memory - component manager is up and running . [ 116.691723 ] handle_p2m_fork () : nid : 0 , pid : 22 , tgid : 22 , parent_tgid : 1 [ 116.697038 ] handle_p2m_fork () : reply : 0 : OKAY [ 116.791088 ] handle_p2m_execve () : pid : 22 , argc : 2 , envc : 2 , file : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread [ 116.792357 ] argc [ 0 ] ( len : 65 ) : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread [ 116.793439 ] argc [ 1 ] ( len : 82 ) : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt [ 116.794653 ] envc [ 0 ] ( len : 7 ) : HOME =/ [ 116.795196 ] envc [ 1 ] ( len : 11 ) : TERM = linux [ 116.795772 ] __bprm_mm_init vma : ffff88083effe6b8 [ 116.796209 ] faultin_page vma : ffff88083effe6b8 [ 116.796729 ] faultin_page vma : ffff88083effe6b8 [ 116.797150 ] handle_pte_fault vma : ffff88083effe6b8 entry : 0xffff88083e8c1067 [ 116.798044 ] pte : ffff88083e8c0ff0 pfn : 0x8083e8c1 flags :( present | writable | user | accessed | dirty | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 116.799462 ] ------------ [ cut here ] ------------ [ 116.800049 ] WARNING : CPU : 4 PID : 15 at managers / memory / vm / fault . c : 148 handle_lego_mm_fault + 0x4d8 / 0x550 [ 116.801148 ] CPU : 4 PID : 15 Comm : mc - manager 4.0.0 - lego + # 78 [ 116.801818 ] Stack : [ 116.802179 ] ffff88083e893c50 ffffffff8100e827 00007ff fffffef94 ffff88083effe6b8 [ 116.803283 ] ffff88083e894008 ffff88083e8c1067 ffff88083e893c60 ffffffff8100e91f [ 116.804387 ] ffff88083e893cf0 ffffffff8102b008 0000000000000031 ffff88083e893cf0 [ 116.805488 ] 00000000000002 96 00003ff fffe00000 ffff800000000067 ffff88083e893d50 [ 116.806590 ] ffff880000000001 ffffffff81066798 ffff88083effe6b8 ffff88083e893d50 [ 116.807691 ] Call Trace : [ 116.808087 ] < TSK > [ 116.808448 ] [ < ffffffff8100e836 > ] __warn . constprop .0 + 0xa6 / 0x100 [ 116.809126 ] [ < ffffffff8100e91f > ] warn_slowpath_null + 0xf / 0x20 [ 116.809802 ] [ < ffffffff8102b008 > ] handle_lego_mm_fault + 0x4d8 / 0x550 [ 116.810505 ] [ < ffffffff8102cfe3 > ] faultin_page + 0x43 / 0xb0 [ 116.811131 ] [ < ffffffff8102dab1 > ] copy_strings . isra .1 + 0xe1 / 0x130 [ 116.811819 ] [ < ffffffff8102dd1e > ] exec_loader + 0x21e / 0x350 [ 116.812457 ] [ < ffffffff8102680a > ] handle_p2m_execve + 0x1aa / 0x290 This is a temporary stack vma that loader created for saving argv and envp. So, this vma was created here: static int __bprm_mm_init ( struct lego_binprm * bprm ) { ... bprm -> vma = vma = kzalloc ( sizeof ( * vma ), GFP_KERNEL ); ... } And then copy_strings will call faultin_page to populate a page for a specific user virtual adddress: int faultin_page ( struct vm_area_struct * vma , unsigned long start , unsigned long flags , unsigned long * kvaddr ) { ... ret = handle_lego_mm_fault ( vma , start , flags , kvaddr ); ... } Eventually, the handle_lego_mm_fault will call handle_pte_fault : static int handle_pte_fault ( struct vm_area_struct * vma , unsigned long address , unsigned int flags , pte_t * pte , pmd_t * pmd , unsigned long * mapping_flags ) { ... if ( ! pte_present ( entry )) { ... } pr_info ( \"%s vma: %p entry: %#lx \\n \" , FUNC , vma , entry . pte ); dump_pte ( pte , NULL ); WARN_ON_ONCE ( 1 ); ... } Apparently, pte is wrong! But I don\u2019t have time today. Continue tomorrow. Hmm forgot that we are saving kernel virtual addresses in the pte. Just take a quick look at the lego_pud_alloc things, seems will have some issues. I defenitly need to check all these stuff tomorrow. I\u2019ve not touch this part for too long! 02/13 Tue Sunny \u00b6 Checking our SLOB allocator today. So I found Yutong\u2019s code is using set_page_private when slob get a new page from buddy. This private field is only intended to be used by buddy to record the order . This mixed usage will confuse buddy and create bug. Even though I removed the set_page_private ( page , 0 ) after free_page , word_count-pthread still fails. Damn. 02/12 Mon Cloudy \u00b6 Add this commit 4cb3a8b6a943c90714fd9bb5e5465ee315f0aa30 : memory: Use kzalloc instead of kmalloc in __bprm_mm_init (loader) This was an potentionl bug that was not triggered previously. It is simply because kmalloc'ed vma contains some garbage area, while later in the pgfault code, we use if (vma->vm_ops && vma->vm_ops->fault) ... to check if it is an file-backed fault. Fortunately the vma->vm_ops happens to have some leftover value. So this bug was triggered. This actually reminds me that this is a series of potential bugs! Even though before I've added things like force GFP_ZERO in all physical page allocation, I missed the kmalloc's case! The story is: I patched the stop_machine code today, and tried to run code with P+M on VM, everything works fine. However, when I tried to run the new code with P+M+S on physical machine, M crashed at a very weird point: [ 7791.998168] handle_p2m_execve(): pid:81,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 7792.129312] BUG: unable to handle kernel NULL pointer dereference at 0000000000000031 [ 7792.222889] IP: [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0 [ 7792.299842] PGD 0 [ 7792.323760] Oops: 0000 [#1] PREEMPT SMP MEMORY [ 7792.376794] CPU: 4 PID: 79 Comm: mc-manager 4.0.0-lego+ #29 [ 7792.443349] RIP: .. [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0 ...... .... [ 7793.750506] Call Trace: [ 7793.779623] <TSK> [ 7793.802501] [<ffffffff810053f4>] ? apic_timer_interrupt+0x54/0x90 [ 7793.875295] [<ffffffff8102e469>] faultin_page+0x9/0x70 [ 7793.936649] [<ffffffff8102ef01>] copy_strings.isra.1+0xe1/0x130 [ 7794.007362] [<ffffffff8102f11e>] exec_loader+0x1ce/0x340 [ 7794.070796] [<ffffffff81027def>] handle_p2m_execve+0x12f/0x200 [ 7794.140469] [<ffffffff810274fb>] mc_manager+0x1ab/0x2b0 [ 7794.202864] [<ffffffff81027350>] ? bitmap_fill+0x33/0x33 [ 7794.266298] [<ffffffff8101c6b7>] kthread+0x107/0x130 [ 7794.325572] [<ffffffff8101c5b0>] ? __kthread_parkme+0x90/0x90 [ 7794.394205] [<ffffffff8100b462>] ret_from_fork+0x22/0x30 So faulting source code is: static int handle_pte_fault ( struct vm_area_struct * vma , unsigned long address , unsigned int flags , pte_t * pte , pmd_t * pmd ) { .... if ( vma -> vm_ops && vma -> vm_ops -> fault ) return do_linear_fault ( vma , address , flags , pte , pmd , entry ) .... Something wrong with vma ? At this loader stage, this vma is a temporaty stack vma created for saving argv and envp . So I look back into the code that created this vma: managers / memory / loader / core . c : static int __bprm_mm_init ( struct lego_binprm * bprm ) { int err ; struct vm_area_struct * vma = NULL ; struct lego_mm_struct * mm = bprm -> mm ; bprm -> vma = vma = kmalloc ( sizeof ( * vma ), GFP_KERNEL ); if ( ! vma ) return - ENOMEM ; The code after this does NOT do necessary cleanup. The vm_ops happens to have some garbage value from last user. So it is not 0, so the above vma->vm_ops is true, and it will try to read vma->vm_ops->fault . And that, my friend, is where garbage turns into crash. This presents a series of potential bugs. Ugh, memory safety ! 02/09 Fri Cloudy \u00b6 Tried to modify Phoneix code: replace realloc with malloc+mempcy . Thus the mremap syscall is avoided, but it still has general protection fault. Same with yesterday, corrupted at __strcmp_sse42 , with corrupted RSI or RDI . So I guess it is not about mremap itself at all. I will follow yesterday\u2019s checking list. 02/08 Thur Cloudy \u00b6 00000000004272d0 <__strcmp_sse42>: 4272d0: 89 f1 mov %esi,%ecx 4272d2: 89 f8 mov %edi,%eax 4272d4: 48 83 e1 3f and $0x3f,%rcx 4272d8: 48 83 e0 3f and $0x3f,%rax 4272dc: 83 f9 30 cmp $0x30,%ecx 4272df: 77 3f ja 427320 <__strcmp_sse42+0x50> 4272e1: 83 f8 30 cmp $0x30,%eax 4272e4: 77 3a ja 427320 <__strcmp_sse42+0x50> 4272e6: f3 0f 6f 0f movdqu (%rdi),%xmm1 * 4272ea: f3 0f 6f 16 movdqu (%rsi),%xmm2 4272ee: 66 0f ef c0 pxor %xmm0,%xmm0 4272f2: 66 0f 74 c1 pcmpeqb %xmm1,%xmm0 4272f6: 66 0f 74 ca pcmpeqb %xmm2,%xmm1 4272fa: 66 0f f8 c8 psubb %xmm0,%xmm1 4272fe: 66 0f d7 d1 pmovmskb %xmm1,%edx 427302: 81 ea ff ff 00 00 sub $0xffff,%edx 427308: 0f 85 42 0d 00 00 jne 428050 <__strcmp_sse42+0xd80> 42730e: 48 83 c6 10 add $0x10,%rsi 427312: 48 83 c7 10 add $0x10,%rdi 427316: 66 2e 0f 1f 84 00 00 nopw %cs:0x0(%rax,%rax,1) 42731d: 00 00 00 427320: 48 83 e6 f0 and $0xfffffffffffffff0,%rsi 427324: 48 83 e7 f0 and $0xfffffffffffffff0,%rdi 427328: ba ff ff 00 00 mov $0xffff,%edx 42732d: 45 31 c0 xor %r8d,%r8d 427330: 83 e1 0f and $0xf,%ecx 427333: 83 e0 0f and $0xf,%eax 427336: 66 0f ef c0 pxor %xmm0,%xmm0 42733a: 39 c1 cmp %eax,%ecx 42733c: 74 32 je 427370 <__strcmp_sse42+0xa0> 42733e: 77 07 ja 427347 <__strcmp_sse42+0x77> 427340: 41 89 d0 mov %edx,%r8d 427343: 91 xchg %eax,%ecx 427344: 48 87 f7 xchg %rsi,%rdi * 427347: 66 0f 6f 17 movdqa (%rdi),%xmm2 (RDI: 0000000000000000) Frustrating! What is wrong with multithread program? Because of broken FPU-switch code? of inappropriate TLB flush? of IB corrupts memory? of what? ugh? I\u2019m done with this random guess and frustrated general protection or segfault, I need to first make sure underlying kernel is 100% percent correct, this is a checking list: fpu save/restore always fail at some XMM instruction always with corrupted RDI or RSI switch_to_asm %gs and %fs switch_mm (pgd) stack frame set_arch_tls (%fs) glibc\u2019s way of using per thread data some cpu may miss tlb flush kernel entry/exit assembly current_task macro stack_stratch per-cpu data in entry.S futex clear_tid set_tid shared mm robust list interrupts vector array APIC setup IO-APIC timer interrupt cpu_init and Trampoline faked kernel version P side pgfault handling code (SMP) and M side pgfault handling (SMP) mremap, munmap check pgtable boundary In all, check SMP implications Is there any code, that is solely used to test if the underlying kernel has appropriate behaviors? Like glibc test code? How to protect kernel virtual memory? Any existing solutions in Linux? What is the implication of multiple CPU entering kernel at the same time? How can it corrupt user pages? Maybe: kernel entry code, per-cpu data in entry code, fpu code, switch_to, scheduler. Why it always fail at those FPU code i.e. the strcmp function? I failed to compile without those sse, any solution? How it hurt performance? 02/07 Wed Cloudy \u00b6 20:07 Pushed a small patch on mremap issue. Hope it will work. mremap really makes the whole thing very interesting, will be a very good research finding on combing virtual cache and operating system. Need to go gym with a friend, will be back on debugging late tonight. 9:30 Have two meetings to do today, and an security class, won\u2019t have too much time coding during daytime. 02/06 Tue Sunny \u00b6 Well. We\u2019ve ruled out both smp_call_function and workqueue yesterday with Yiying\u2019s help. But the multi-thread word-count still fails :-( Single thread word-count just finished 4GB dataset (with 8GB pcache). So what could be still wrong with multithread one???? chill check exit code (Checked) check pcache\u2019s usage of task_struct, should always use the group_leader check cpu boot code and check the switch code again I believe pinpoint the issue in multithread word-count can solve a lot issues, it must be some thread creation, removal, schedule things. How about adding a lock for ibapi, make it sequential? Sweet, I tried, finally it is a bug that we are able to debug . 22:39 Done for today. I\u2019m trying to patch move_pte and pcache_move_pte . Although in theory we defenitly need to patch it, I keep thinking the code before should not trigger any serious bus or memory corruption. Ugh. Maybe it is concurrent mremap that one of them remap from A to B, while another one remap from C to A. It is possible. But my dead brain can not think of this anymore. I\u2019m going to hit the gym and do some squats. 17:01 Criminal found: mremap() and virtual cache did the crime. Interesting, I have not seen any research paper, tech-reports, writeup, code about this, not even the OVC paper, which, by the way, I think they must consider this case. Otherwise, a mremap will simply crash its virtual cache. Many thanks went to my smoke-and-think time. 15:14 Something new came up! After adding a spinlock for ibapi, this showed up (I tried one more time after this, which does not show up). We are lucky to catch this. At least I know where to look at. Also, this is defenitly triggered by mremap . It is seems it is overlapped mremap() . One thing I did not know is which thread trigger this bug, the sweep thread? Cause mremap related pcache rmap functions do not use rmap_get_locked_pte . [ 3826.048774] normal_p2s_open(): f_name: word_100MB.txt, mode: 04400, flags: 0 [ 3827.891622] SYSC_mremap(cpu18): move: [0x7fffe5788000 - 0x7fffe5806000] -> [0x7fffe531b000 - 0x7fffe5399000] [ 3828.178643] SYSC_mremap(cpu14): move: [0x7fffe5941000 - 0x7fffe5980000] -> [0x7fffe57c7000 - 0x7fffe5806000] **** ERROR: mismatched PTE and rmap **** rmap->owner_process: word_count-pthr uva: 0x7fffe57c8000 ptep: ffff88107efe0e40, rmap->page_table: ffff88107efe0e40 **** pcache_pfn: 0x1257c8, pte_pfn: 0x125942 14:00 word_count-pthread : 100MB dataset pcache : 8GB, 8-way victim : 8 entries [ 1294.845313] STDOUT: ---[ Wordcount: Running... ]--- [ 1294.903661] STDOUT: ---[ o; ]--- [ 1294.946301] normal_p2s_open(): f_name: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt, mode: 04400, flags: 0 [ 1295.100517] SYSC_close(): [4] -> [/sys/devices/system/cpu/online] [ 1295.594658] word_count-pthr[59] general protection ip:4272ea sp:7ffff1b8ed28 error:0 [ 1295.685236] CPU: 10 PID: 59 Comm: word_count-pthr 4.0.0-lego+ #113 [ 1295.759070] RIP: 0033:[<00000000004272ea>] [<00000000004272ea>] 0x4272ea [ 1295.840184] RSP: 002b:00007ffff1b8ed28 EFLAGS: 00010283 [ 1295.903621] RAX: 000000000000000f RBX: 00007fffe5a3d010 RCX: 0000000000000001 [ 1295.988893] RDX: 0000000000000000 RSI: 4854005942004441 RDI: 00007ffff1c1e80f [ 1296.074166] RBP: 00007ffff1c1e80f R08: 0000000000000000 R09: 0000000000000010 [ 1296.211435] R10: 0000000000427ce0 R11: 00007ffff1bbb3ba R12: 0000000000001de4 [ 1296.296711] R13: 00000000006e4a80 R14: 0000000000001d9e R15: 0000000000001dc1 [ 1296.433978] FS: 00007ffff1b8f700(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000 [ 1296.582686] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1296.963297] CR2: 00007ffff1c1e000 CR3: 000000207fd8a000 CR4: 00000000000406a0 So what is this ip:4272ea , let us objdump the binary: 0000000000425e60 <strcmp>: 425e60: 48 8d 05 69 14 00 00 lea 0x1469(%rip),%rax # 4272d0 <__strcmp_sse42> 425e67: f7 05 5f b8 2b 00 00 testl $0x100000,0x2bb85f(%rip) # 6e16d0 <_dl_x86_cpu_features+0x10> 425e6e: 00 10 00 425e71: 75 1a jne 425e8d <strcmp+0x2d> 425e73: 48 8d 05 46 b0 00 00 lea 0xb046(%rip),%rax # 430ec0 <__strcmp_ssse3> 425e7a: f7 05 4c b8 2b 00 00 testl $0x200,0x2bb84c(%rip) # 6e16d0 <_dl_x86_cpu_features+0x10> 425e81: 02 00 00 425e84: 75 07 jne 425e8d <strcmp+0x2d> 425e86: 48 8d 05 03 00 00 00 lea 0x3(%rip),%rax # 425e90 <__GI_strcmp> 425e8d: c3 retq 425e8e: 66 90 xchg %ax,%ax .. .. .. .. 00000000004272d0 <__strcmp_sse42>: 4272d0: 89 f1 mov %esi,%ecx 4272d2: 89 f8 mov %edi,%eax 4272d4: 48 83 e1 3f and $0x3f,%rcx 4272d8: 48 83 e0 3f and $0x3f,%rax 4272dc: 83 f9 30 cmp $0x30,%ecx 4272df: 77 3f ja 427320 <__strcmp_sse42+0x50> 4272e1: 83 f8 30 cmp $0x30,%eax 4272e4: 77 3a ja 427320 <__strcmp_sse42+0x50> 4272e6: f3 0f 6f 0f movdqu (%rdi),%xmm1 * 4272ea: f3 0f 6f 16 movdqu (%rsi),%xmm2 4272ee: 66 0f ef c0 pxor %xmm0,%xmm0 You can see %rsi has some garbage value RSI: 4854005942004441 . Something went wrong. Will it be our FPU? I\u2019m not quite sure. If FPU code has error, why single-thread one succeed? Why it only shows up at multithread ones? 02/05 Mon Sunny \u00b6 From yesterday\u2019s testing of Phoenix, it looks like something is wrong in smp_call_functions() . They are invoked through tlb flush , which was further invoked by mremap , or munmap . The warning from smp is: [ 1260.586696 ] WARNING : CPU : 0 PID : 73 at kernel / smp . c : 129 generic_smp_call_function_single_interrupt + 0xb8 / 0x160 [ 1260.705251 ] CPU : 0 PID : 73 Comm : word_count - pthr 4.0.0 - lego + # 99 [ 1260.777008 ] Stack : [ 1260.800927 ] ffff88207fdffef8 ffffffff8100ec67 ffff88107fc00000 ffff88107fc00000 [ 1260.888283 ] ffffffff8100d410 ffff88207fe23df0 ffff88207fdfff08 ffffffff8100ed5f [ 1260.975639 ] ffff88207fdfff38 ffffffff8100fe68 00007ff fe58c3010 0000000000000f 96 [ 1261.062995 ] 000000000000f 960 0000000000000f 95 ffff88207fdfff48 ffffffff810020dd [ 1261.150351 ] 00007ff ff58869c1 ffffffff8100b2e9 0000000000000f 96 0000000000000f 95 [ 1261.237707 ] Call Trace : [ 1261.266825 ] < TSK > [ 1261.289704 ] [ < ffffffff8100ec76 > ] __warn . constprop .0 + 0xa6 / 0x100 [ 1261.359381 ] [ < ffffffff8100d410 > ] ? pgd_free + 0x90 / 0x90 [ 1261.419699 ] [ < ffffffff8100ed5f > ] warn_slowpath_null + 0xf / 0x20 [ 1261.487295 ] [ < ffffffff8100fe68 > ] generic_smp_call_function_single_interrupt + 0xb8 / 0x160 [ 1261.581931 ] [ < ffffffff810020dd > ] call_function_interrupt + 0x1d / 0x20 [ 1261.655767 ] [ < ffffffff8100b2e9 > ] smp__call_function_interrupt + 0x69 / 0x70 So I decided to look into smp.c a little bit to find out if there is something wrong (I wrote it long time ago). The warning itself is true, it means some inconsistent behavior.. I saw alloc_percpu stuff during call_function_init , hence probably I also need to check percpu code a little code cause I\u2019m not sure if I port all the functionalities. In all, today\u2019s task, check percpu and smp_call_function code. Esp, percpu code, they are crucial and very hard to relate real bugs to it. Well\u2026 things changed. I found a more serious bug: something about cpuhotplug , even though lego is not using it. cpuhotplug is a set of implict callbacks to all different subsystems who want to do some initialization work on each offline->online cpu. Let us dig into how secondary cpu boots: Trampoline .. setup 64 bit mode start_secondary () smp_callin () notify_cpu_starting () ... while ( st -> state < target ) { st -> state ++ ; cpuhp_invoke_callback ( cpu , st -> state , true , NULL ); } cpuhp_invoke_callback () See? There will be some callbacks! What are those callbacks exactly? Well, they are predefined at the kernel/cpu.c . To save the trouble of reading code, I just print what functions are executed, the log is: [ 0.118235] cpuhp_invoke_callback(): 136 CPU:0 page_writeback_cpu_online+0x0/0x20 [ 0.368478] cpuhp_invoke_callback(): 136 CPU:1 smpboot_create_threads+0x0/0x90 [ 0.370196] cpuhp_invoke_callback(): 136 CPU:1 perf_event_init_cpu+0x0/0xa0 [ 0.370403] cpuhp_invoke_callback(): 136 CPU:1 workqueue_prepare_cpu+0x0/0x80 [ 0.371112] cpuhp_invoke_callback(): 136 CPU:1 hrtimers_prepare_cpu+0x0/0x60 [ 0.371339] cpuhp_invoke_callback(): 136 CPU:1 smpcfd_prepare_cpu+0x0/0x80 [ 0.371584] cpuhp_invoke_callback(): 136 CPU:1 relay_prepare_cpu+0x0/0xe0 [ 0.371794] cpuhp_invoke_callback(): 136 CPU:1 rcutree_prepare_cpu+0x0/0x170 [ 0.372333] cpuhp_invoke_callback(): 136 CPU:1 notify_prepare+0x0/0xa0 [ 0.372744] cpuhp_invoke_callback(): 136 CPU:1 bringup_cpu+0x0/0x100 [ 0.008000] cpuhp_invoke_callback(): 136 CPU:1 sched_cpu_starting+0x0/0x60 [ 0.926124] cpuhp_invoke_callback(): 136 CPU:1 smpboot_unpark_threads+0x0/0x90 [ 0.926124] cpuhp_invoke_callback(): 136 CPU:1 perf_event_init_cpu+0x0/0xa0 [ 0.927028] cpuhp_invoke_callback(): 136 CPU:1 workqueue_online_cpu+0x0/0x2a0 [ 0.927768] cpuhp_invoke_callback(): 136 CPU:1 rcutree_online_cpu+0x0/0x70 [ 0.928045] cpuhp_invoke_callback(): 136 CPU:1 notify_online+0x0/0x20 [ 0.928256] cpuhp_invoke_callback(): 136 CPU:1 page_writeback_cpu_online+0x0/0x20 [ 0.928527] cpuhp_invoke_callback(): 136 CPU:1 sched_cpu_activate+0x0/0x190 [ 0.929084] cpuhp_invoke_callback(): 136 CPU:2 smpboot_create_threads+0x0/0x90 [ 0.930240] cpuhp_invoke_callback(): 136 CPU:2 perf_event_init_cpu+0x0/0xa0 [ 0.930434] cpuhp_invoke_callback(): 136 CPU:2 workqueue_prepare_cpu+0x0/0x80 [ 0.931070] cpuhp_invoke_callback(): 136 CPU:2 hrtimers_prepare_cpu+0x0/0x60 [ 0.931264] cpuhp_invoke_callback(): 136 CPU:2 smpcfd_prepare_cpu+0x0/0x80 [ 0.931464] cpuhp_invoke_callback(): 136 CPU:2 relay_prepare_cpu+0x0/0xe0 [ 0.931649] cpuhp_invoke_callback(): 136 CPU:2 rcutree_prepare_cpu+0x0/0x170 [ 0.932245] cpuhp_invoke_callback(): 136 CPU:2 notify_prepare+0x0/0xa0 [ 0.932475] cpuhp_invoke_callback(): 136 CPU:2 bringup_cpu+0x0/0x100 [ 0.008000] cpuhp_invoke_callback(): 136 CPU:2 sched_cpu_starting+0x0/0x60 [ 1.005023] cpuhp_invoke_callback(): 136 CPU:2 smpboot_unpark_threads+0x0/0x90 [ 1.005065] cpuhp_invoke_callback(): 136 CPU:2 perf_event_init_cpu+0x0/0xa0 [ 1.005408] cpuhp_invoke_callback(): 136 CPU:2 workqueue_online_cpu+0x0/0x2a0 [ 1.005729] cpuhp_invoke_callback(): 136 CPU:2 rcutree_online_cpu+0x0/0x70 [ 1.006029] cpuhp_invoke_callback(): 136 CPU:2 notify_online+0x0/0x20 [ 1.006206] cpuhp_invoke_callback(): 136 CPU:2 page_writeback_cpu_online+0x0/0x20 [ 1.006549] cpuhp_invoke_callback(): 136 CPU:2 sched_cpu_activate+0x0/0x190 Interesting! Currently, Lego need to add the smpboot_create_threads() , workqueue_prepare_cpu() , workqueue_prepare_cpu() , bringup_cpu() , smpboot_unpark_threads() , workqueue_online_cpu() . This hidden things is really hard to find and not easy to track during boot. Especially during boot, they should do something like for_each_online_cpu and init one by one. But I guess, after adding support of cpu hotplug, code kind of merged. Some stuff will be executed whenever a cpu has been teardown or bought up. And bang, why not use the same set of hotplug during boot, right? Well.","title":"Feb 2018"},{"location":"lego/log/log-02-2018/#feb-2018","text":"","title":"Feb 2018"},{"location":"lego/log/log-02-2018/#0228-wed","text":"patch fork, and cow handler debug pcache, while running python hello world add vDSO, gettimeofday So, it is end of the day. After adding wp handler, I now have the whole picture of pcache activities, and the interactions between them. The reclaim, zap, move, copy, add, operations needs to be carefully synchronized. Also the refcount etc. I feel the ground rule is we need to make sure a PCM that a function is currently using, can not suddenly become invalid due to other operations. This has to be synced by: refcount, lock, flags. Oh well, mm is hard with SMP, but also fun. We are very close to have a fully working OS. I did not have time to look into the python hello world bug issue. It is a very serious one. It may also rule out some root bugs.","title":"02/28 Wed"},{"location":"lego/log/log-02-2018/#0227-tue","text":"Spent two days on CS527 source project, implemented a small SSHD and SSD client. And we have to inject exactly five bugs, or vulnerabilities into the systems. Lol, it is really hard to intentionally plant BUGs! Anyway, back to Lego. Since others are having a hard time compile program statically, I will try to add dynamic loader today. The interpreter: /lib64/ld-linux-x86-64.so.2 . Linux seq.c maps (no randomization): 00400000-00401000 r-xp 00000000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00600000-00601000 r--p 00000000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00601000-00602000 rw-p 00001000 fd:00 18752683 /root/ys/LegoOS/usr/a.out 00602000-00604000 rw-p 00000000 00:00 0 [heap] 7ffff7a18000-7ffff7bd0000 r-xp 00000000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7bd0000-7ffff7dd0000 ---p 001b8000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd0000-7ffff7dd4000 r--p 001b8000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd4000-7ffff7dd6000 rw-p 001bc000 fd:00 55051990 /usr/lib64/libc-2.17.so 7ffff7dd6000-7ffff7ddb000 rw-p 00000000 00:00 0 7ffff7ddb000-7ffff7dfc000 r-xp 00000000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7fde000-7ffff7fe1000 rw-p 00000000 00:00 0 7ffff7ff9000-7ffff7ffa000 rw-p 00000000 00:00 0 7ffff7ffa000-7ffff7ffc000 r-xp 00000000 00:00 0 [vdso] 7ffff7ffc000-7ffff7ffd000 r--p 00021000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7ffd000-7ffff7ffe000 rw-p 00022000 fd:00 55051983 /usr/lib64/ld-2.17.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] lego after loading 00400000-00401000 r-xp 00000000 /root/ys/LegoOS/usr/a.out 00600000-00602000 rw-p 00000000 /root/ys/LegoOS/usr/a.out 00602000-00604000 rw-p 00000000 [heap] 7ffff7ddb000-7ffff7dfc000 r-xp 00000000 /lib64/ld-linux-x86-64.so.2 7ffff7ffc000-7ffff7ffe000 rw-p 00021000 /lib64/ld-linux-x86-64.so.2 7ffff7ffe000-7ffff7fff000 rw-p 00000000 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] [ 2066.379224] **** Finish dump final mm [ 2066.426023] handle_p2m_execve(): reply_status: OKAY, new_ip: 0x7ffff7ddc170, new_sp: 0x7fffffffede0 [ 2066.628949] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:150 vaddr:0x7ffff7ddc170 [ 2066.732034] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:150 vaddr:0x7ffff7ddc170 [ 2066.934947] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:51 vaddr:0x7fffffffedd8 [ 2067.036978] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:51 vaddr:0x7fffffffedd8 [ 2067.238842] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ffce00 [ 2067.340880] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ffce00 [ 2067.542747] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:51 vaddr:0x7ffff7ffd9a8 [ 2067.644774] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:51 vaddr:0x7ffff7ffd9a8 [ 2067.846640] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ddb8e0 [ 2067.948679] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7ddb8e0 [ 2068.355424] ------------[ cut here ]------------ [ 2068.408568] WARNING: CPU: 4 PID: 31 at managers/memory/handle_pcache/fault.c:54 handle_p2m_pcache_miss+0x29d/0x380 [ 2068.532327] src_nid:0,pid:32,vaddr:0x7ffff7e0e000 [ 2068.588487] CPU: 4 PID: 31 Comm: mc-manager 4.0.0-lego-ys+ #100 [ 2068.659207] Stack: [root@wuklab13: lib64] $ ll ld-* -rwxr-xr-x 1 root root 164112 Nov 30 13:53 ld-2.17.so lrwxrwxrwx 1 root root 10 Jan 8 12:34 ld-linux-x86-64.so.2 -> ld-2.17.so [root@wuklab13: lib64] It turns out there is a bug in mmap code: forgot to increment the file ref count when a file-backed vma is created. Some put_file in loader accidentally free the ld-linux file. Bug fixed, dyloader works like a charm.","title":"02/27 Tue"},{"location":"lego/log/log-02-2018/#0224-sat","text":"Well. PhDs do not have weekends. Anyway, it is Saturday after all, relaxed a little bit. I was looking into the pcache issue. Also added our own kernel version strace.","title":"02/24 Sat"},{"location":"lego/log/log-02-2018/#0223-fri","text":"","title":"02/23 Fri"},{"location":"lego/log/log-02-2018/#solved-fpu-bug","text":"current is fine. I should not compare the old implementation with the new per-cpu current. I forgot that the kernel stack is switched in the __switch_to_asm . This means in __switch_to() , we are actually using the next_p \u2018s kernel stack. So there is small time frame, where current_thread_info() points to next_p , while current_task is still prev_p . Since interrupts are disabled during context switch, we are good with this mismatch. Rule out current, the only thing left is fpu__copy warning, which happens during copy_process() . One weird thing is this function has been called multiple times before it showed a warning. System itself use this function to create a lot background threads, which are fine. Only when it was triggered by sys_clone then we have the warning: [ 3213.055639 ] CPU : 6 PID : 17 sys_clone + 0x0 / 0x30 [ 3213.056584 ] new task_struct : ffff88083e4c9838 [ 3213.057530 ] arch_dup_task_struct cpu6 dst : ffff88083e4c9838 17 word_count - seq src : ffff88083e457838 17 word_count - seq [ 3213.059536 ] TRAP do_general_protection in CPU6 , error_code : 0 current : ffff88083e457838 17 word_count - seq [ 3213.061289 ] fixup_exception pid ( 17 ) cpu ( 6 ) insn : 0xffffffff81009a21 ( fpu__copy + 0x81 / 0x260 ) fixup : 0xffffffff8105d9b2 ( __fixup_text_start + 0xc2 / 0x322 ) handler : ex_handler_default + 0x0 / 0x20 [ 3213.064114 ] ------------ [ cut here ] ------------ [ 3213.065040 ] WARNING : CPU : 6 PID : 17 at . / arch / x86 / include / asm / fpu / internal . h : 354 fpu__copy + 0xc3 / 0x260 [ 3213.066760 ] CPU : 6 PID : 17 Comm : word_count - seq 4.0.0 - lego + # 6 [ 3213.067855 ] Stack : [ 3213.068424 ] ffff88083e4c7dd0 ffffffff810124b5 ffff88083e4c9bf8 ffff88083e4c9c38 [ 3213.070133 ] ffff88083e4c9838 00007ff ff7ffd700 ffff88083e4c7de0 ffffffff8101258f [ 3213.071775 ] ffff88083e4c7e08 ffffffff81009a63 ffff88083e457838 ffff88083e4c9838 [ 3213.073419 ] ffff88083e457838 ffff88083e4c7e40 ffffffff81000ebb ffff88083e457838 [ 3213.075057 ] ffff880800000011 ffff88083e457a68 00000000003 d0f00 ffff88083e457838 [ 3213.076703 ] Call Trace : [ 3213.077295 ] < TSK > [ 3213.077828 ] [ < ffffffff810124c1 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 3213.078855 ] [ < ffffffff8101258f > ] warn_slowpath_null + 0xf / 0x20 [ 3213.081653 ] [ < ffffffff81009a63 > ] fpu__copy + 0xc3 / 0x260 [ 3213.082543 ] [ < ffffffff81000ebb > ] arch_dup_task_struct + 0x7b / 0x90 [ 3213.083667 ] [ < ffffffff8101d32e > ] copy_process + 0x14e / 0x10e0 [ 3213.084618 ] [ < ffffffff8103a3c6 > ] ? n_tty_write + 0x166 / 0x3c0 [ 3213.085564 ] [ < ffffffff8101e2e6 > ] do_fork + 0x26 / 0x140 [ 3213.086439 ] [ < ffffffff8101e4a0 > ] ? sys_vfork + 0x40 / 0x40 [ 3213.087333 ] [ < ffffffff8101e4a0 > ] ? sys_vfork + 0x40 / 0x40 [ 3213.088232 ] [ < ffffffff8101e4c9 > ] sys_clone + 0x29 / 0x30 [ 3213.089109 ] [ < ffffffff8100e719 > ] do_syscall_64 + 0x69 / 0xf0 [ 3213.090030 ] [ < ffffffff8100d5ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3213.091078 ] < EOT > [ 3213.091580 ] --- [ end trace 0000000000000000 ] --- [ 3213.093250 ] TRAP do_general_protection in CPU7 , error_code : 0 current : ffff88083fd0f008 0 swapper / 7 [ 3213.096526 ] fixup_exception pid ( 0 ) cpu ( 7 ) insn : 0xffffffff81000c62 ( __switch_to + 0x452 / 0x630 ) fixup : 0xffffffff8105d922 ( __fixup_text_start + 0x32 / 0x322 ) handler : ex_handler_default + 0x0 / 0x20 [ 3213.101241 ] ------------ [ cut here ] ------------ [ 3213.103285 ] WARNING : CPU : 7 PID : 0 at . / arch / x86 / include / asm / fpu / internal . h : 369 __switch_to + 0x47e / 0x630 So, dig into fpu__copy() , find out why it fails at this certain point. Glad I have something to dig into. The instruction leads to GP is: ffffffff8100b0f5 : 48 0f ae 27 xsave64 ( % rdi ) which is generated by: #define XSTATE_XSAVE(st, lmask, hmask, err) \\ asm volatile(ALTERNATIVE_2(XSAVE, \\ XSAVEOPT, X86_FEATURE_XSAVEOPT, \\ XSAVES, X86_FEATURE_XSAVES) \\ \"\\n\" \\ \"xor %[err], %[err]\\n\" \\ \"3:\\n\" \\ \".pushsection .fixup,\\\"ax\\\"\\n\" \\ \"4: movl $-2, %[err]\\n\" \\ \"jmp 3b\\n\" \\ \".popsection\\n\" \\ _ASM_EXTABLE(661b, 4b) \\ : [err] \"=r\" (err) \\ : \"D\" (st), \"m\" (*st), \"a\" (lmask), \"d\" (hmask) \\ : \"memory\") static inline void copy_xregs_to_kernel ( struct xregs_state * xstate ) { u64 mask = - 1 ; u32 lmask = mask ; u32 hmask = mask >> 32 ; int err ; WARN_ON ( ! alternatives_patched ); XSTATE_XSAVE ( xstate , lmask , hmask , err ); /* We should never fault when copying to a kernel buffer: */ WARN_ON_FPU ( err ); } From SDM on XSAVE : Use of a destination operand not aligned to 64-byte boundary (in either 64-bit or 32-bit modes) results in a general-protection (#GP) exception. In 64-bit mode, the upper 32 bits of RDX and RAX are ignored. %rdi is struct xregs_state *xstate in above code. Thus, check if xstate if 64-bytes aligned. Of course, it is not: [10894.999997] copy_xregs_to_kernel CPU6 xstate: ffff88083e4c8c38 Hehe. Criminal identified. But why? The xstate structure is already marked as __attribute__(aliged 64) in the code. It is the task_struct , which is NOT 0x40 aligned. But god why? Because we currently use kmalloc to allocate new task_struct, whose minimum alignment is 8 bytes . Anyway, use __alloc_pages instead. Such an deeply hidden bug. Took me almost a month to find out.","title":"Solved FPU BUG"},{"location":"lego/log/log-02-2018/#ib","text":"Seen this during boot (at both P and M, although lego continue running correctly): [ 54017.712533 ] *** NodeID Hostname LID QPN [ 54017.770776 ] *** ------------------------------------- [ 54017.834220 ] *** 0 wuklab12 13 72 [ 54017.892462 ] *** 1 wuklab14 16 72 <--- [ 54017.955906 ] *** 2 wuklab16 20 74 [ 54018.014149 ] *** [ 54074.552844 ] *** Start establish connection ( mynodeid : 1 ) [ 54102.554407 ] ib_process_mad mad_ifc fails [ 54130.960691 ] *** recvpollcq runs on CPU2 [ 54131.070918 ] *** Successfully built QP for node 0 [ LID : 13 QPN : 72 ] [ 54131.152936 ] *** Successfully built QP for node 2 [ LID : 20 QPN : 74 ] [ 54161.228245 ] *** FIT layer ready to go ! [ 54161.272034 ] *** Another one: [ 1966.930409 ] *** [ 1966.951210 ] *** FIT_initial_timeout_s : 30 [ 1967.002168 ] *** FIT_local_id : 0 [ 1967.052087 ] *** [ 1967.072887 ] *** NodeID Hostname LID QPN [ 1967.131126 ] *** ------------------------------------- [ 1967.194567 ] *** 0 wuklab12 13 72 <--- [ 1967.258005 ] *** 1 wuklab14 16 72 [ 1967.316244 ] *** 2 wuklab16 20 74 [ 1967.374484 ] *** [ 2032.926448 ] *** Start establish connection ( mynodeid : 0 ) [ 2032.996068 ] Fail to modify qp [ 6 ] [ 2033.032572 ] Fail to do client_init_ctx [ 2033.077287 ] client_establish_conn : ctx ( null ) fail to init_interface [ 2033.164646 ] ibapi_establish_conn : ctx ( null ) fail to init_interface [ 2033.250967 ] *** [ 2035.620167 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000004 [ 2035.713763 ] IP : [ < ffffffff8105c589 > ] client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0 [ 2035.812562 ] PGD 0 [ 2035.836482 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 2035.884321 ] CPU : 0 PID : 1 Comm : kernel_init 4.0.0 - lego - ys + # 253 [ 2035.955041 ] RIP : 0010 : [ < ffffffff8105c589 > ] [ < ffffffff8105c589 > ] client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0 ... [ 2037.313267 ] < TSK > [ 2037.336146 ] [ < ffffffff8105a377 > ] ibapi_send_reply_timeout + 0x57 / 0x70 [ 2037.411025 ] [ < ffffffff81033d24 > ] ? net_send_reply_timeout + 0x94 / 0x132 [ 2037.486944 ] [ < ffffffff81033d24 > ] net_send_reply_timeout + 0x94 / 0x132","title":"IB"},{"location":"lego/log/log-02-2018/#pcache","text":"Running word_count-pthread, with 100MB dataset, finally got some reasonable bug: [ 54211.243181 ] pcache_evict_line () : pset : ffff88207f86e3c0 , for uva : 0x7ffff1b8f000 [ 54211.385654 ] pcache : ffff88207f86e3a8 mapcount : 8 refcount : 0 flags :() [ 54211.510447 ] pcache dumped because : PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm )) [ 54212.080336 ] BUG : failure at managers / processor / pcache / evict . c : 240 / pcache_evict_line () ! [ 54212.664785 ] Kernel Panic - not syncing : BUG ! [ 54212.715742 ] CPU : 8 PID : 81 Comm : word_count - pthr 4.0.0 - lego - ys + # 252 ... [ 54213.391706 ] < TSK > [ 54213.414584 ] [ < ffffffff81024180 > ] panic + 0xc2 / 0xeb [ 54213.524818 ] [ < ffffffff8101b81c > ] ? task_tick_rt + 0x2c / 0xd0 [ 54213.589295 ] [ < ffffffff81018f75 > ] ? scheduler_tick + 0x55 / 0x60 [ 54213.655850 ] [ < ffffffff81016625 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 54213.728647 ] [ < ffffffff81006634 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 54213.801443 ] [ < ffffffff8100e22a > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 54213.879439 ] [ < ffffffff8101256d > ] ? printk + 0x11d / 0x1b0 [ 54214.103027 ] [ < ffffffff8102ecf4 > ] pcache_evict_line + 0x134 / 0x220 [ 54214.172703 ] [ < ffffffff8102c6ae > ] pcache_alloc + 0x22e / 0x2e0 [ 54214.237179 ] [ < ffffffff8102be0a > ] common_do_fill_page + 0x2a / 0x1f0 [ 54214.307895 ] [ < ffffffff8102baf0 > ] ? move_page_tables + 0x4c0 / 0x4c0 [ 54214.378612 ] [ < ffffffff8102c172 > ] pcache_handle_fault + 0x1a2 / 0x3a0 [ 54214.450367 ] [ < ffffffff8100fc02 > ] do_page_fault + 0xa2 / 0x1a0 [ 54214.514843 ] [ < ffffffff8100d85f > ] page_fault + 0x1f / 0x30 [ 54214.575161 ] [ < ffffffff81034842 > ] ? copy_user_enhanced_fast_string + 0x2 / 0x10 [ 54214.657316 ] [ < ffffffff81032368 > ] ? seq_read + 0x248 / 0x360 [ 54214.719714 ] [ < ffffffff810307af > ] sys_read + 0x3f / 0xc0 [ 54214.777949 ] [ < ffffffff81030770 > ] ? sweep_pset_lru + 0x220 / 0x220 [ 54214.846587 ] [ < ffffffff8100e619 > ] do_syscall_64 + 0x69 / 0xf0 [ 54214.910022 ] [ < ffffffff8100d4ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 54214.985939 ] < EOT > Another one: [ 735.393244 ] pcache_evict_line () : pset : ffff88207f86e3c0 , for uva : 0x7ffff1b8fd90 [ 735.537804 ] pcache : ffff88207f86e3a8 mapcount : 8 refcount : 0 flags :() [ 735.663642 ] pcache dumped because : PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm )) Do note this happens after computation. This happens when phoenix create a lot threads to sort the results. Both bug happen to the same set, same user page. The pcache is clearly corrupted: mapcount:8, refcount:0, flags:(). Come back after dinner. Remember to check altenative, cause the XSAVE above should be XSAVEOPT. Make sure it does not override other memory. Also, check linker script. Do not forget to link any sections. Another several bug logs in wuklab13 and wuklab15: 022318-* . I\u2019m really tired today after fixing the FPU bug. But I\u2019m also pretty confident pcache is something I\u2019m able to debug. Even thought it is hard in SMP case. Anyway, I gonna call for the day.","title":"pcache"},{"location":"lego/log/log-02-2018/#0222-thur","text":"context switch fpu signal compat check, all good. make current use percpu current_task, so all code in Lego is consistent. checked entry_SYSCALL-64 again, which looks good to me. The only concern is rsp_scratch and current_top_of_stack , which are per-cpu variables. If these per-cpu is setup wrong, then we are doomed. Also check if per-cpu is all cleared up? try big syscall lock does x86 has to use different kernel stacks? Interrupt is using different stack in Linux, has to do so??? check current is correct. compare with old implementation. First of all, FPU is definitely functional for now. Since I replaced the current macro today, I add some code to check if this current matches our old implementation: static __always_inline struct task_struct *get_current(void) { return this_cpu_read_stable(current_task); } //#define current get_current() #define current \\ ({ \\ struct task_struct *old = current_thread_info()->task; \\ struct task_struct *new = get_current(); \\ \\ if (old != new) { \\ printk(\"%s:%d() cpu:%d old:%pS %d %s new:%pS %d %s\\n\", \\ __func__, __LINE__, smp_processor_id(), old, old->pid, old->comm, \\ new, new->pid, new->comm); \\ BUG(); \\ } \\ get_current(); \\ }) Combined with some FPU warning, it is now like this: [ 3273.748819 ] CPU : 5 PID : 32 sys_clone + 0x0 / 0x30 [ 3273.800808 ] alloc_task_struct_node : size : 740 ffff88107e831838 [ 3273.869451 ] arch_dup_task_struct () CPU5 current : 32 new : ffff88107e831838 old : ffff88107e827838 32 [ 3273.975533 ] ------------ [ cut here ] ------------ [ 3274.030651 ] WARNING : CPU : 5 PID : 32 at . / arch / x86 / include / asm / fpu / internal . h : 354 fpu__copy + 0xe2 / 0x310 [ 3274.140895 ] CPU : 5 PID : 32 Comm : word_count - pthr 4.0.0 - lego - ys - gdbe6dbe - dirty # 249 [ 3274.231377 ] Stack : [ 3274.255298 ] ffff88107e82fd68 ffffffff81016dbf 00000000ff ffffff 0000000000000000 [ 3274.342659 ] 00000000ff ffffff 0000000000000000 ffff88107e831bf8 ffff88107e831c38 [ 3274.430021 ] ffff88107e831838 000000207f e64000 ffff88107e82fd78 ffffffff810170af [ 3274.517382 ] ffff88107e82fdc0 ffffffff8100b052 0000000000000020 ffff88107e831838 [ 3274.604745 ] ffff88107e827838 ffff88107e827838 ffff88107e831838 ffff88107e827838 [ 3274.692106 ] Call Trace : [ 3274.721229 ] < TSK > [ 3274.744109 ] [ < ffffffff81016dd8 > ] __warn . constprop .0 + 0xe8 / 0x3b0 [ 3274.813790 ] [ < ffffffff810170af > ] warn_slowpath_null + 0xf / 0x20 [ 3274.881391 ] [ < ffffffff8100b052 > ] fpu__copy + 0xe2 / 0x310 [ 3274.941713 ] [ < ffffffff810012e4 > ] arch_dup_task_struct + 0x84 / 0x120 [ 3275.013475 ] [ < ffffffff81022c10 > ] copy_process + 0x160 / 0x1e60 [ 3275.078996 ] [ < ffffffff81024936 > ] do_fork + 0x26 / 0x140 [ 3275.137238 ] [ < ffffffff81024af0 > ] ? sys_vfork + 0x40 / 0x40 [ 3275.198599 ] [ < ffffffff81024af0 > ] ? sys_vfork + 0x40 / 0x40 [ 3275.259960 ] [ < ffffffff81024b19 > ] sys_clone + 0x29 / 0x30 [ 3275.319242 ] [ < ffffffff81012314 > ] do_syscall_64 + 0x84 / 0x240 [ 3275.383723 ] [ < ffffffff8101106c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3275.459645 ] < EOT > [ 3275.482526 ] --- [ end trace 0000000000000000 ] --- [ 3275.537648 ] wake_up_new_task CPU5 task : ffff88107e831838 , dest_cpu : 6 current : 32 [ 3275.623970 ] SMP IPI : reschedule_interrupt () CPU ( 6 ) PID ( 0 ) [ 3275.739412 ] do_general_protection : 186 () cpu : 6 old : 0xffff88107e831838 33 word_count - pthr new : 0xffff88107fcaf008 0 swapper / 6 [ 3275.871493 ] ------------ [ cut here ] ------------ [ 3275.926614 ] BUG : failure at arch / x86 / kernel / traps . c : 186 / do_general_protection () ! [ 3276.015018 ] Kernel Panic - not syncing : BUG ! [ 3276.065978 ] panic : 107 () cpu : 6 old : 0xffff88107e831838 33 word_count - pthr new : 0xffff88107fcaf008 0 swapper / 6 Based on the switch code: __switch_to ( struct task_struct * prev_p , struct task_struct * next_p ) { this_cpu_write ( current_task , next_p ); /* Reload sp0 This changes current_thread_info(). */ load_sp0 ( tss , next ); } Based on log line 30, load_sp0() already happened, which means this_cpu_write(..) happened too. If this_cpu_write(..) happened, then log line 30\u2019s new should have been updated to 0xffff88107e831838 . Something wrong with percpu?","title":"02/22 Thur"},{"location":"lego/log/log-02-2018/#0221-wed","text":"irq_regs, old code, check signal frame, and fpu hook together Done in_interrupt() , it is empty, TODO check arch/x86/Makefile, it introduce a lot FPU flags. added more than 4K lines today. Damn FPU. Ugh go home sleep.","title":"02/21 Wed"},{"location":"lego/log/log-02-2018/#0220-tue-cloudy","text":"Not too many Sunny days recently. Well, continue yesterday\u2019s work. I don\u2019t think I can easily find out why so many /proc/memoinfo open happened. Instead, I\u2019m trying to enable the flush_thread in P\u2019s exec code. During the way, I found some issue related to __ARCH_HAS_SA_RESTORER in signal code. I need to check if these x86 macros are defined, but lego does not port them. Well, it turns out flush_thread does not make too much difference. Next I\u2019m going to try to disable exit_thread , which uses fpu__drop() . Hmm, disable exit_thread also does not work.","title":"02/20 Tue Cloudy"},{"location":"lego/log/log-02-2018/#0219-mon-rainy","text":"It is another week. I can not deny I\u2019m a little tired about the bug. Tried so many possible solutions, but none of them work. Well, today I first need to test the vma changes (pgoff and anon_vma) thing. Especially the vma merge and split. This morning I fixed a bug in kernel_init process: make kernel_init able to run all possible CPUs. Because the first user process is forked from kernel_init, it is quite important that it gets the right cpu affinity: static int kernel_init ( void * unused ) { ... set_cpus_allowed_ptr ( current , cpu_possible_mask ); ... } Well, interestingly, the unmodified word_count-pthread succeed with 50MB dataset\u2026 with or without any DEBUG option! Amazing! I need to find out why the cpus_allowed becomes 0 at the beginning of kernel_init. Because init_task actually has: . cpus_allowed = CPU_MASK_ALL , . nr_cpus_allowed = NR_CPUS , Things to do next: check why the cpus_allowed changed check why word_count-pthread open /dev/../cpu so many times. Anything wrong with our copy_files , or open, close? here is an idea, to verify if FPU code is correct, run some scientific benchmarks. Okay, findings: cpus_allowd is fine, it is reset inside sched_init() , when it tries make the init_task as the idle thread. Thus it is reasonable to set cpus_allowed again at kernel_init thread. And it should NOTHING to do with the bug. about the second, check the following log: [ 11838.364543 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 11838.422886 ] STDOUT : --- [ ] --- [ 11838.463445 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_50MB . txt , flags : 0 , mode : 900 [ 11838.619460 ] SYSC_open ( cpu5 pid : 32 ) : fd : 3 [ 11838.667406 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / sys / devices / system / cpu / online , flags : 80000 , mode : 0 [ 11838.773351 ] SYSC_open ( cpu5 pid : 32 ) : fd : 4 [ 11838.821239 ] seq_file : dest_uva : 00007ff fffffc8d0 , nr_chars : 5 string : [ 0 - 23 ] [ 11838.913791 ] SYSC_close ( cpu5 pid : 32 ) : fd : 4 [ 11838.962622 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 11840.223255 ] STDOUT : --- [ Word Count : Computation Completed 1.555581 sec ] --- [ 11840.309678 ] SYSC_open ( cpu5 pid : 32 ) : f_name : / sys / devices / system / cpu / online , flags : 80000 , mode : 0 [ 11840.415754 ] SYSC_open ( cpu5 pid : 32 ) : fd : 4 [ 11840.463593 ] seq_file : dest_uva : 00007ff fffffc8a0 , nr_chars : 5 string : [ 0 - 23 ] [ 11840.556147 ] SYSC_close ( cpu5 pid : 32 ) : fd : 4 [ 11840.605024 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 11840.677821 ] STDOUT : --- [ THe number of processors is 24 \u00f4 ] --- [ 11840.753769 ] SYSC_open ( cpu7 pid : 80 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11840.844212 ] SYSC_open ( cpu19 pid : 92 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11840.935728 ] SYSC_open ( cpu7 pid : 80 ) : fd : 4 [ 11840.983567 ] SYSC_open ( cpu19 pid : 92 ) : fd : 5 [ 11841.032444 ] seq_file : dest_uva : 00007ff ff444c000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11841.305953 ] seq_file : dest_uva : 00007ff ff444b000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11841.579460 ] SYSC_close ( cpu7 pid : 80 ) : fd : 4 [ 11841.628339 ] SYSC_close ( cpu19 pid : 92 ) : fd : 5 [ 11841.678257 ] SYSC_close () : [ 4 ] -> [ / proc / meminfo ] [ 11841.733375 ] SYSC_close () : [ 5 ] -> [ / proc / meminfo ] [ 11841.788493 ] SYSC_open ( cpu18 pid : 91 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11841.880008 ] SYSC_open ( cpu6 pid : 102 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11841.971523 ] SYSC_open ( cpu12 pid : 85 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.063040 ] SYSC_open ( cpu0 pid : 97 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.153516 ] SYSC_open ( cpu14 pid : 87 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.245032 ] SYSC_open ( cpu16 pid : 89 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.336548 ] SYSC_open ( cpu4 pid : 100 ) : f_name : / proc / meminfo , flags : 80000 , mode : 1 b6 [ 11842.428064 ] SYSC_open ( cpu16 pid : 89 ) : fd : 9 [ 11842.476942 ] SYSC_open ( cpu4 pid : 100 ) : fd : 10 [ 11842.526860 ] seq_file : dest_uva : 00007ff ff444c000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11842.800368 ] seq_file : dest_uva : 00007ff ff444b000 , nr_chars : 172 string : [ MemTotal : 115355128 kB MemFree : 115355128 kB MemAvailable : 115355128 kB DirectMap4k : 5812 kB DirectMap2M : 1861632 kB DirectMap1G : 134217728 kB ] [ 11843.073877 ] SYSC_close ( cpu16 pid : 89 ) : fd : 9 However, in a normal Linux exeution: strace - C - o strace_2 . / word_count - pthread . / word_count_datafiles / word_50MB . txt % time seconds usecs / call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 86.41 0.052074 1736 30 futex 6.89 0.004151 67 62 munmap 2.47 0.001490 17 88 mmap 2.12 0.001278 14 93 clone 1.51 0.000912 14 64 mprotect 0.19 0.000117 7 16 write 0.15 0.000092 46 2 open $ cat strace_2 | grep open open ( \"./word_count_datafiles/word_50MB.txt\" , O_RDONLY ) = 3 open ( \"/sys/devices/system/cpu/online\" , O_RDONLY | O_CLOEXEC ) = 4 It opened the /proc/meminfo for way too many times. In the normal Linux execution, this should not happen. Is it because our meminfo is faked, so glibs is complaining? But why it does not open meminfo while running in Linux? Or does our entry assembly messed up some stuff in stack, so the return path changed? oh, about the FPU. It reminds our flush_thread function actually has an issue before. When I enabled this function during loading in P, the P will crash. Within flush_thread , there is a fpu_clear !!! So, check this tomorrow! (12:00am, need to go home)","title":"02/19 Mon Rainy"},{"location":"lego/log/log-02-2018/#0218-sun-sunny","text":"It is a nice day. Yesterday I\u2019ve changed one line of code in mmap code path: change anonymous vma\u2019s pgoff from some value to 0. The result is I got several succeed work-count-pthread(bind to one core) testing. However, it still fail with unmodified word-count-pthread. It brings me to inspect pgoff manipulation code and all mmap.c code. We ported everything from linux without almost zero modification. That means we ported all those useless anon_vma and pgoff code, which is used a lot by vma_merge, vma_split code. The thing is: our memory manager, our vma code do not need such anon_vma structure, and do not maintain pgoff. Thus, I\u2019m a little bit worried linux code may doing some crazy behind our back: mess vma and pages, then pcache miss gets some wrong pages Well. Lego does not use anon_vma , and pgoff should only be used by file-backed vma. So, I decided to remove anon_vma from our code, and make sure pgoff is used properly. Of course, the goal is to make vma_merge, split, copy, do the things we intended. Lesson learned.","title":"02/18 Sun Sunny"},{"location":"lego/log/log-02-2018/#0217-sat-snowy","text":"Fixed the bss bug. It comes from loader. We did not implement the lego_clear_user function, so some part of bss is non-zero. Bad news is word_count-pthread still fail at same fpu instruction. Have to look into memory code more. This is actually a fun debugging story. We should always add TODO or XXX or some warnings to unfinished code, no matter what. Lesson learned.","title":"02/17 Sat Snowy"},{"location":"lego/log/log-02-2018/#0216-fri-cloudy","text":"Yilun found a major loader bug yesterday: the .bss section variables are not 0, in the iozone benchmark. I did not encounter this issue before with simple test program. This is pretty serious.","title":"02/16 Fri Cloudy"},{"location":"lego/log/log-02-2018/#0215-thur-rainy","text":"Today is Chinese New Year. Line 7 and 8 show the uva belong to the same page. Need to revisit get_arg_pages etc functions. [ 108.393991] handle_p2m_execve(): pid:22,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 108.395255] argc[0] (len: 65): /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 108.396329] argc[1] (len: 82): /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt [ 108.397530] envc[0] (len: 7): HOME=/ [ 108.398069] envc[1] (len: 11): TERM=linux [ 108.398640] __bprm_mm_init vma: ffff88083effe6b8 [ 108.399226] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffefed [ 108.399949] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffef94 Well, this is 100% fine. I wrote this loader code long time ago and need some time to pickup. So, after I read the loader code, especially the copy_strings function, I found this is okay. Because copy_strings will be invoked three times, so the faultin_page basically will be invoked at least three times. That is why it went to that pte fault handling code. Although actually I think copy_strings should not use faultin_page , instead, it should use get_user_pages , which will walk through the pgtable first, then went to handle_lego_mm_fault .","title":"02/15 Thur Rainy"},{"location":"lego/log/log-02-2018/#0214-wed-rainy","text":"Hmm, tried to make kmalloc behave as kzalloc, and bind all threads to one core, still gave the same old bug: 42731a: f3 0f 6f 16 movdqu (%rsi),%xmm2 [93182.657376] word_count-pthr[85] general protection ip:42731a sp:7fffe3ffed28 error:0 [93182.747959] CPU: 8 PID: 85 Comm: word_count-pthr 4.0.0-lego+ #170 [93182.820758] RIP: 0033:[<000000000042731a>] [<000000000042731a>] 0x42731a [93182.901878] RSP: 002b:00007fffe3ffed28 EFLAGS: 00010283 [93182.965317] RAX: 000000000000001f RBX: 00007ffff001b010 RCX: 0000000000000005 [93183.050596] RDX: 0000000000000000 RSI: 5345485355420045 RDI: 00007ffff294791f [93183.135876] RBP: 00007ffff294791f R08: 000000000000ffff R09: 0000000000000008 [93183.221156] R10: fffffffffffff048 R11: 00000000004acfc0 R12: 0000000000001cde [93183.306435] R13: 00000000006e4a8c R14: 0000000000001cd7 R15: 0000000000001cda [93183.391716] FS: 00007fffe3fff700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000 [93183.488434] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [93183.557075] CR2: 00007ffff27a4000 CR3: 000000107e924000 CR4: 00000000000406a0 427377: 66 0f 6f 17 movdqa (%rdi),%xmm2 [93180.527248] word_count-pthr[93]: segfault at 0x0 ip 0000000000427377 sp 00007fffdfff6d28 error 4 [93180.630314] CPU: 8 PID: 93 Comm: word_count-pthr 4.0.0-lego+ #170 [93180.703114] RIP: 0033:[<0000000000427377>] [<0000000000427377>] 0x427377 [93180.784234] RSP: 002b:00007fffdfff6d28 EFLAGS: 00010297 [93180.847674] RAX: 0000000000000000 RBX: 000000000073c4c0 RCX: 000000000000000d [93180.932953] RDX: 000000000000ffff RSI: 00007ffff4999070 RDI: 0000000000000000 [93181.018233] RBP: 00007ffff499907d R08: 000000000000ffff R09: 0000000000000000 [93181.103513] R10: 0000000000427760 R11: 00007ffff49982c0 R12: 0000000000000118 [93181.188791] R13: 00000000006e4aac R14: 0000000000000116 R15: 0000000000000117 [93181.274072] FS: 00007fffdfff7700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000 [93181.370790] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [93181.439430] CR2: 0000000000000000 CR3: 000000107e924000 CR4: 00000000000406a0 Tried several ways to ensure memory safety. It still failed even if I enabled all of them. So, I guess the memory safety is ensured? Still some other things? force alloc_pages to use __GFP_ZERO make kmalloc behave as kzalloc make kfree empty I also suspect munmap may free extra wrong pgtable entries. Although I\u2019ve went through all the code and checked, but in addition to the above things, I\u2019m going to: make munmap dummy (no p2m_munmap, return 0 directly) Failed. Next, I\u2019m going to: add checksum for every page transferred across network. add warning for unnormal cases Bang! I found something while running P+M: [ 115.727597 ] Memory - component manager is up and running . [ 116.691723 ] handle_p2m_fork () : nid : 0 , pid : 22 , tgid : 22 , parent_tgid : 1 [ 116.697038 ] handle_p2m_fork () : reply : 0 : OKAY [ 116.791088 ] handle_p2m_execve () : pid : 22 , argc : 2 , envc : 2 , file : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread [ 116.792357 ] argc [ 0 ] ( len : 65 ) : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread [ 116.793439 ] argc [ 1 ] ( len : 82 ) : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt [ 116.794653 ] envc [ 0 ] ( len : 7 ) : HOME =/ [ 116.795196 ] envc [ 1 ] ( len : 11 ) : TERM = linux [ 116.795772 ] __bprm_mm_init vma : ffff88083effe6b8 [ 116.796209 ] faultin_page vma : ffff88083effe6b8 [ 116.796729 ] faultin_page vma : ffff88083effe6b8 [ 116.797150 ] handle_pte_fault vma : ffff88083effe6b8 entry : 0xffff88083e8c1067 [ 116.798044 ] pte : ffff88083e8c0ff0 pfn : 0x8083e8c1 flags :( present | writable | user | accessed | dirty | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 116.799462 ] ------------ [ cut here ] ------------ [ 116.800049 ] WARNING : CPU : 4 PID : 15 at managers / memory / vm / fault . c : 148 handle_lego_mm_fault + 0x4d8 / 0x550 [ 116.801148 ] CPU : 4 PID : 15 Comm : mc - manager 4.0.0 - lego + # 78 [ 116.801818 ] Stack : [ 116.802179 ] ffff88083e893c50 ffffffff8100e827 00007ff fffffef94 ffff88083effe6b8 [ 116.803283 ] ffff88083e894008 ffff88083e8c1067 ffff88083e893c60 ffffffff8100e91f [ 116.804387 ] ffff88083e893cf0 ffffffff8102b008 0000000000000031 ffff88083e893cf0 [ 116.805488 ] 00000000000002 96 00003ff fffe00000 ffff800000000067 ffff88083e893d50 [ 116.806590 ] ffff880000000001 ffffffff81066798 ffff88083effe6b8 ffff88083e893d50 [ 116.807691 ] Call Trace : [ 116.808087 ] < TSK > [ 116.808448 ] [ < ffffffff8100e836 > ] __warn . constprop .0 + 0xa6 / 0x100 [ 116.809126 ] [ < ffffffff8100e91f > ] warn_slowpath_null + 0xf / 0x20 [ 116.809802 ] [ < ffffffff8102b008 > ] handle_lego_mm_fault + 0x4d8 / 0x550 [ 116.810505 ] [ < ffffffff8102cfe3 > ] faultin_page + 0x43 / 0xb0 [ 116.811131 ] [ < ffffffff8102dab1 > ] copy_strings . isra .1 + 0xe1 / 0x130 [ 116.811819 ] [ < ffffffff8102dd1e > ] exec_loader + 0x21e / 0x350 [ 116.812457 ] [ < ffffffff8102680a > ] handle_p2m_execve + 0x1aa / 0x290 This is a temporary stack vma that loader created for saving argv and envp. So, this vma was created here: static int __bprm_mm_init ( struct lego_binprm * bprm ) { ... bprm -> vma = vma = kzalloc ( sizeof ( * vma ), GFP_KERNEL ); ... } And then copy_strings will call faultin_page to populate a page for a specific user virtual adddress: int faultin_page ( struct vm_area_struct * vma , unsigned long start , unsigned long flags , unsigned long * kvaddr ) { ... ret = handle_lego_mm_fault ( vma , start , flags , kvaddr ); ... } Eventually, the handle_lego_mm_fault will call handle_pte_fault : static int handle_pte_fault ( struct vm_area_struct * vma , unsigned long address , unsigned int flags , pte_t * pte , pmd_t * pmd , unsigned long * mapping_flags ) { ... if ( ! pte_present ( entry )) { ... } pr_info ( \"%s vma: %p entry: %#lx \\n \" , FUNC , vma , entry . pte ); dump_pte ( pte , NULL ); WARN_ON_ONCE ( 1 ); ... } Apparently, pte is wrong! But I don\u2019t have time today. Continue tomorrow. Hmm forgot that we are saving kernel virtual addresses in the pte. Just take a quick look at the lego_pud_alloc things, seems will have some issues. I defenitly need to check all these stuff tomorrow. I\u2019ve not touch this part for too long!","title":"02/14 Wed Rainy"},{"location":"lego/log/log-02-2018/#0213-tue-sunny","text":"Checking our SLOB allocator today. So I found Yutong\u2019s code is using set_page_private when slob get a new page from buddy. This private field is only intended to be used by buddy to record the order . This mixed usage will confuse buddy and create bug. Even though I removed the set_page_private ( page , 0 ) after free_page , word_count-pthread still fails. Damn.","title":"02/13 Tue Sunny"},{"location":"lego/log/log-02-2018/#0212-mon-cloudy","text":"Add this commit 4cb3a8b6a943c90714fd9bb5e5465ee315f0aa30 : memory: Use kzalloc instead of kmalloc in __bprm_mm_init (loader) This was an potentionl bug that was not triggered previously. It is simply because kmalloc'ed vma contains some garbage area, while later in the pgfault code, we use if (vma->vm_ops && vma->vm_ops->fault) ... to check if it is an file-backed fault. Fortunately the vma->vm_ops happens to have some leftover value. So this bug was triggered. This actually reminds me that this is a series of potential bugs! Even though before I've added things like force GFP_ZERO in all physical page allocation, I missed the kmalloc's case! The story is: I patched the stop_machine code today, and tried to run code with P+M on VM, everything works fine. However, when I tried to run the new code with P+M+S on physical machine, M crashed at a very weird point: [ 7791.998168] handle_p2m_execve(): pid:81,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 7792.129312] BUG: unable to handle kernel NULL pointer dereference at 0000000000000031 [ 7792.222889] IP: [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0 [ 7792.299842] PGD 0 [ 7792.323760] Oops: 0000 [#1] PREEMPT SMP MEMORY [ 7792.376794] CPU: 4 PID: 79 Comm: mc-manager 4.0.0-lego+ #29 [ 7792.443349] RIP: .. [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0 ...... .... [ 7793.750506] Call Trace: [ 7793.779623] <TSK> [ 7793.802501] [<ffffffff810053f4>] ? apic_timer_interrupt+0x54/0x90 [ 7793.875295] [<ffffffff8102e469>] faultin_page+0x9/0x70 [ 7793.936649] [<ffffffff8102ef01>] copy_strings.isra.1+0xe1/0x130 [ 7794.007362] [<ffffffff8102f11e>] exec_loader+0x1ce/0x340 [ 7794.070796] [<ffffffff81027def>] handle_p2m_execve+0x12f/0x200 [ 7794.140469] [<ffffffff810274fb>] mc_manager+0x1ab/0x2b0 [ 7794.202864] [<ffffffff81027350>] ? bitmap_fill+0x33/0x33 [ 7794.266298] [<ffffffff8101c6b7>] kthread+0x107/0x130 [ 7794.325572] [<ffffffff8101c5b0>] ? __kthread_parkme+0x90/0x90 [ 7794.394205] [<ffffffff8100b462>] ret_from_fork+0x22/0x30 So faulting source code is: static int handle_pte_fault ( struct vm_area_struct * vma , unsigned long address , unsigned int flags , pte_t * pte , pmd_t * pmd ) { .... if ( vma -> vm_ops && vma -> vm_ops -> fault ) return do_linear_fault ( vma , address , flags , pte , pmd , entry ) .... Something wrong with vma ? At this loader stage, this vma is a temporaty stack vma created for saving argv and envp . So I look back into the code that created this vma: managers / memory / loader / core . c : static int __bprm_mm_init ( struct lego_binprm * bprm ) { int err ; struct vm_area_struct * vma = NULL ; struct lego_mm_struct * mm = bprm -> mm ; bprm -> vma = vma = kmalloc ( sizeof ( * vma ), GFP_KERNEL ); if ( ! vma ) return - ENOMEM ; The code after this does NOT do necessary cleanup. The vm_ops happens to have some garbage value from last user. So it is not 0, so the above vma->vm_ops is true, and it will try to read vma->vm_ops->fault . And that, my friend, is where garbage turns into crash. This presents a series of potential bugs. Ugh, memory safety !","title":"02/12 Mon Cloudy"},{"location":"lego/log/log-02-2018/#0209-fri-cloudy","text":"Tried to modify Phoneix code: replace realloc with malloc+mempcy . Thus the mremap syscall is avoided, but it still has general protection fault. Same with yesterday, corrupted at __strcmp_sse42 , with corrupted RSI or RDI . So I guess it is not about mremap itself at all. I will follow yesterday\u2019s checking list.","title":"02/09 Fri Cloudy"},{"location":"lego/log/log-02-2018/#0208-thur-cloudy","text":"00000000004272d0 <__strcmp_sse42>: 4272d0: 89 f1 mov %esi,%ecx 4272d2: 89 f8 mov %edi,%eax 4272d4: 48 83 e1 3f and $0x3f,%rcx 4272d8: 48 83 e0 3f and $0x3f,%rax 4272dc: 83 f9 30 cmp $0x30,%ecx 4272df: 77 3f ja 427320 <__strcmp_sse42+0x50> 4272e1: 83 f8 30 cmp $0x30,%eax 4272e4: 77 3a ja 427320 <__strcmp_sse42+0x50> 4272e6: f3 0f 6f 0f movdqu (%rdi),%xmm1 * 4272ea: f3 0f 6f 16 movdqu (%rsi),%xmm2 4272ee: 66 0f ef c0 pxor %xmm0,%xmm0 4272f2: 66 0f 74 c1 pcmpeqb %xmm1,%xmm0 4272f6: 66 0f 74 ca pcmpeqb %xmm2,%xmm1 4272fa: 66 0f f8 c8 psubb %xmm0,%xmm1 4272fe: 66 0f d7 d1 pmovmskb %xmm1,%edx 427302: 81 ea ff ff 00 00 sub $0xffff,%edx 427308: 0f 85 42 0d 00 00 jne 428050 <__strcmp_sse42+0xd80> 42730e: 48 83 c6 10 add $0x10,%rsi 427312: 48 83 c7 10 add $0x10,%rdi 427316: 66 2e 0f 1f 84 00 00 nopw %cs:0x0(%rax,%rax,1) 42731d: 00 00 00 427320: 48 83 e6 f0 and $0xfffffffffffffff0,%rsi 427324: 48 83 e7 f0 and $0xfffffffffffffff0,%rdi 427328: ba ff ff 00 00 mov $0xffff,%edx 42732d: 45 31 c0 xor %r8d,%r8d 427330: 83 e1 0f and $0xf,%ecx 427333: 83 e0 0f and $0xf,%eax 427336: 66 0f ef c0 pxor %xmm0,%xmm0 42733a: 39 c1 cmp %eax,%ecx 42733c: 74 32 je 427370 <__strcmp_sse42+0xa0> 42733e: 77 07 ja 427347 <__strcmp_sse42+0x77> 427340: 41 89 d0 mov %edx,%r8d 427343: 91 xchg %eax,%ecx 427344: 48 87 f7 xchg %rsi,%rdi * 427347: 66 0f 6f 17 movdqa (%rdi),%xmm2 (RDI: 0000000000000000) Frustrating! What is wrong with multithread program? Because of broken FPU-switch code? of inappropriate TLB flush? of IB corrupts memory? of what? ugh? I\u2019m done with this random guess and frustrated general protection or segfault, I need to first make sure underlying kernel is 100% percent correct, this is a checking list: fpu save/restore always fail at some XMM instruction always with corrupted RDI or RSI switch_to_asm %gs and %fs switch_mm (pgd) stack frame set_arch_tls (%fs) glibc\u2019s way of using per thread data some cpu may miss tlb flush kernel entry/exit assembly current_task macro stack_stratch per-cpu data in entry.S futex clear_tid set_tid shared mm robust list interrupts vector array APIC setup IO-APIC timer interrupt cpu_init and Trampoline faked kernel version P side pgfault handling code (SMP) and M side pgfault handling (SMP) mremap, munmap check pgtable boundary In all, check SMP implications Is there any code, that is solely used to test if the underlying kernel has appropriate behaviors? Like glibc test code? How to protect kernel virtual memory? Any existing solutions in Linux? What is the implication of multiple CPU entering kernel at the same time? How can it corrupt user pages? Maybe: kernel entry code, per-cpu data in entry code, fpu code, switch_to, scheduler. Why it always fail at those FPU code i.e. the strcmp function? I failed to compile without those sse, any solution? How it hurt performance?","title":"02/08 Thur Cloudy"},{"location":"lego/log/log-02-2018/#0207-wed-cloudy","text":"20:07 Pushed a small patch on mremap issue. Hope it will work. mremap really makes the whole thing very interesting, will be a very good research finding on combing virtual cache and operating system. Need to go gym with a friend, will be back on debugging late tonight. 9:30 Have two meetings to do today, and an security class, won\u2019t have too much time coding during daytime.","title":"02/07 Wed Cloudy"},{"location":"lego/log/log-02-2018/#0206-tue-sunny","text":"Well. We\u2019ve ruled out both smp_call_function and workqueue yesterday with Yiying\u2019s help. But the multi-thread word-count still fails :-( Single thread word-count just finished 4GB dataset (with 8GB pcache). So what could be still wrong with multithread one???? chill check exit code (Checked) check pcache\u2019s usage of task_struct, should always use the group_leader check cpu boot code and check the switch code again I believe pinpoint the issue in multithread word-count can solve a lot issues, it must be some thread creation, removal, schedule things. How about adding a lock for ibapi, make it sequential? Sweet, I tried, finally it is a bug that we are able to debug . 22:39 Done for today. I\u2019m trying to patch move_pte and pcache_move_pte . Although in theory we defenitly need to patch it, I keep thinking the code before should not trigger any serious bus or memory corruption. Ugh. Maybe it is concurrent mremap that one of them remap from A to B, while another one remap from C to A. It is possible. But my dead brain can not think of this anymore. I\u2019m going to hit the gym and do some squats. 17:01 Criminal found: mremap() and virtual cache did the crime. Interesting, I have not seen any research paper, tech-reports, writeup, code about this, not even the OVC paper, which, by the way, I think they must consider this case. Otherwise, a mremap will simply crash its virtual cache. Many thanks went to my smoke-and-think time. 15:14 Something new came up! After adding a spinlock for ibapi, this showed up (I tried one more time after this, which does not show up). We are lucky to catch this. At least I know where to look at. Also, this is defenitly triggered by mremap . It is seems it is overlapped mremap() . One thing I did not know is which thread trigger this bug, the sweep thread? Cause mremap related pcache rmap functions do not use rmap_get_locked_pte . [ 3826.048774] normal_p2s_open(): f_name: word_100MB.txt, mode: 04400, flags: 0 [ 3827.891622] SYSC_mremap(cpu18): move: [0x7fffe5788000 - 0x7fffe5806000] -> [0x7fffe531b000 - 0x7fffe5399000] [ 3828.178643] SYSC_mremap(cpu14): move: [0x7fffe5941000 - 0x7fffe5980000] -> [0x7fffe57c7000 - 0x7fffe5806000] **** ERROR: mismatched PTE and rmap **** rmap->owner_process: word_count-pthr uva: 0x7fffe57c8000 ptep: ffff88107efe0e40, rmap->page_table: ffff88107efe0e40 **** pcache_pfn: 0x1257c8, pte_pfn: 0x125942 14:00 word_count-pthread : 100MB dataset pcache : 8GB, 8-way victim : 8 entries [ 1294.845313] STDOUT: ---[ Wordcount: Running... ]--- [ 1294.903661] STDOUT: ---[ o; ]--- [ 1294.946301] normal_p2s_open(): f_name: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt, mode: 04400, flags: 0 [ 1295.100517] SYSC_close(): [4] -> [/sys/devices/system/cpu/online] [ 1295.594658] word_count-pthr[59] general protection ip:4272ea sp:7ffff1b8ed28 error:0 [ 1295.685236] CPU: 10 PID: 59 Comm: word_count-pthr 4.0.0-lego+ #113 [ 1295.759070] RIP: 0033:[<00000000004272ea>] [<00000000004272ea>] 0x4272ea [ 1295.840184] RSP: 002b:00007ffff1b8ed28 EFLAGS: 00010283 [ 1295.903621] RAX: 000000000000000f RBX: 00007fffe5a3d010 RCX: 0000000000000001 [ 1295.988893] RDX: 0000000000000000 RSI: 4854005942004441 RDI: 00007ffff1c1e80f [ 1296.074166] RBP: 00007ffff1c1e80f R08: 0000000000000000 R09: 0000000000000010 [ 1296.211435] R10: 0000000000427ce0 R11: 00007ffff1bbb3ba R12: 0000000000001de4 [ 1296.296711] R13: 00000000006e4a80 R14: 0000000000001d9e R15: 0000000000001dc1 [ 1296.433978] FS: 00007ffff1b8f700(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000 [ 1296.582686] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 1296.963297] CR2: 00007ffff1c1e000 CR3: 000000207fd8a000 CR4: 00000000000406a0 So what is this ip:4272ea , let us objdump the binary: 0000000000425e60 <strcmp>: 425e60: 48 8d 05 69 14 00 00 lea 0x1469(%rip),%rax # 4272d0 <__strcmp_sse42> 425e67: f7 05 5f b8 2b 00 00 testl $0x100000,0x2bb85f(%rip) # 6e16d0 <_dl_x86_cpu_features+0x10> 425e6e: 00 10 00 425e71: 75 1a jne 425e8d <strcmp+0x2d> 425e73: 48 8d 05 46 b0 00 00 lea 0xb046(%rip),%rax # 430ec0 <__strcmp_ssse3> 425e7a: f7 05 4c b8 2b 00 00 testl $0x200,0x2bb84c(%rip) # 6e16d0 <_dl_x86_cpu_features+0x10> 425e81: 02 00 00 425e84: 75 07 jne 425e8d <strcmp+0x2d> 425e86: 48 8d 05 03 00 00 00 lea 0x3(%rip),%rax # 425e90 <__GI_strcmp> 425e8d: c3 retq 425e8e: 66 90 xchg %ax,%ax .. .. .. .. 00000000004272d0 <__strcmp_sse42>: 4272d0: 89 f1 mov %esi,%ecx 4272d2: 89 f8 mov %edi,%eax 4272d4: 48 83 e1 3f and $0x3f,%rcx 4272d8: 48 83 e0 3f and $0x3f,%rax 4272dc: 83 f9 30 cmp $0x30,%ecx 4272df: 77 3f ja 427320 <__strcmp_sse42+0x50> 4272e1: 83 f8 30 cmp $0x30,%eax 4272e4: 77 3a ja 427320 <__strcmp_sse42+0x50> 4272e6: f3 0f 6f 0f movdqu (%rdi),%xmm1 * 4272ea: f3 0f 6f 16 movdqu (%rsi),%xmm2 4272ee: 66 0f ef c0 pxor %xmm0,%xmm0 You can see %rsi has some garbage value RSI: 4854005942004441 . Something went wrong. Will it be our FPU? I\u2019m not quite sure. If FPU code has error, why single-thread one succeed? Why it only shows up at multithread ones?","title":"02/06 Tue Sunny"},{"location":"lego/log/log-02-2018/#0205-mon-sunny","text":"From yesterday\u2019s testing of Phoenix, it looks like something is wrong in smp_call_functions() . They are invoked through tlb flush , which was further invoked by mremap , or munmap . The warning from smp is: [ 1260.586696 ] WARNING : CPU : 0 PID : 73 at kernel / smp . c : 129 generic_smp_call_function_single_interrupt + 0xb8 / 0x160 [ 1260.705251 ] CPU : 0 PID : 73 Comm : word_count - pthr 4.0.0 - lego + # 99 [ 1260.777008 ] Stack : [ 1260.800927 ] ffff88207fdffef8 ffffffff8100ec67 ffff88107fc00000 ffff88107fc00000 [ 1260.888283 ] ffffffff8100d410 ffff88207fe23df0 ffff88207fdfff08 ffffffff8100ed5f [ 1260.975639 ] ffff88207fdfff38 ffffffff8100fe68 00007ff fe58c3010 0000000000000f 96 [ 1261.062995 ] 000000000000f 960 0000000000000f 95 ffff88207fdfff48 ffffffff810020dd [ 1261.150351 ] 00007ff ff58869c1 ffffffff8100b2e9 0000000000000f 96 0000000000000f 95 [ 1261.237707 ] Call Trace : [ 1261.266825 ] < TSK > [ 1261.289704 ] [ < ffffffff8100ec76 > ] __warn . constprop .0 + 0xa6 / 0x100 [ 1261.359381 ] [ < ffffffff8100d410 > ] ? pgd_free + 0x90 / 0x90 [ 1261.419699 ] [ < ffffffff8100ed5f > ] warn_slowpath_null + 0xf / 0x20 [ 1261.487295 ] [ < ffffffff8100fe68 > ] generic_smp_call_function_single_interrupt + 0xb8 / 0x160 [ 1261.581931 ] [ < ffffffff810020dd > ] call_function_interrupt + 0x1d / 0x20 [ 1261.655767 ] [ < ffffffff8100b2e9 > ] smp__call_function_interrupt + 0x69 / 0x70 So I decided to look into smp.c a little bit to find out if there is something wrong (I wrote it long time ago). The warning itself is true, it means some inconsistent behavior.. I saw alloc_percpu stuff during call_function_init , hence probably I also need to check percpu code a little code cause I\u2019m not sure if I port all the functionalities. In all, today\u2019s task, check percpu and smp_call_function code. Esp, percpu code, they are crucial and very hard to relate real bugs to it. Well\u2026 things changed. I found a more serious bug: something about cpuhotplug , even though lego is not using it. cpuhotplug is a set of implict callbacks to all different subsystems who want to do some initialization work on each offline->online cpu. Let us dig into how secondary cpu boots: Trampoline .. setup 64 bit mode start_secondary () smp_callin () notify_cpu_starting () ... while ( st -> state < target ) { st -> state ++ ; cpuhp_invoke_callback ( cpu , st -> state , true , NULL ); } cpuhp_invoke_callback () See? There will be some callbacks! What are those callbacks exactly? Well, they are predefined at the kernel/cpu.c . To save the trouble of reading code, I just print what functions are executed, the log is: [ 0.118235] cpuhp_invoke_callback(): 136 CPU:0 page_writeback_cpu_online+0x0/0x20 [ 0.368478] cpuhp_invoke_callback(): 136 CPU:1 smpboot_create_threads+0x0/0x90 [ 0.370196] cpuhp_invoke_callback(): 136 CPU:1 perf_event_init_cpu+0x0/0xa0 [ 0.370403] cpuhp_invoke_callback(): 136 CPU:1 workqueue_prepare_cpu+0x0/0x80 [ 0.371112] cpuhp_invoke_callback(): 136 CPU:1 hrtimers_prepare_cpu+0x0/0x60 [ 0.371339] cpuhp_invoke_callback(): 136 CPU:1 smpcfd_prepare_cpu+0x0/0x80 [ 0.371584] cpuhp_invoke_callback(): 136 CPU:1 relay_prepare_cpu+0x0/0xe0 [ 0.371794] cpuhp_invoke_callback(): 136 CPU:1 rcutree_prepare_cpu+0x0/0x170 [ 0.372333] cpuhp_invoke_callback(): 136 CPU:1 notify_prepare+0x0/0xa0 [ 0.372744] cpuhp_invoke_callback(): 136 CPU:1 bringup_cpu+0x0/0x100 [ 0.008000] cpuhp_invoke_callback(): 136 CPU:1 sched_cpu_starting+0x0/0x60 [ 0.926124] cpuhp_invoke_callback(): 136 CPU:1 smpboot_unpark_threads+0x0/0x90 [ 0.926124] cpuhp_invoke_callback(): 136 CPU:1 perf_event_init_cpu+0x0/0xa0 [ 0.927028] cpuhp_invoke_callback(): 136 CPU:1 workqueue_online_cpu+0x0/0x2a0 [ 0.927768] cpuhp_invoke_callback(): 136 CPU:1 rcutree_online_cpu+0x0/0x70 [ 0.928045] cpuhp_invoke_callback(): 136 CPU:1 notify_online+0x0/0x20 [ 0.928256] cpuhp_invoke_callback(): 136 CPU:1 page_writeback_cpu_online+0x0/0x20 [ 0.928527] cpuhp_invoke_callback(): 136 CPU:1 sched_cpu_activate+0x0/0x190 [ 0.929084] cpuhp_invoke_callback(): 136 CPU:2 smpboot_create_threads+0x0/0x90 [ 0.930240] cpuhp_invoke_callback(): 136 CPU:2 perf_event_init_cpu+0x0/0xa0 [ 0.930434] cpuhp_invoke_callback(): 136 CPU:2 workqueue_prepare_cpu+0x0/0x80 [ 0.931070] cpuhp_invoke_callback(): 136 CPU:2 hrtimers_prepare_cpu+0x0/0x60 [ 0.931264] cpuhp_invoke_callback(): 136 CPU:2 smpcfd_prepare_cpu+0x0/0x80 [ 0.931464] cpuhp_invoke_callback(): 136 CPU:2 relay_prepare_cpu+0x0/0xe0 [ 0.931649] cpuhp_invoke_callback(): 136 CPU:2 rcutree_prepare_cpu+0x0/0x170 [ 0.932245] cpuhp_invoke_callback(): 136 CPU:2 notify_prepare+0x0/0xa0 [ 0.932475] cpuhp_invoke_callback(): 136 CPU:2 bringup_cpu+0x0/0x100 [ 0.008000] cpuhp_invoke_callback(): 136 CPU:2 sched_cpu_starting+0x0/0x60 [ 1.005023] cpuhp_invoke_callback(): 136 CPU:2 smpboot_unpark_threads+0x0/0x90 [ 1.005065] cpuhp_invoke_callback(): 136 CPU:2 perf_event_init_cpu+0x0/0xa0 [ 1.005408] cpuhp_invoke_callback(): 136 CPU:2 workqueue_online_cpu+0x0/0x2a0 [ 1.005729] cpuhp_invoke_callback(): 136 CPU:2 rcutree_online_cpu+0x0/0x70 [ 1.006029] cpuhp_invoke_callback(): 136 CPU:2 notify_online+0x0/0x20 [ 1.006206] cpuhp_invoke_callback(): 136 CPU:2 page_writeback_cpu_online+0x0/0x20 [ 1.006549] cpuhp_invoke_callback(): 136 CPU:2 sched_cpu_activate+0x0/0x190 Interesting! Currently, Lego need to add the smpboot_create_threads() , workqueue_prepare_cpu() , workqueue_prepare_cpu() , bringup_cpu() , smpboot_unpark_threads() , workqueue_online_cpu() . This hidden things is really hard to find and not easy to track during boot. Especially during boot, they should do something like for_each_online_cpu and init one by one. But I guess, after adding support of cpu hotplug, code kind of merged. Some stuff will be executed whenever a cpu has been teardown or bought up. And bang, why not use the same set of hotplug during boot, right? Well.","title":"02/05 Mon Sunny"},{"location":"lego/log/log-03-2018/","text":"March 2018 \u00b6 03/31 Sat \u00b6 Stay humble. Be real. 03/30 Fri \u00b6 Our scheduling, or IB do have issues. I must revisit this. The case is: in P, we boot only 12 cores, and three of them are used by flush, sweep, and IB. So there are 9 cores left for user. Phoenix create 24 threads. During the run, a lot ib timeout will happen. If we have a good scheduling, this should never happen. I probably need to check more on this. Anyway. Today I reorganized the opcode things. And now I\u2019m adding the final large piece of Lego: replication. It should be much simpler than the pcache part. I will first write down what code I need to add, e.g., opcode, handler, buffer mgmt etc. End of day. Want to write down some simple thoughts on building system. Building system is fun, but you have to know that devil is in the details. And, you may end up debugging for many many hours on a very very little issue. But that is how it is. Building system does not mean you are always working on fantastic beautiful ideas. It is always about those little bugs, little things, trivial fixes, that make your system robust and usable. For example, the patch Yilun sent me today is about handling special cases of stat and lseek. The patch does not improve any performance or adding fancy features, it is a minor fix to make user progam run. But this enable us to run TF. I think it is a great patch and it stands for 90% of building systems in middle or late stage. Of course, there are other trivial things on building systems: 1) initialize every possible used variables, can be local variables, malloced buffers. 2) have decent cleanup, which is a counterpart of your initialization, like dequeue list, decrease counter etc. 3) Clear coding style, write code for others, for yourself when you read the code two weeks later. This one is hard, need experience. But can be learned. I think Yilun and Yutong both improved a lot during this project. Me? I learned this from NVM emulator protect. It is a painful one, but also a valuable one. 4) Decent protect source file organization. 5) Remember, draw, the connections between subsystems. By adding this new feature to this subsystem A, will it broke subsystem B, which is using subsystem A. Something like this. 6) clear mind on lock usage, multithread issue. This is the most difficult one. I would say I learned this by coding pcache, or mm. I would say, mm is the most difficult multithread issue one can encounter. 03/26 Mon \u00b6 Spent several days on replication design. Now I\u2019m back on coding and debuging track. Fixed a bug introduced by per-pte lock. A one hided by previous one big giant page table lock. Also add an option to boot socket 0 only if Processor is configured. This is because pcache is normally registered at socket 0, if we schedule user threads to sockets other than socket 0, that will have bad performance. 03/22 Thur \u00b6 Clear Registers for execve() \u00b6 Want to figure out execve problem today. Check if pcache is clean after process_exit. Check if pgtable is clean. Well. Checked, both are clean. The bug looks like the return of main, evevntually does not go to library\u2019s exit. Is it because library pages are not loaded properly? Since the number of pgfault equals to normal setting, I guess it may originate from Memory side. TLB is also flushed, so TLB should not be a hidden issue. Going to check checsum. Well, checsum is okay too. Syscall execve will change ip, sp, flags registers. So it will use iretq instead of sysexit to return to userspace. Got an insteresting IB bug after execve. The CPU5 seems fail to return to userspace, and the CPU0 has the IB bug followed: [ 1201.940681 ] CPU : 5 PID : 32 Comm : seq . o 4.0.0 - lego - ys + # 609 [ 1202.006200 ] RIP : 0033 : [ < 0000000000401 d1d > ] [ < 0000000000401 d1d > ] 0x401d1d [ 1202.087320 ] RSP : 002 b : 00007ff fffffedb0 EFLAGS : 00000200 [ 1202.150760 ] RAX : 0000000000000000 RBX : 00000000004002e0 RCX : 000000000043 b2c7 [ 1202.236041 ] RDX : 00007ff fffffedc8 RSI : 00007ff fffffeb40 RDI : 000000000048f9f 0 [ 1202.321320 ] RBP : 00007ff fffffeb60 R08 : 00000000006 ba4a0 R09 : 00000000006 bc880 [ 1202.406601 ] R10 : 000000000000000f R11 : 0000000000000246 R12 : 0000000000000000 [ 1202.491881 ] R13 : 0000000000401 930 R14 : 0000000000401 9 c0 R15 : 0000000000000006 [ 1202.577161 ] FS : 0000000000000000 ( 0000 ) GS : ffff88207fc40000 ( 0000 ) knlGS : 0000000000000000 [ 1202.673880 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1202.742521 ] CR2 : 000000000042 c9a0 CR3 : 000000207f c2f000 CR4 : 00000000000406 a0 [ 1220.465601 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 1220.557225 ] IP : [ < ffffffff810591ef > ] ib_mad_completion_handler + 0x6f / 0x7c0 [ 1220.638344 ] PGD 0 [ 1220.662265 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 1220.710105 ] CPU : 0 PID : 27 Comm : ib_mad_completi 4.0.0 - lego - ys + # 609 [ 1220.786025 ] RIP : 0010 : [ < ffffffff810591ef > ] [ < ffffffff810591ef > ] ib_mad_completion_handler + 0x6f / 0x7c0 [ 1220.896265 ] RSP : 0000 : ffff88103eea7e30 EFLAGS : 00010246 [ 1220.959704 ] RAX : 0000000000000000 RBX : ffff88103eeac728 RCX : 0000000000000001 [ 1221.044985 ] RDX : 000000002 8000000 RSI : ffff88103ee8f000 RDI : ffff88107ff841d8 [ 1221.130265 ] RBP : ffff88103eea7ec0 R08 : 0000000000000000 R09 : ffff88103eea03c0 [ 1221.215545 ] R10 : ffff88103eea7ea0 R11 : 0000000000000001 R12 : ffff88103ee8c3f0 [ 1221.300825 ] R13 : ffff88103ee8c4e8 R14 : ffff88103eeac620 R15 : ffff88103eeac5f8 [ 1221.386106 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 1221.482825 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1221.551466 ] CR2 : 0000000000000020 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 1221.636746 ] Stack : [ 1221.660666 ] ffff88103eeaac10 ffff881000000001 ffff88103eeaac10 ffff88103eeaab50 [ 1221.748026 ] ffff88107fc05d80 ffff88103eea0000 ffff88103eeac728 000000 8000000000 [ 1221.835386 ] 0000012 83 eea7ea8 ffff88103ee8c9a8 000000007f cf2000 ffff000000000000 [ 1221.922746 ] ffff88107fcf0000 ffff88207ff6cbd8 ffff88107fcf76e8 ffff88103ee8c3f0 [ 1222.010106 ] ffffffff81059180 0000000000000000 ffff88103eea7f48 ffffffff81020866 [ 1222.097466 ] Call Trace : [ 1222.126586 ] < TSK > [ 1222.149466 ] [ < ffffffff81059180 > ] ? ib_mad_send_done_handler . isra .21 + 0x1d0 / 0x1d0 [ 1222.236826 ] [ < ffffffff81020866 > ] kthread + 0xf6 / 0x120 [ 1222.295066 ] [ < ffffffff81020770 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1222.363707 ] [ < ffffffff8100e4b2 > ] ret_from_fork + 0x22 / 0x30 [ root @ wuklab12 : LegoOS git :( master )] $ addr2line - e vmImage - i ffffffff810591ef / root / ys / LegoOS / drivers / infiniband / core / mad . c : 1899 / root / ys / LegoOS / drivers / infiniband / core / mad . c : 2324 It is ib_mad_recv_done_handler () Well\u2026 Eventually, at 22:09, I figured out.. After I cleaned up all registers (except IP, SP, CS, SS, FLAGS) within start_thread, the execve\u2019ed program can run to end successfully. I did not clear the registers because linux does not clear it. I thought this is fine. Glibc should clear it anyway, right? But anyway, this works. 03/21 Wed \u00b6 Task 1: add some checking in ib, flush, sweep thread. 1) If cpu changed, 2) if nr_threads on this core > 1. Had an issue while testing: execve(). I ran a exec.o first, then do execve to run seq.o: wuklab13 0321 - 10 [ 970.380252 ] STDOUT : --- [ uname () : --- [ 970.431212 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x44605d flags : 0x150 [ 1101.862429 ] mlx4_ib_handle_error_cqe syndrome 21 [ 1101.915570 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1101.969649 ] send request failed at connection 4 as 12 [ 1102.029968 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.084046 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.138125 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.192203 ] fit_poll_cq : failed status ( 5 ) for wr_id 1054 [ 1102.256681 ] fit_poll_cq : failed status ( 5 ) for wr_id 1055 [ 1102.321160 ] csum : 442 a97c0 , reply -> csum : 2 d352c33 [ 1102.377319 ] fit_poll_cq : connection 4 Recv weird event as - 1 [ 1102.444916 ] pcache : ffff880180011180 mapcount : 0 refcount : 1 flags :( allocated | usable ) kva : ffff880100446000 [ 1102.558273 ] fit_poll_cq : failed status ( 5 ) for wr_id 1056 [ 1102.622751 ] pcache dumped because : csum mismatch [ 1102.677871 ] fit_poll_cq : connection 4 Recv weird event as - 30704 [ 1102.749627 ] ------------ [ cut here ] ------------ [ 1102.804746 ] fit_poll_cq : failed status ( 5 ) for wr_id 1057 [ 1102.869225 ] BUG : failure at managers / processor / pcache / fault . c : 237 / __pcache_do_fill_page () ! [ 1102.968022 ] fit_poll_cq : connection 4 Recv weird event as - 30704 [ 1103.039780 ] Kernel Panic - not syncing : BUG ! [ 1103.090739 ] CPU : 5 PID : 32 Comm : seq . o 4.0.0 - lego - ys + # 599 [ 1103.156256 ] Stack : [ 1103.180177 ] ffff88103e85be18 ffffffff8102676c ffffffff00000008 ffff88103e85be28 [ 1103.267533 ] ffff88103e85bde0 0000000021475542 00000000000002 96 ffff88103e85ba10 [ 1103.354892 ] ffffffff810195c5 ffff88207fc44980 0000000000000005 ffff88103e85ba28 [ 1103.442249 ] ffffffff81016c75 ffff88103e85ba40 ffff88103e85ba50 ffffffff810065d4 [ 1103.529607 ] ffffffff811d36e0 000000000000003 9 ffffffff81081718 ffff88103e85bb80 [ 1103.616964 ] Call Trace : 03/20 Tue \u00b6 Task 1: calculate failure numbers Task 2: read 0319-4 Log Task 3: opt pte lock Hmm, I finished the per-pte per-pmd lock patch. I think it works. But I do found an issue. When I run MT+2GB, it will create 24 threads. Since I marked 3 CPUs inactive, so all new 24 threads will be scheduled to other cores (I may need to check this!). At some point, Lego P either stuck, or a lot ibapi_send_reply timeout. When I change the cpu_online to may 0-6 , it finished. When I change it to 0-18 , also succeed. I really doubt if actually those pinned threads are not pinned. Need to check. IB Bug again \u00b6 Running MT-phoenix, 2GB, somehow crashed in the middle: [ 60095.857381 ] SYSC_close () CPU6 PID : 33 [ fd : 4 ] -> [ / proc / stat ] [ 60286.127359 ] mlx4_ib_handle_error_cqe syndrome 21 [ 60286.180503 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.234582 ] send request failed at connection 4 as 12 [ 60286.294903 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.348981 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.403062 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.457141 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.511221 ] send request failed at connection 4 as 5 [ 60286.570500 ] fit_poll_cq : failed status ( 5 ) for wr_id 1056 [ 60286.634980 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.689059 ] fit_poll_cq : failed status ( 5 ) for wr_id 1057 [ 60286.753539 ] send request failed at connection 4 as 5 [ 60286.812819 ] fit_poll_cq : failed status ( 5 ) for wr_id 1058 [ 60286.877298 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.931378 ] fit_poll_cq : failed status ( 5 ) for wr_id 1059 [ 60286.995857 ] send request failed at connection 4 as 5 [ 60287.055138 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.109217 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.163297 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.217376 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.271456 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.325536 ] send request failed at connection 4 as 5 [ 60287.384815 ] fit_poll_cq : failed status ( 5 ) for wr_id 1060 [ 60287.449294 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.503375 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 60287.596973 ] IP : [ < ffffffff81063ffd > ] fit_poll_cq + 0x4dd / 0x530 [ 60287.664574 ] send request failed at connection 4 as 5 [ 60287.723853 ] PGD 0 [ 60287.747772 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.801852 ] Oops : 0002 [ # 1 ] PREEMPT SMP PROCESSOR [ 60287.858013 ] send request failed at connection 4 as 5 [ 60287.917292 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 569 [ 60287.988010 ] RIP : 0010 : [ < ffffffff81063ffd > ] [ < ffffffff81063ffd > ] fit_poll_cq + 0x4dd / 0x530 [ 60288.084731 ] RSP : 0000 : ffff88103e84fd88 EFLAGS : 00010206 [ 60288.148170 ] RAX : 000000000000100 8 RBX : ffff88103e848438 RCX : 0000000000000014 [ 60288.233450 ] RDX : 0000000000000000 RSI : ffffffff811d36e0 RDI : ffffffff811dac08 [ 60288.318728 ] RBP : ffff88103e84fea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 60288.404008 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 60288.489288 ] R13 : ffff88207fd6e008 R14 : 0000000000000004 R15 : ffff88103e84fda0 [ 60288.574568 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60288.628647 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 60288.725367 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 60288.794006 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 60288.879285 ] send request failed at connection 4 as 5 [ 60288.938565 ] Stack : [ 60288.962484 ] ffffffff810031d9 000 801 d43e84fda0 0000000000000007 0000000000000424 [ 60289.049844 ] 000000 8100000005 00001008000000f 9 ffff88103e848868 00616e6440000014 [ 60289.137204 ] 0020004000000002 ffff88207fc00000 0000000000000425 000000 8100000005 [ 60289.224563 ] 00001008000000f 9 ffff88103e848868 007370654000000 d 0010004000000002 [ 60289.311922 ] ffffffff81010000 0000000000000426 000000 8100000005 00001008000000f 9 [ 60289.399282 ] Call Trace : [ 60289.428402 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60289.482482 ] < TSK > [ 60289.505361 ] [ < ffffffff810031d9 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 60289.584400 ] send request failed at connection 4 as 5 [ 60289.643680 ] [ < ffffffff81010000 > ] ? __ioremap_caller + 0x170 / 0x570 [ 60289.714400 ] [ < ffffffff81060000 > ] ? cm_work_handler + 0x270 / 0x1450 [ 60289.785119 ] [ < ffffffff81064050 > ] ? fit_poll_cq + 0x530 / 0x530 [ 60289.850639 ] [ < ffffffff81064064 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 60289.917198 ] [ < ffffffff81020c06 > ] kthread + 0xf6 / 0x120 [ 60289.975438 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60290.029518 ] [ < ffffffff81020b10 > ] ? __kthread_parkme + 0x70 / 0x70 [ 60290.098157 ] [ < ffffffff8100e722 > ] ret_from_fork + 0x22 / 0x30 Uuh: [ 1002.803051 ] mlx4_ib_handle_error_cqe syndrome 1 [ 1002.855153 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1002.909232 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1002.963310 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1003.017390 ] fit_poll_cq : failed status ( 1 ) for wr_id 512 [ 1003.080829 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000200 [ 1003.174425 ] IP : [ < ffffffff8105d499 > ] fit_poll_cq + 0x179 / 0x510 [ 1003.242024 ] PGD 0 [ 1003.265943 ] Oops : 0000 [ # 1 ] SMP MEMORY [ 1003.310661 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 149 [ 1003.381380 ] RIP : 0010 : [ < ffffffff8105d499 > ] [ < ffffffff8105d499 > ] fit_poll_cq + 0x179 / 0x510 [ 1003.478098 ] RSP : 0000 : ffff88104e84fd88 EFLAGS : 00010246 [ 1003.541537 ] RAX : ffff880000000000 RBX : ffff88104e848008 RCX : 00000000000000 80 [ 1003.626814 ] RDX : 0000000000000200 RSI : ffffffff811c76e0 RDI : ffffffff811d0988 [ 1003.712092 ] RBP : ffff88104e84fea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 1003.797369 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 1003.882648 ] R13 : ffff88207ff75008 R14 : 0000000000000004 R15 : ffff88104e84fda0 [ 1003.967925 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 1004.064644 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1004.133282 ] CR2 : 0000000000000200 CR3 : 0000000001131000 CR4 : 00000000000406 a0 [ 1004.218559 ] Stack : [ 1004.242479 ] ffffffff810031a9 ffff88104e84fda0 ffffffff81018ef4 0000000000000200 [ 1004.329837 ] 000000 8000000001 0000004 8000000 d7 ffff88104e848c98 00000000 81019302 [ 1004.417194 ] 0014000000000000 ffff88207fc00000 0000000000000201 ffffffff00000005 [ 1004.504552 ] ffff8810000000f9 ffff88104e848c98 0000000000000000 ffff88104e84fe38 [ 1004.591910 ] ffffffff810195a4 0000000000000202 ffff881000000005 ffff8810000000f9 [ 1004.679268 ] Call Trace : [ 1004.708388 ] < TSK > [ 1004.731267 ] [ < ffffffff810031a9 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 1004.810305 ] [ < ffffffff81018ef4 > ] ? resched_curr + 0x34 / 0x40 [ 1004.874783 ] [ < ffffffff810195a4 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 1004.942382 ] [ < ffffffff8105f458 > ] ? __schedule + 0xf8 / 0x1e0 [ 1005.005820 ] [ < ffffffff8105d830 > ] ? fit_poll_cq + 0x510 / 0x510 [ 1005.071338 ] [ < ffffffff8105d844 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 1005.137897 ] [ < ffffffff8101fdc6 > ] kthread + 0xf6 / 0x120 [ 1005.196135 ] [ < ffffffff8101fcd0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1005.264773 ] [ < ffffffff8100e472 > ] ret_from_fork + 0x22 / 0x30 03/19 Mon \u00b6 Not too many days left!!! Got to design full replication mechanism and algorithm today. Merged pull request for pipe , pipe2 and /dev/null from Yilun. Our simple file op mgmt concerns me. I left a note at kernel/fork.c. Got a bug report from Yilun, syscall execv failed. To be honest, I\u2019ve never tried this syscall, always call it directly within kernel. [ 943.650712 ] CPU6 PID17 sys_execve + 0x0 / 0x10 [ 943.701899 ] BUG : unable to handle kernel paging request at 00000000004 90523 [ 943.702776 ] IP : [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.711501 ] PGD 0 [ 943.711911 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 943.712433 ] CPU : 6 PID : 17 Comm : word_count - pthr 4.0.0 - lego + # 64 [ 943.713126 ] RIP : 0010 : [ < ffffffff8103db86 > ] [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.714090 ] RSP : 001 8 : ffff88083e4bfe98 EFLAGS : 00010246 [ 943.714724 ] RAX : 0000000000000000 RBX : ffff88083e4b3780 RCX : 0000000000000000 [ 943.715511 ] RDX : 00000000ff ffffff RSI : 000000000000002f RDI : 00000000004 90523 [ 943.716297 ] RBP : ffff88083e4bfe98 R08 : 0000160000000000 R09 : ffff88083e4b8400 [ 943.717085 ] R10 : ffff880000000000 R11 : 6 db6db6db6db6db7 R12 : ffff88083e4b8000 [ 943.717871 ] R13 : ffff88083e4e6290 R14 : 00000000004 90523 R15 : ffff88083e4b3920 [ 943.718683 ] FS : 0000000000000000 ( 0000 ) GS : ffff88083fd80000 ( 0000 ) knlGS : 0000000000000000 [ 943.719650 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 943.720319 ] CR2 : 00000000004 90523 CR3 : 000000083e4 e7000 CR4 : 00000000000406 a0 [ 943.721106 ] Stack : [ 943.721459 ] ffff88083e4bff18 ffffffff8102c6bf ffff880800000000 0000000000000e10 [ 943.722541 ] 00007ff fffffedb0 0000000000400 d0d ffff88083e4c0000 00000000004 90523 [ 943.723624 ] ffff88083e4b9008 00007ff fffffed30 000000 8400000084 ffff88083e4bff58 [ 943.724706 ] 000000000000003 b 0000000000401 9 d0 0000000000401 a60 0000000000000000 [ 943.725789 ] ffff88083e4bff28 ffffffff8102c989 ffff88083e4bff48 ffffffff8100e5f5 [ 943.726870 ] Call Trace : [ 943.727260 ] < TSK > [ 943.727619 ] [ < ffffffff8102c6bf > ] do_execve + 0x4af / 0x770 [ 943.728236 ] [ < ffffffff8102c989 > ] sys_execve + 0x9 / 0x10 [ 943.728868 ] [ < ffffffff8100e5f5 > ] do_syscall_64 + 0x45 / 0xd0 [ 943.729499 ] [ < ffffffff8100d4ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 943.730222 ] < EOT > [ 943.730570 ] Code : d2 74 18 40 38 f2 89 f1 75 06 eb 0f 38 ca 74 0 b 48 83 c0 01 0f b6 10 84 d2 75 f1 5 d c3 0f 1f 84 00 00 00 00 00 55 31 c0 48 89 e5 < 0f > b6 17 40 38 f2 48 0f 44 c7 48 83 c7 01 84 d2 75 ee 5 d c3 66 [ 943.735455 ] RIP [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.736120 ] RSP < ffff88083e4bfe98 > [ 943.736598 ] CR2 : 00000000004 90523 It is setup_new_exec() -> set_task_comm() . I passed the user pointer to set_task_comm() , which I should pass a kernel pointer. And I actually found we missed a function: do_close_on_exec() . I also add a note above. Random IB Bug \u00b6 Another weird bug after pathing loader. Actually, I tried the same setting twice, the second time it works. I guess this is some random IB bug. (Setting: 1P, 1M, 1S. Running a simple exec.c testing program, this have not reach that point yet.) wuklab13 031 9 - 2 [ 496.288272 ] p2m_fork ( cpu0 ) : I cur : 1 - kernel_init new : 31 [ 496.349624 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000004 [ 496.443216 ] IP : [ < ffffffff81064935 > ] fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0 [ 496.538892 ] PGD 0 [ 496.562811 ] Oops : 0002 [ # 1 ] PREEMPT SMP PROCESSOR [ 496.618968 ] CPU : 0 PID : 1 Comm : kernel_init 4.0.0 - lego - ys + # 559 [ 496.689684 ] RIP : 0010 : [ < ffffffff81064935 > ] [ < ffffffff81064935 > ] fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0 [ 496.814478 ] RSP : 0000 : ffff88107fcf7d00 EFLAGS : 00010202 [ 496.877915 ] RAX : 000000000000004 c RBX : 0000000000000004 RCX : 000000000000002 c [ 496.963190 ] RDX : 0000000000000004 RSI : 0000000000000001 RDI : ffff88207ff6d008 [ 497.048466 ] RBP : ffff88107fcf7d98 R08 : ffff88107fcf7e3c R09 : 0000000000000004 [ 497.133742 ] R10 : ffffffff81145fe0 R11 : 000000000000001 c R12 : 000000000000002 c [ 497.219018 ] R13 : 0000000000000001 R14 : ffff88107fcf7e40 R15 : ffff88207ff6d008 [ 497.304293 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 497.401009 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 497.469645 ] CR2 : 0000000000000004 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 497.554922 ] Stack : [ 497.578840 ] ffff88107fcf7d08 0000000000000000 00000000000002 82 ffffffff81077b10 [ 497.666195 ] 000000000000003 a 000000047f cf7e18 ffff88107fcf7e3c ffff88107fd5ed88 [ 497.753552 ] 000000010000002 c ffffff9b00000040 0000000000000034 ffffffff81145fe0 [ 497.840906 ] ffff88107fcf7db0 00000000000002 97 ffff88107fd5ed88 000000000000002 c [ 497.928263 ] ffff88107fcf7e3c ffff88107fcf7e40 000000000000003 9 ffff88107fcf7dc8 [ 498.015618 ] Call Trace : [ 498.044736 ] < TSK > [ 498.067615 ] [ < ffffffff810622ff > ] ibapi_send_reply_timeout + 0x3f / 0x50 [ 498.142492 ] [ < ffffffff8103b0d4 > ] ? net_send_reply_timeout + 0x94 / 0x132 [ 498.218408 ] [ < ffffffff8103b0d4 > ] net_send_reply_timeout + 0x94 / 0x132 [ 498.292244 ] [ < ffffffff8102c683 > ] p2m_fork + 0xd3 / 0x200 [ 498.351521 ] [ < ffffffff8101f490 > ] do_fork + 0xf0 / 0x150 [ 498.409758 ] [ < ffffffff8101f514 > ] kernel_thread + 0x24 / 0x30 [ 498.473195 ] [ < ffffffff8115bf21 > ] processor_manager_init + 0x21 / 0x50 [ 498.545991 ] [ < ffffffff81000354 > ] kernel_init + 0x94 / 0x120 [ 498.608388 ] [ < ffffffff810002c0 > ] ? 0xffffffff810002c0 [ 498.668706 ] [ < ffffffff81019b0a > ] ? schedule_tail + 0xa / 0x40 [ 498.733182 ] [ < ffffffff810002c0 > ] ? 0xffffffff810002c0 [ 498.793499 ] [ < ffffffff8100e762 > ] ret_from_fork + 0x22 / 0x30 [ 498.856936 ] < EOT > 03/18 Sun \u00b6 Got a bug report after enable preempt and sweep thread [ 582.545444 ] pcache : ffff8801812cb680 mapcount : 1 refcount : 2 flags :( locked | allocated | usable | valid | reclaim ) kva : ffff88014b2da000 [ 582.678677 ] pcache dumped because : PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm )) [ 582.758760 ] rmap : ffff88207e5e37e8 flags : 0x0 owner - tgid : 33 user_va : 0x7fff0b2da000 ptep : ffff88207e4a86d0 [ 582.870046 ] pte : ffff88207e4a86d0 pfn : 0x0 flags :() [ 582.926210 ] ------------ [ cut here ] ------------ [ 582.981333 ] BUG : failure at managers / processor / pcache / victim . c : 604 / victim_finish_insert () ! [ 583.080137 ] Kernel Panic - not syncing : BUG ! ... ... [ 588.847239 ] nr_pgfault : 591101 [ 588.883641 ] nr_clflush : 66176 [ 588.919003 ] nr_pgfault_wp : 0 [ 588.953325 ] nr_pgfault_wp_cow : 0 [ 588.991806 ] nr_pgfault_wp_reuse : 0 [ 589.032368 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 589.091651 ] nr_pcache_fill_from_memory : 587057 [ 589.144694 ] nr_pcache_fill_from_victim : 4038 [ 589.195656 ] nr_pcache_eviction_triggered : 439562 [ 589.250780 ] nr_pcache_eviction_eagain_freeable : 373382 [ 589.312143 ] nr_pcache_eviction_eagain_concurrent : 0 [ 589.370386 ] nr_pcache_eviction_failure_find : 0 [ 589.423429 ] nr_pcache_eviction_failure_evict : 0 [ 589.477512 ] nr_pcache_eviction_succeed : 66176 [ 589.529514 ] nr_victim_eviction_triggered : 733361 [ 589.584638 ] nr_victim_eviction_eagain : 671227 [ 589.636640 ] nr_victim_eviction_succeed : 62134 [ 589.688642 ] nr_victim_prepare_insert : 66180 [ 589.738566 ] nr_victim_finish_insert : 66176 [ 589.787447 ] nr_victim_flush_submitted : 66176 [ 589.838411 ] nr_victim_flush_finished : 66176 [ 589.888332 ] nr_victim_flush_async_run : 0 [ 589.935135 ] nr_victim_flush_sync : 0 [ 589.976738 ] nr_sweep_run : 50580 [ 590.014179 ] nr_sweep_nr_pset : 116770383 [ 590.059943 ] nr_sweep_nr_moved_pcm : 100686435 This is an interesting bug. Two threads, one doing munmap or mremap, one doing eviction. They are using the same pcm. munmap and mremap will use pte_get_and_clear() to get the pcm. While eviction will call pcache_try_to_unamp , which will further call rmap_get_locked_pte() , in which we check if the pte is none, if it is, then we know this is under munmap or mremap, then we skip. This is absolutely wrong. When pcache_try_to_unamp is called by eviction, it should always unmap ALL rmap. The above case is triggered because both two threads skip the final __pcache_remove_rmap . Hmm, looks like open/close filename is wrong. I need to check. Last Log from MT+2GB, computation finished: wuklab13 031 8 - 10 [ 627.280016 ] **** ERROR : *** current : 32 : kevict_sweepd caller : ( null ) **** [ pte == rmap -> page_table ] && [ pcache_pfn != pte_pfn ] **** rmap -> owner_process : word_count - pthr uva : 0x7fff78f52000 ptep : ffff88107e87fa90 , rmap -> page_table : ffff88107e87fa90 **** pcache_pfn : 0x168f52 , pte_pfn : 0x178f52 [ 627.624239 ] rmap : ffff88107dc73740 flags : 0x0 owner - tgid : 33 user_va : 0x7fff78f52000 ptep : ffff88107e87fa90 [ 627.735513 ] pte : ffff88107e87fa90 pfn : 0x0 flags :() [ 627.791670 ] pcache_rmap dumped because : Corrupted RMAP [ 627.853026 ] pcache : ffff880181a3d480 mapcount : 1 refcount : 2 flags :( locked | allocated | usable | valid ) kva : ffff880168f52000 [ 627.979901 ] pcache dumped because : Corrupted RMAP [ 628.036057 ] ------------ [ cut here ] ------------ [ 628.091175 ] BUG : failure at managers / processor / pcache / rmap . c : 109 / report_bad_rmap () ! [ 628.182691 ] Kernel Panic - not syncing : BUG ! [ 628.233647 ] CPU : 5 PID : 32 Comm : kevict_sweepd 4.0.0 - lego - ys + # 543 [ 628.307483 ] Stack : [ 628.331401 ] ffff88107e85bd00 ffffffff81026d24 000000000000000 8 ffff88107e85bd10 [ 628.418756 ] ffff88107e85bcc8 0000000021475542 0000000000000000 0000000000000000 [ 628.506113 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.593468 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.680823 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.768179 ] Call Trace : [ 628.797299 ] < TSK > [ 628.820176 ] [ < ffffffff81026d30 > ] panic + 0xc2 / 0x102 [ 628.876334 ] [ < ffffffff8101c6ac > ] ? task_tick_rt + 0x2c / 0xd0 [ 628.940811 ] [ < ffffffff8101c6ac > ] ? task_tick_rt + 0x2c / 0xd0 [ 629.005288 ] [ < ffffffff81019bfc > ] ? scheduler_tick + 0x5c / 0x70 [ 629.071843 ] [ < ffffffff81017195 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 629.144639 ] [ < ffffffff81006704 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 629.217436 ] [ < ffffffff8100e4da > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 629.295432 ] [ < ffffffff81012d94 > ] ? printk + 0x124 / 0x1c0 [ 629.355748 ] [ < ffffffff8103ad1f > ] report_bad_rmap + 0x144 / 0x144 [ 629.423345 ] [ < ffffffff81031046 > ] pcache_referenced_trylock_one + 0x1c6 / 0x2c0 [ 629.505500 ] [ < ffffffff8100e4da > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 629.583497 ] [ < ffffffff810328a1 > ] rmap_walk + 0x71 / 0xe0 [ 629.642774 ] [ < ffffffff81033329 > ] pcache_referenced_trylock + 0x59 / 0xd0 03/17 Sat \u00b6 I\u2019m too tired today. Coding side, I will only optimize sweep. Besides, I will book tickets for Iceland trip. 03/16 Friday \u00b6 Task 1 : Add physical memory counter. It is a per-zone based counter, even though there is also some global counters. In Linux, per-cpu counter is first accumlated, global counter is updated only when per-cpu ones overflow. Lego\u2019s initial version save the trouble of per-cpu counter, I only port one global counter today, because I\u2019m not quite confident about our percpu_alloc\u2026 Anway, the info is reported in the format of manager_sysinfo . Do note this is different from the oirginal sysinfo structure, which is used by sysinfo syscall. Task 2 : Patch get_random_number and /dev/urandom /dev/random. Others wrote the code, but he did not stick to the tradition of format naming. So I have to rewrite some of them. Sigh. Task 3 : optimize sweep 03/15 Thur \u00b6 Forgot to write the log yesterday. I actually solved the major bug, the refcount and eviction one. That is really nasty. I basically used pte lock, pcache_lock, and refcount to synchronize between eviction routine and other users such as munmap, mremap, write-protected-handler. I\u2019m really not sure if this mode can be reproduced if I have any other similar systems. But I\u2019m glad that I find a way to do this. Today I got few tasks going on. First merge storage syscall branch, then add sched_yield syscall, add zone/node counters, and probably patch get_random_number. Task 1 : Merge Yilun\u2019s storage pull request, has bunch syscalls. I\u2019m reviewing now. truncate ftruncate getdents getcwd mkdir rmdir creat unlink unlinkat readlink statfs sync Task 2 : Add sched_yield() . Fairly simple. Task 3 : Add physical memory counter. Fairly complex. The underlying is built long time ago. Need to pick up some. Well some facts: pg_data_t (and zone) is allcoated by alloc_node_data if NUMA is configured. all zones are built and initliazed in memory_init() in Lego stats are reset to 0 when pg_data_t allocated (DUH?). Played directly in page_alloc.c Have to continue tomorrow. Task 4 : Patch get_random_number and /dev/urandom 03/13 Wed \u00b6 The slow victim flush issue is solved by pinning the thread to a core and remove that core from active_cpu mask. Today I\u2019m going to solve the SMP object issue. I\u2019m hoping by solving this, we can have a complete working pcache and victim cache. Continue yesterday\u2019s log: wuklab13 0313 - 12 [ 1073.616269 ] pcache : ffff880180777a80 mapcount : 0 refcount : 3 flags :( locked | allocated | usable ) kva : ffff88011ddea000 [ 1073.734941 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff4ddea000 [ 1073.822304 ] pcache dumped because : evict / ref bug [ 1073.987667 ] BUG : failure at managers / processor / pcache / evict . c : 301 / pcache_evict_line () ! [ 1074.082308 ] BUG : failure at managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () ! [ 1074.172789 ] Kernel Panic - not syncing : BUG ! [ 1074.223751 ] CPU : 23 PID : 50 Comm : word_count - pthr 4.0.0 - lego - ys + # 476 Time CPU0 CPU1 0 pcache_evict_line() zap_pte_range() 1 find @pcm to evict prepare to unmap pte which points to @pcm 2 lock_pcache() .. 3 pcache_try_to_unmap() pte_offset_lock() 4 try to lock pte pcache_zap_pte() 5 ..spin.. trylock_pcache (failed) 6 ..spin.. unlock pte 7 lock pte trylock pcache 8 clear pte ..spin.. 9 unlock pte ..spin.. 10 unlock pcache ..spin.. 11 .. lock pcache 12 .. lock pte 13 .. HERE, should check if pte changed! Huh, patched both eviction and other code. Use refcount, pcache lock, pte lock to synchronize between all users. Make sure a going-to-be-evicted pcm will not be used by others. And others will not have a chance to use such line. 03/12 Tue \u00b6 Continue victim cache. The current conclusion is victim has a unbalanced input and output rate. That is why some cores timeout and abort. Got some more clean log. The log told us that the flushd_victim is too slow at flushing content. Next I going to print the current flush queue content. Make sure that they are really not flushed. If so, I want to add code to flush sync. [ 318.193591 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 318.330986 ] -- Start Dump Victim Cache -- [ 318.388190 ] -- CPU4 [ word_count - pthr ][ pid = 54 , tgid = 32 ] -- [ 318.456835 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 318.627406 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90748000 [ 318.707492 ] pset : ffff88207f81d200 set_idx : 1864 nr_lru : 8 [ 318.775096 ] [ 318.792778 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 318.963349 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90749000 [ 319.043435 ] pset : ffff88207f81d240 set_idx : 1865 nr_lru : 8 [ 319.111040 ] [ 319.128721 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 319.299292 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90746000 [ 319.379378 ] pset : ffff88207f81d180 set_idx : 1862 nr_lru : 8 [ 319.446983 ] [ 319.464664 ] victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 319.635237 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 319.715321 ] pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 319.782927 ] [ 319.800608 ] victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 319.971179 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90745000 [ 320.051265 ] pset : ffff88207f81d140 set_idx : 1861 nr_lru : 8 [ 320.118870 ] [ 320.136551 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 320.307123 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074c000 [ 320.387208 ] pset : ffff88207f81d300 set_idx : 1868 nr_lru : 8 [ 320.454813 ] [ 320.472494 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 320.643066 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90747000 [ 320.723152 ] pset : ffff88207f81d1c0 set_idx : 1863 nr_lru : 8 [ 320.790756 ] [ 320.808438 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 320.979009 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 321.059096 ] pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 [ 321.126700 ] [ 321.144381 ] -- End Dump Victim Cache -- [ 321.200545 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 321.278552 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 321.511925 ] nr_pgfault : 551908 [ 321.546357 ] nr_clflush : 33449 [ 321.581718 ] nr_pgfault_wp : 0 [ 321.616040 ] nr_pgfault_wp_cow : 0 [ 321.654523 ] nr_pgfault_wp_reuse : 0 [ 321.695087 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 321.754371 ] nr_pcache_fill_from_memory : 546067 [ 321.807414 ] nr_pcache_fill_from_victim : 5750 [ 321.858378 ] nr_pcache_eviction_triggered : 38689 [ 321.912461 ] nr_pcache_eviction_eagain : 5239 [ 321.962385 ] nr_pcache_eviction_succeed : 33449 [ 322.014389 ] nr_victim_eviction_triggered : 41887455 [ 322.071592 ] nr_victim_eviction_eagain : 41859764 [ 322.125676 ] nr_victim_eviction_succeed : 27691 [ 322.177680 ] nr_victim_prepare_insert : 33450 [ 322.227603 ] nr_victim_finish_insert : 33449 [ 322.276487 ] nr_victim_flush_submitted : 33449 [ 322.327451 ] nr_victim_flush_finished : 33449 [ 322.377374 ] nr_victim_flush_async_run : 26989 [ 322.428338 ] nr_victim_flush_sync : 0 Yes, this victims are truly not being flushed. They are inside the flush_queue. No bug, hoo! Just some performance coding issues. But god why the flushd does not get a chance to run in 10 seconds? Hmm\u2026 [ 5520.236187 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff464fa000 [ 5530.404269 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 5530.541664 ] CPU4 PID54 -- Start Dump Victim Cache [ 0 ] [ 5530.606147 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 5530.789194 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90747000 [ 5530.880717 ] CPU4 PID54 rmap to pset : ffff88207f81d1c0 set_idx : 1863 nr_lru : 8 [ 5530.968080 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 5531.151128 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 5531.242652 ] CPU4 PID54 rmap to pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 5531.330015 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 5531.513063 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074c000 [ 5531.604586 ] CPU4 PID54 rmap to pset : ffff88207f81d300 set_idx : 1868 nr_lru : 8 [ 5531.691950 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 5531.874997 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 5531.966521 ] CPU4 PID54 rmap to pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 [ 5532.053885 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 5532.236932 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90748000 [ 5532.328456 ] CPU4 PID54 rmap to pset : ffff88207f81d200 set_idx : 1864 nr_lru : 8 [ 5532.415819 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 5532.598867 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90749000 [ 5532.690390 ] CPU4 PID54 rmap to pset : ffff88207f81d240 set_idx : 1865 nr_lru : 8 [ 5532.777753 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 5532.960802 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90746000 [ 5533.052325 ] CPU4 PID54 rmap to pset : ffff88207f81d180 set_idx : 1862 nr_lru : 8 [ 5533.139689 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 5533.322736 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90745000 [ 5533.414259 ] CPU4 PID54 rmap to pset : ffff88207f81d140 set_idx : 1861 nr_lru : 8 [ 5533.501623 ] CPU4 PID54 -- End Dump Victim Cache [ 0 ] [ 5533.566106 ] CPU4 PID54 -- Start Dump Victim Flush Queue [ 0 ] [ 5533.635789 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 5533.818837 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 5534.001884 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 5534.184931 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 5534.367978 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 5534.551025 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 5534.734074 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 5534.917120 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 5535.100168 ] CPU4 PID54 -- End Dump Victim Flush Queue [ 0 ] [ 5535.169851 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 5535.247854 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 5535.480513 ] nr_pgfault : 549578 [ 5535.514943 ] nr_clflush : 31822 [ 5535.550304 ] nr_pgfault_wp : 0 [ 5535.584625 ] nr_pgfault_wp_cow : 0 [ 5535.623107 ] nr_pgfault_wp_reuse : 0 [ 5535.663669 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 5535.722952 ] nr_pcache_fill_from_memory : 544279 [ 5535.775993 ] nr_pcache_fill_from_victim : 5201 [ 5535.826955 ] nr_pcache_eviction_triggered : 37437 [ 5535.881038 ] nr_pcache_eviction_eagain : 5614 [ 5535.930960 ] nr_pcache_eviction_succeed : 31822 [ 5535.982963 ] nr_victim_eviction_triggered : 42000029 [ 5536.040165 ] nr_victim_eviction_eagain : 41973416 [ 5536.094247 ] nr_victim_eviction_succeed : 26613 [ 5536.146249 ] nr_victim_prepare_insert : 31823 [ 5536.196171 ] nr_victim_finish_insert : 31822 [ 5536.245052 ] nr_victim_flush_submitted : 31822 [ 5536.296015 ] nr_victim_flush_finished : 31822 [ 5536.345937 ] nr_victim_flush_async_run : 26718 [ 5536.396899 ] nr_victim_flush_sync : 0 Hmm, got some interesting bug, which never happened before. We did a unmap before finish_insert , so the mapcount must be zero. Since we have the Reclaim set for the candidate. But it looks like other code does not too much about the Reclaim bit. I need to check. [ 1009.676839 ] victim_flush_async CPU4 jobs 1 [ 1009.725830 ] victim_flush_async CPU4 jobs 1 [ 1009.774423 ] victim_flush_async CPU4 jobs 1 [ 1009.823147 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff465fc000 [ 1009.910479 ] pcache : ffff88018098d740 mapcount : 1 refcount : 3 flags :( locked | allocated | usable | valid | reclaim ) kva : ffff88012635d000 [ 1010.045652 ] pcache dumped because : PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm )) [ 1010.125725 ] victim_flush_async CPU4 jobs 1 [ 1010.174602 ] ------------ [ cut here ] ------------ [ 1010.229717 ] BUG : failure at managers / processor / pcache / victim . c : 601 / victim_finish_insert () ! [ 1010.328509 ] victim_flush_async CPU4 jobs 1 [ 1010.377385 ] Kernel Panic - not syncing : BUG ! [ 1010.428341 ] CPU : 20 PID : 47 Comm : word_count - pthr 4.0.0 - lego - ys + # 468 [ 1010.505294 ] Stack : [ 1010.529212 ] ffff881f2040fe08 ffffffff810259f4 000000000000000 8 ffff881f2040fe18 [ 1010.616565 ] ffff881f2040fdd0 0000000021475542 0000000000000000 0000000000000000 [ 1010.703918 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.791270 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.878623 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.965976 ] Call Trace : [ 1010.995095 ] < TSK > [ 1011.017972 ] [ < ffffffff81025a00 > ] panic + 0xc2 / 0x102 [ 1011.074127 ] [ < ffffffff81063a8a > ] ? client_internal_poll_sendcq + 0x2a / 0x80 [ 1011.154202 ] [ < ffffffff81063c2d > ] ? client_send_message_with_rdma_write_with_imm_request + 0x14d / 0x360 [ 1011.262351 ] [ < ffffffff8101bffc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1011.326827 ] [ < ffffffff81019755 > ] ? scheduler_tick + 0x55 / 0x60 [ 1011.393382 ] [ < ffffffff81016e25 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1011.466175 ] [ < ffffffff810066e4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1011.538969 ] [ < ffffffff8100e4aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1011.616964 ] [ < ffffffff81012cfd > ] ? printk + 0x11d / 0x1b0 [ 1011.677279 ] [ < ffffffff81032a19 > ] victim_finish_insert + 0x89 / 0x230 [ 1011.749032 ] [ < ffffffff81031a99 > ] pcache_evict_line + 0x79 / 0x280 [ 1011.817667 ] [ < ffffffff8102f00a > ] pcache_alloc + 0x23a / 0x340 [ 1011.882141 ] [ < ffffffff8102e4da > ] common_do_fill_page + 0x2a / 0x1b0 [ 1011.952856 ] [ < ffffffff8102e160 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1012.023570 ] [ < ffffffff8102e802 > ] pcache_handle_fault + 0x1a2 / 0x660 [ 1012.095324 ] [ < ffffffff810102b2 > ] do_page_fault + 0xa2 / 0x1a0 [ 1012.159799 ] [ < ffffffff8100dadf > ] page_fault + 0x1f / 0x30 Interesting. Memory consistency issue? Actually, I\u2019m not sure if it is the v->flags = 0 issue. Others use atomic bit operations to play with this flag, while the reset is a simple store. I checked the list operations, all of them are protected by spinlock. So the below should never happen in theory. I\u2019m changing the v->flags = 0 to smp_store_mb(v->flags, 0) , which is a xchg in x86. Same for pcache. [ 1773.814490] CPU17 PID44 victim:ffff88207ff71000 index:0 refcount:1 nr_fill:0 locked:0 flags:(allocated|usable) pcm: (null) pset: (null) [ 1773.979705] CPU17 PID44 hit[0] owner: [word_count-pthr][32] addr: 0x7fff95b1c000 [ 1774.072260] CPU17 PID44 rmap to pset:ffff88207f96c700 set_idx: 23324 nr_lru:8 [ 1774.161694] CPU17 PID44 victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimUsable(v)) [ 1774.259451] ------------[ cut here ]------------ [ 1774.314567] BUG: failure at managers/processor/pcache/victim.c:231/find_victim_to_evict()! [ 1774.413363] Kernel Panic - not syncing: BUG! [ 1774.464320] CPU: 17 PID: 44 Comm: word_count-pthr 4.0.0-lego-ys+ #47 ... [ 1781.363348] nr_pcache_fill_from_victim: 2 Did another run. I added an explicit wake_up_victim_flushd if victim failed to evict any line. But this fails with IB failure.. [ 2336.950087 ] CPU4 PID : 54 Abort victim alloc ( 20010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 2337.087474 ] CPU4 PID54 -- Start Dump Victim Cache [ 0 ] [ 2337.151955 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 2337.334999 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 2337.426521 ] CPU4 PID54 rmap to pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 2337.513883 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 2337.696927 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 2337.788450 ] CPU4 PID54 rmap to pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 ... ... [ 2340.111861 ] CPU4 PID54 -- Start Dump Victim Flush Queue [ 0 ] [ 2340.181543 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 2340.364587 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 2340.547632 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 2340.730675 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 2340.913720 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 2341.096763 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 2341.279808 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 2341.462851 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 2341.645895 ] CPU4 PID54 -- End Dump Victim Flush Queue [ 0 ] [ 2341.715577 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 2341.793579 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 2476.201442 ] mlx4_ib_handle_error_cqe syndrome 21 [ 2476.254590 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.308670 ] send request failed at connection 4 as 12 [ 2476.368991 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.423073 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.477153 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.531236 ] client_poll_cq : failed status ( 5 ) for wr_id 1051 [ 2476.598837 ] client_poll_cq : failed status ( 5 ) for wr_id 1052 [ 2476.666438 ] __clflush_one () : EPERM : Operation not permitted tsk : 32 user_va : 0x7fff90745000 [ 2476.765240 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2476.840122 ] client_poll_cq : failed status ( 5 ) for wr_id 1053 [ 2476.907724 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2476.982605 ] client_poll_cq : failed status ( 5 ) for wr_id 1054 [ 2477.050207 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2477.125089 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.179169 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.233251 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.287332 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.341414 ] client_poll_cq : failed status ( 5 ) for wr_id 1055 [ 2477.409016 ] client_poll_cq : failed status ( 5 ) for wr_id 1056 .. .. [ 2477.761583 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2477.836464 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.890545 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.944626 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.998707 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2478.052789 ] client_poll_cq : failed status ( 5 ) for wr_id 1059 [ 2478.120392 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 2478.213992 ] IP : [ < ffffffff81064894 > ] client_poll_cq + 0x1f4 / 0x6c0 [ 2478.284714 ] PGD 0 [ 2478.308635 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 2478.356476 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 473 [ 2478.427197 ] RIP : 0010 : [ < ffffffff81064894 > ] [ < ffffffff81064894 > ] client_poll_cq + 0x1f4 / 0x6c0 [ 2478.527040 ] RSP : 0000 : ffff88107e143d90 EFLAGS : 00010246 [ 2478.590481 ] RAX : 0000000000000000 RBX : ffff88207fc6e000 RCX : 0000000000000000 [ 2478.675762 ] RDX : 000000000000100 8 RSI : ffffffff811d36e0 RDI : ffffffff811dab08 [ 2478.761044 ] RBP : ffff88107e143eb0 R08 : 0000000000000000 R09 : 0000000000000000 [ 2478.846327 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : ffff88207fd4f000 [ 2478.931609 ] R13 : 0000000000000004 R14 : ffff88107e143da8 R15 : 0000000000000000 [ 2479.016890 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 2479.113613 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 2479.182254 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 2479.267536 ] Stack : [ 2479.291457 ] ffff88107e143da0 001012 9 c81019794 0000000000000001 0000000000000423 [ 2479.378818 ] 000000 8100000005 00001008000000f 9 ffff88207fd39000 0000000040000000 [ 2479.466180 ] 000f 004000000002 ffff88107e140000 0000000000000424 ffff881000000005 [ 2479.553542 ] 00000000000000f 9 ffff88207fd39000 ffff88107e143e38 ffffffff81019e44 [ 2479.640904 ] 0000000000000001 0000000000000425 ffff881000000005 ffffffff000000f9 [ 2479.728266 ] Call Trace : [ 2479.757386 ] < TSK > [ 2479.780268 ] [ < ffffffff81019e44 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 2479.847869 ] [ < ffffffff81066d78 > ] ? __schedule + 0xf8 / 0x1e0 [ 2479.911311 ] [ < ffffffff81064d60 > ] ? client_poll_cq + 0x6c0 / 0x6c0 [ 2479.979952 ] [ < ffffffff81064d70 > ] client_poll_cq_pass + 0x10 / 0x20 [ 2480.049634 ] [ < ffffffff81020336 > ] kthread + 0xf6 / 0x110 [ 2480.107875 ] [ < ffffffff81020240 > ] ? __kthread_parkme + 0x70 / 0x70 [ 2480.176516 ] [ < ffffffff8100e732 > ] ret_from_fork + 0x22 / 0x30 A classical SMP bug. Lucky to find this one. Let me try to describe this. There are two CPU1. CPU0 and CPU1. CPU0 is doing eviction while CPU1 is doing munmap->pcache_zap_pte. The CPU0 slected a pcm, while this pcm happen to be zapped at the same time by CPU1. There are not enough actions to either 1) prevent CPU0 from selecting this pcm, 2) prevent CPU1 from using this pcm. Both solutions might be work. But we need as least one. wuklab13 0313 - 12 [ 1073.616269 ] pcache : ffff880180777a80 mapcount : 0 refcount : 3 flags :( locked | allocated | usable ) kva : ffff88011ddea000 [ 1073.734941 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff4ddea000 [ 1073.822304 ] pcache dumped because : evict / ref bug [ 1073.987667 ] BUG : failure at managers / processor / pcache / evict . c : 301 / pcache_evict_line () ! [ 1074.082308 ] BUG : failure at managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () ! [ 1074.172789 ] Kernel Panic - not syncing : BUG ! [ 1074.223751 ] CPU : 23 PID : 50 Comm : word_count - pthr 4.0.0 - lego - ys + # 476 03/11 Mon \u00b6 Debug victim cache \u00b6 Morning. Today I will continue debugging victim and clflush, running with MT phoenix+2GB, seq+4GB. Sounds good. Digging into yesterday\u2019s 21th run log. The warning comes from victim_alloc_slowpath . The allocation abort after 10 seconds timeout. And interestingly, a lot threads abort. (The case is, pset is full, so pcache_alloc will try to evict one to victim cache. But victim cache is full as well. So it needs to evict one victim cache line too. Somehow this does not proceed as planned.) I guess somewhere deadlock happens. [ 1682.040428 ] WARNING : CPU : 7 PID : 34 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1682.161063 ] WARNING : CPU : 19 PID : 46 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1686.602779 ] WARNING : CPU : 10 PID : 37 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.384837 ] WARNING : CPU : 3 PID : 53 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.505474 ] WARNING : CPU : 21 PID : 48 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.737386 ] WARNING : CPU : 16 PID : 43 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.859063 ] WARNING : CPU : 4 PID : 54 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.034819 ] WARNING : CPU : 6 PID : 56 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.210574 ] WARNING : CPU : 14 PID : 41 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.488246 ] WARNING : CPU : 5 PID : 55 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1689.598935 ] WARNING : CPU : 22 PID : 49 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1689.953565 ] WARNING : CPU : 0 PID : 51 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1691.740234 ] WARNING : CPU : 13 PID : 40 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1691.861911 ] WARNING : CPU : 1 PID : 52 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1791.554552 ] WARNING : CPU : 11 PID : 38 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 1 st run. MT+2GB. Victim allocation as predicted. Somehow I already forgot how the code is designed. I need to take a detailed reread. Along the testing, fixed a bug in eviction code: handle failed evict_line properly. If eviction mechanism failed, we need to clear what the algorithm part has done. This is also related to yesterday\u2019s big idea: always do proper cleanup. Many thanks go to pcache free checking, help me to find this bug. Less is more. I printed too much useless info when pcache_alloc or victim_alloc fail. I removed all the dump_pset from the failing path. It can give me a much more clean message to debug. Hmm, it is really weird. I dump all victims once alloc timeout. You can see that all victim are not Flushed, that means none of them can be evicted. Take a look at the stat. Hmm, I probabaly should not do this per-cpu counter?? ... [ 4751.460819 ] -- Start Dump Victim Cache -- [ 4751.518022 ] -- CPU19 [ word_count - pthr ][ pid = 46 , tgid = 32 ] -- [ 4751.587706 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800440 [ 4751.747872 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20011000 [ 4751.827955 ] pset : ffff88207f800440 set_idx : 17 nr_lru : 8 [ 4751.893478 ] [ 4751.911159 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f8003c0 [ 4752.071326 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff2000f000 [ 4752.428060 ] pset : ffff88207f8003c0 set_idx : 15 nr_lru : 8 [ 4752.630868 ] [ 4752.931441 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800540 [ 4753.370339 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20015000 [ 4753.450422 ] pset : ffff88207f800540 set_idx : 21 nr_lru : 8 [ 4753.515945 ] [ 4753.533627 ] victim : ffff88207ff710d8 index : 3 refcount : 3 nr_fill : 1 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207fbdff40 [ 4753.693792 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fffbf7fd000 [ 4753.773875 ] pset : ffff88207fbdff40 set_idx : 63485 nr_lru : 7 [ 4753.842518 ] [ 4753.860199 ] victim : ffff88207ff71120 index : 4 refcount : 3 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800500 [ 4754.020367 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20014000 [ 4754.100449 ] pset : ffff88207f800500 set_idx : 20 nr_lru : 8 [ 4754.165971 ] [ 4754.183653 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800480 [ 4754.343819 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30012000 [ 4754.423902 ] pset : ffff88207f800480 set_idx : 18 nr_lru : 8 [ 4754.489426 ] [ 4754.507106 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f8004c0 [ 4754.808718 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30013000 [ 4754.888802 ] pset : ffff88207f8004c0 set_idx : 19 nr_lru : 8 [ 4754.954325 ] [ 4754.972006 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800400 [ 4755.132172 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20010000 [ 4755.212255 ] pset : ffff88207f800400 set_idx : 16 nr_lru : 8 [ 4755.277778 ] [ 4755.295458 ] -- End Dump Victim Cache -- ... [ 4757.948641 ] nr_pgfault : 313898 [ 4757.983067 ] nr_clflush : 488 [ 4758.016347 ] nr_pgfault_wp : 0 [ 4758.050669 ] nr_pgfault_wp_cow : 0 [ 4758.089151 ] nr_pgfault_wp_reuse : 0 [ 4758.129713 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 4758.188995 ] nr_pcache_fill_from_memory : 313833 [ 4758.242038 ] nr_pcache_fill_from_victim : 54 [ 4758.290919 ] nr_pcache_eviction_triggered : 243280263 [ 4758.349161 ] nr_pcache_eviction_eagain : 243279763 [ 4758.404283 ] nr_pcache_eviction_succeed : 488 [ 4758.454207 ] nr_victim_eviction : 426 [ 4758.495807 ] nr_victim_prepare_insert : 500 [ 4758.543649 ] nr_victim_finish_insert : 488 [ 4758.590451 ] nr_victim_flush_submitted : 488 [ 4758.639333 ] nr_victim_flush_finished : 488 I counted it wrong. Below is the log. Since nr_victim_flushd_run * 8 = nr_victim_flush_finished , it basically means for every run, victim_flushd needs to flush all 8 victims, which implies eviction rate is much higher than the flushd running rate. nr_pcache_fill_from_victim: 21 , which means there are some succeed refills, but I don\u2019t know how it can improve performance. [ 475.468489 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f800000 , pset_idx : 0 , nr_lru : 7 [ 475.602752 ] CPU3 PID : 53 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f900a00 , pset_idx : 16424 , nr_lru : 7 [ 476.029145 ] CPU5 PID : 55 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fbdff40 , pset_idx : 63485 , nr_lru : 7 [ 476.169542 ] CPU9 PID : 36 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f900000 , pset_idx : 16384 , nr_lru : 7 [ 477.360322 ] CPU1 PID : 52 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fbfff80 , pset_idx : 65534 , nr_lru : 7 [ 479.206291 ] CPU18 PID : 45 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fb00000 , pset_idx : 49152 , nr_lru : 7 [ 475.743150 ] -- Start Dump Victim Cache -- [ 475.800350 ] -- CPU4 [ word_count - pthr ][ pid = 54 , tgid = 32 ] -- [ 475.868989 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a80 [ 476.309940 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002a000 [ 476.390020 ] pset : ffff88207f800a80 set_idx : 42 nr_lru : 8 [ 476.455538 ] [ 476.473218 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800bc0 [ 476.633376 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff4002f000 [ 476.713453 ] pset : ffff88207f800bc0 set_idx : 47 nr_lru : 8 [ 476.778972 ] [ 476.796652 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b80 [ 476.956809 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002e000 [ 477.036889 ] pset : ffff88207f800b80 set_idx : 46 nr_lru : 8 [ 477.102406 ] [ 477.120086 ] victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a00 [ 477.280245 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30028000 [ 477.500721 ] pset : ffff88207f800a00 set_idx : 40 nr_lru : 8 [ 477.566239 ] [ 477.583918 ] victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b40 [ 477.744077 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002d000 [ 477.824155 ] pset : ffff88207f800b40 set_idx : 45 nr_lru : 8 [ 477.889673 ] [ 477.907353 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b00 [ 478.067511 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002c000 [ 478.147590 ] pset : ffff88207f800b00 set_idx : 44 nr_lru : 8 [ 478.213109 ] [ 478.230788 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a40 [ 478.390946 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30029000 [ 478.471024 ] pset : ffff88207f800a40 set_idx : 41 nr_lru : 8 [ 478.536542 ] [ 478.554222 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800ac0 [ 478.714380 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002b000 [ 478.794458 ] pset : ffff88207f800ac0 set_idx : 43 nr_lru : 8 [ 478.859977 ] [ 478.877657 ] -- End Dump Victim Cache -- [ 480.324070 ] nr_pgfault : 372353 [ 480.358494 ] nr_clflush : 336 [ 480.391774 ] nr_pgfault_wp : 0 [ 480.426093 ] nr_pgfault_wp_cow : 0 [ 480.464573 ] nr_pgfault_wp_reuse : 0 [ 480.505132 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 480.564410 ] nr_pcache_fill_from_memory : 372326 [ 480.617450 ] nr_pcache_fill_from_victim : 21 [ 480.666330 ] nr_pcache_eviction_triggered : 178320088 [ 480.724569 ] nr_pcache_eviction_eagain : 178319746 [ 480.779687 ] nr_pcache_eviction_succeed : 336 [ 480.829606 ] nr_victim_eviction_triggered : 20589049 [ 480.886805 ] nr_victim_eviction_eagain : 20588741 [ 480.940885 ] nr_victim_eviction_succeed : 308 [ 480.990804 ] nr_victim_prepare_insert : 342 [ 481.038643 ] nr_victim_finish_insert : 336 [ 481.085442 ] nr_victim_flush_submitted : 336 [ 481.134321 ] nr_victim_flush_finished : 336 [ 481.182161 ] nr_victim_flushd_run : 42 03/10 Sun \u00b6 Fix bug from __unhash_procees() \u00b6 [Summary]: a bug cause by laziness. When fork happens, the new thread is added into parent\u2019s thread_group list. However, we forgot to remove it when the new thread exit. Thus, the field in parent\u2019s thread_group will point to a freed page. To make it worse, the freed page got allocated again. In our case, the page was used by pgtable. So, when the parent tries to use that field, it simply corrupts pgtable. This bug is fixed by this commit: 64d43fc. Got something going on. Huh. Anyway, pick up what left last night. 8 th run, [ 426.595911] SYSC_mmap(cpu5): ret_addr:0x7ffefbeac000 pte page got allocated [ 426.653216] pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90 [ 426.734334] __pte_alloc(): for addr: 0x7ffefc6acd90 pte_index: ac [ 426.807132] pte is none index 0x38 line 574 from_addr 0x7ffefc6acd90 [ 427.304148] pte is none index 0x38 line 576 from_addr 0x7ffefc6acd90 this addr seems fine [ 427.382251] pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78 [ 427.462329] pte is none index 0x38 line 574 from_addr 0x7ffefc6abe78 [ 427.644439] pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78 Something happen in between corrupted pgtable [ 427.722547] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000) [ 427.858779] line: 567 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0 [ 427.938858] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000) [ 428.075095] line: 574 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0 9 th run, found actually it created another thread. And it exit. And it corrupted aftet the pid33 exit. Bang, it should be something wrong in exit(). wuklab13 0311 - 4 [ 813.127325 ] CPU6 pid : 33 pmd is none index 0x1e3 line 586 from_addr 0x4b0db0 [ 813.214683 ] CPU5 pid : 32 pmd is none index 0x1e3 line 593 from_addr 0x6f4768 [ 813.302042 ] CPU6 pid : 33 pmd is none index 0x1e3 line 593 from_addr 0x4b0db0 [ 813.397836 ] CPU5 pid : 32 pmd is none index 0x1e3 line 595 from_addr 0x6f4768 [ 813.593364 ] CPU6 pid : 33 pmd is none index 0x1e3 line 595 from_addr 0x4b0db0 [ 813.678751 ] do_exit () pid : 33 , tgid : 32 code : 0x0 [ 814.474321 ] CPU5 pid : 32 pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90 [ 814.567918 ] CPU5 pid : 32 pmd is none index 0x1e3 line 575 from_addr 0x7ffefc6acd90 [ 814.661516 ] CPU5 pid : 32 pmd is none index 0x1e3 line 583 from_addr 0x7ffefc6acd90 [ 814.755115 ] CPU5 pid : 32 pmd is none index 0x1e3 line 586 from_addr 0x7ffefc6acd90 [ 814.848714 ] __pte_alloc () : for addr : 0x7ffefc6acd90 pte_index : ac [ 814.921511 ] CPU5 pid : 32 pte is none index 0x38 line 593 from_addr 0x7ffefc6acd90 [ 815.125249 ] CPU5 pid : 32 pte is none index 0x38 line 595 from_addr 0x7ffefc6acd90 [ 815.215833 ] After pcache_handle_fault [ 815.259511 ] CPU5 pid : 32 pte is none index 0x38 line 726 from_addr 0x7ffefc6acd90 [ 815.352071 ] CPU5 pid : 32 pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78 [ 815.444627 ] CPU5 pid : 32 pte is none index 0x38 line 575 from_addr 0x7ffefc6abe78 [ 815.537186 ] CPU5 pid : 32 pte is none index 0x38 line 583 from_addr 0x7ffefc6abe78 [ 815.629744 ] CPU5 pid : 32 pte is none index 0x38 line 586 from_addr 0x7ffefc6abe78 [ 815.722303 ] CPU5 pid : 32 pte is none index 0x38 line 593 from_addr 0x7ffefc6abe78 [ 815.916890 ] CPU5 pid : 32 pte is none index 0x38 line 595 from_addr 0x7ffefc6abe78 [ 816.007471 ] After pcache_handle_fault [ 816.051151 ] CPU5 pid : 32 pte is none index 0x38 line 726 from_addr 0x7ffefc6abe78 [ 816.143715 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 816.279946 ] do_exit () pid : 34 , tgid : 32 code : 0x0 [ 816.331945 ] CPU5 pid : 32 line : 567 from_addr : 0x6fc6d8 pte . cont : 0xffff88207e8c31c0 10 th run, actually 2 threads are created. When pid 33 exit, everything stays okay. But after fork of pid 34. It went wrong: wuklab13 0311 - 8 [ 609.490893 ] do_exit () pid : 33 , tgid : 32 code : 0x0 [ 609.542894 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 401 from_addr 0x0 [ 609.640661 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 443 from_addr 0x0 [ 609.738429 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 465 from_addr 0x0 [ 609.836197 ] exit_mm : 378 mm -> users 2 mm -> count 1 [ 609.891320 ] exit_mm : 380 mm -> users 1 mm -> count 1 [ 609.946445 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 468 from_addr 0x0 [ 610.044212 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 471 from_addr 0x0 [ 610.141979 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 474 from_addr 0x0 [ 610.239747 ] SYSC_mmap ( cpu5 ) : ret_addr : 0x7ffefbeac000 [ 610.299031 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 482 from_addr 0x0 [ 610.396798 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 568 from_addr 0x7ffefc6acd90 [ 610.518489 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 576 from_addr 0x7ffefc6acd90 [ 610.640178 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 584 from_addr 0x7ffefc6acd90 [ 610.761866 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 587 from_addr 0x7ffefc6acd90 [ 610.883557 ] __pte_alloc () : for addr : 0x7ffefc6acd90 pte_index : ac [ 610.956362 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 594 from_addr 0x7ffefc6acd90 [ 611.179051 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 596 from_addr 0x7ffefc6acd90 [ 611.297723 ] After pcache_handle_fault [ 611.341406 ] CPU5 pid : 32 caller : do_page_fault pte is none index 0x38 line 726 from_addr 0x7ffefc6acd90 [ 611.455816 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 568 from_addr 0x7ffefc6abe78 [ 611.576464 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78 [ 611.697113 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 584 from_addr 0x7ffefc6abe78 [ 611.817762 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 587 from_addr 0x7ffefc6abe78 [ 611.938412 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 594 from_addr 0x7ffefc6abe78 [ 612.161103 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 596 from_addr 0x7ffefc6abe78 [ 612.279778 ] After pcache_handle_fault [ 612.323461 ] CPU5 pid : 32 caller : do_page_fault pte is none index 0x38 line 726 from_addr 0x7ffefc6abe78 [ 612.437875 ] do_fork : current : 32 new : 34 [ 612.484676 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 612.620924 ] do_exit () pid : 34 , tgid : 32 code : 0x0 [ 612.672928 ] CPU5 pid : 32 caller : pcache_handle_faultline : 568 from_addr : 0x6fc6d8 pte . cont : 0xffff88207e8c31c0 [ 612.793577 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 612.929828 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 613.066078 ] CPU7 pid : 34 caller : do_exitline : 401 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 11 th run, found it orignate from copy_process() . Good. [ 869.591729 ] CPU5 pid : 32 caller : do_fork pte is none index 0x38 line 886 from_addr 0x0 [ 869.688449 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 869.824681 ] CPU5 pid : 32 caller : do_fork line : 894 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 12 th run, found the opeation that corrupt pgtable: [ 1099.974106 ] CPU5 pid : 32 caller : copy_process pte is none index 0x38 line 897 from_addr 0x0 [ 1100.076032 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 1100.212282 ] CPU5 pid : 32 caller : copy_process line : 902 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 896 if ( current -> tgid == 32 ) 897 jasmine ( 0 , __LINE__ , __func__ ); 898 899 list_add_tail ( & p -> thread_group , 900 & p -> group_leader -> thread_group ); 901 if ( current -> tgid == 32 ) 902 jasmine ( 0 , __LINE__ , __func__ ); 13 th run, interesting, the list_add_tail write to the pgtable. pte.cont = 0xffff88207e8c31c0, p->thread_group: 0xffff88207e8c31c0 . [ 916.269942 ] CPU5 pid : 32 caller : copy_process pte is none index 0x38 line 898 from_addr 0x0 [ 916.371863 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e190000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_grou : ffff88107e1901c0 [ 916.523705 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 916.659947 ] CPU5 pid : 32 caller : copy_process line : 906 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 [ 916.769148 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e190000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_grou : ffff88107e1901c0 14 th run, got an log like this. Clearly, the pte is written the value of p->thread_group. But the leader\u2019s pointer is correct. Weird, going to dig deeper. p: ffff88207e8c3000 p->group_leader: ffff88107e189000(32) p->thread_group: ffff88207e8c31c0 leader->thread_group: ffff88107e1891c0 pte page: ffff88207e8b5000 pte: ffff88207e8b51c0 pte.cont: ffff88207e8c31c0 15 th run, found the bug. wuklab13 0311 - 15 [ 1474.477687 ] dup_task_struct () : current : 32 new : ffff88207e8b5000 .. while pid 33 exit so the ffff88207e8b5000 is freed but allocated again by pte_alloc [ 1481.420200 ] __pte_alloc () : CPU5 for addr : 0x7ffefc6acd90 pte_index : ac new pte page : ffff88207e8b5000 However , we forgot to remove it from group_leader ' s thread_group [ 1485.895938 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e19b000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_group : ffff88107e19b1c0 [ 1486.047784 ] tg -> next : ffff88207e8c31c8 tg -> prev : ffff88207e8c31c0 leader -> tg -> next ffff88107e19b1c8 leader -> tg -> prev ffff88107e19b1c0 [ 1486.191311 ] next ffff88107e19b1c0 prev ffff88207e8b51c0 next ffff88107e19b1c0 [ 1486.276594 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 61 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.401399 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 65 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.526203 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 69 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.651010 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 73 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.775814 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 1486.912060 ] CPU5 pid : 32 caller : __list_add line : 77 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 16 th run, damn, after patching __unhash_process() , it finally works. Going to workout, see you tonight. victim report error \u00b6 17 th run. The phoenix program has bug itself, it is not able to run with 4GB dataset. So try it with 2GB dataset. Uuh, the log is too long. __put_vicim report a victim that has wrong flags. Going to disable the evict log and try again. 18 th run. Happen to run seq with 100MB\u2026 It actually half finished. But the printf of phoenix has funny chars. I guess memory is corrupted. The log shows it is ib_mad_completion. [ 2244.018806 ] Processor : Processor manager is running . [ 2246.394568 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 2246.447719 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 2246.507003 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq ] --- [ 2246.618289 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt ] --- [ 2258.805633 ] STDOUT : --- [ Word - Count - Seq : Computation Completed 12.46633 sec ] --- [ 2258.923180 ] SYSC_close () : [ 4 ] -> [ / proc / meminfo ] [ 2258.995743 ] STDOUT : --- [ Use len is 123748 [ 2263.484774 ] STDOUT : --- [ THE : 1115050 ] --- [ 2263.666785 ] STDOUT : --- [ OF : 615296 ] --- [ 2266.103660 ] STDOUT : --- [ AND : 545303 ( a lot funny chars , deleted .) ] --- [ 2267.016837 ] Code : [ 2267.038680 ] STDOUT : --- [ TO : 475179 +> \u00d5\u00fe\u00da\u00e9\u00d8 ^ G \u00a7 < 87 > k < 80 > z ^ T < 86 > ruJ \u00b7\u00bf\u00bb < 9 e > \u00e9\u00de\u00ed\u00d1 r\u00dc\u00d5 ^ W\u00e5 ^ W *^ _ {( \u00ca ? R\u00f9a\u00e9\u00f6 \u00f7 8 \u00ed < 91 > \u00dc\u00e8 < 8f > \u00f2\u00bf i ^? \u00e8 4 < 94 > \u00d7\u00b2\u00c9\u00b5 ^ V \u00bf\u00ab\u00eb P ] \u00ed\u00ef h ^ G\u00ca\u00eb < 98 >^ T \u00d7 Qp\u00b9O \u00ae\u00ef ^ \\\u00da ^?^ A\u00ed < 91 > \u00d9 v\u00ddBy ^ _\u00e9iwP ^ r < 97 > \u00eb\u00f9\u00ef\u00df ] \u00a3\u00df\u00ad < 98 >< 81 > \u00f8 < 85 > \u00ce Ey ^ Y\u00e5 ^? V\u00f9\u00ba ^ Y\u00de\u00f5\u00cb ] r5\u00c9\u00f0 ^^ ' < 92 > \u00c9 ] ^ ] P ^ \u00c7 i \u00bb z : \u00d4 ^ S \u00ae e < 8 a >+ \\\u00e9 < 8 a > \u00ae\u00b1\u00e0 E\u00d5\u00ce , \u00f0\u00d2\u00e2 3 \u00c1 _ ^ P_ ^ H ^ [ | \u00b8\u00ae\u00e1 s\u00edF \u00bf m < 95 >< 9 d >?< 82 > \u00f2 : \u00be\u00de\u00f5 3 \u00ca\u00d7 T\u00fc \u00ae ] --- [ 2263.339165 ] BUG : unable to handle kernel paging request at ffffffffffff8100 [ 2263.422369 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 2263.570058 ] PGD 1140067 PUD 1142067 PMD 0 [ 2263.618942 ] Oops : 0010 [ # 1 ] SMP PROCESSOR [ 2264.705811 ] CPU : 0 PID : 27 Comm : ib_mad_completi 4.0.0 - lego - ys + # 408 [ 2264.781736 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 2264.873262 ] RSP : 0000 : ffff88107efabc90 EFLAGS : 00010046 [ 2264.936705 ] RAX : 5636000000000098 RBX : db5affffffffffff RCX : 0000000000000001 [ 2265.021990 ] RDX : ffff88107efabd38 RSI : 0000000000000000 RDI : 4460ff ffffff8114 [ 2265.107277 ] RBP : ffff88107efabce0 R08 : 000000000000001f R09 : ffff88107efa43c0 [ 2265.192561 ] R10 : ffff88107efabe68 R11 : 0000000000000001 R12 : ac02000004ecbdbd [ 2265.277847 ] R13 : 0000000000000000 R14 : ffff88107efa4228 R15 : ffff88107e1ab000 [ 2265.363133 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 2265.459858 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 2265.528503 ] CR2 : ffffffffffff8100 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 2265.613789 ] Stack : [ 2265.637710 ] ffffffff810151a7 00000000000000 82 ffff88107fc04980 0000000000000000 [ 2265.725075 ] ffff88107efabcc8 ffff88107fc04980 0000000000000000 0000000000000000 [ 2265.812441 ] ffff88107efa4228 ffff88107e1ab000 ffff88107efabcf8 ffffffff81016e17 [ 2265.899806 ] 000000007 efabe20 ffff88107efabd20 ffffffff810066f4 ffffffff81072f20 [ 2265.987172 ] ffff88107fc05e00 ffff88107efa4000 ffff88107efabe08 ffffffff8100e4aa [ 2266.074538 ] Call Trace : [ 2266.206626 ] < TSK > [ 2266.229507 ] [ < ffffffff810151a7 > ] ? update_wall_time + 0x47 / 0x6b0 [ 2266.299192 ] [ < ffffffff81016e17 > ] tick_handle_periodic + 0x67 / 0x70 [ 2266.369916 ] [ < ffffffff810066f4 > ] apic_timer_interrupt + 0x54 / 0x90 [ 2266.440641 ] [ < ffffffff8100e4aa > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 2266.516565 ] [ < ffffffff810663b8 > ] ? __schedule + 0xf8 / 0x1e0 [ 2266.580010 ] [ < ffffffff810664b3 > ] schedule + 0x13 / 0x30 [ 2266.638254 ] [ < ffffffff81058c97 > ] ib_mad_completion_handler + 0x2b7 / 0x860 [ 2266.716258 ] [ < ffffffff810589e0 > ] ? ib_mad_send_done_handler . isra .22 + 0x1d0 / 0x1d0 [ 2266.803624 ] [ < ffffffff81020376 > ] kthread + 0xf6 / 0x110 [ 2266.861867 ] [ < ffffffff81020280 > ] ? __kthread_parkme + 0x70 / 0x70 [ 2266.930512 ] [ < ffffffff8100e732 > ] ret_from_fork + 0x22 / 0x30 [ 2266.993955 ] < EOT > 19 th , try seq+100MB again. Well succeed. I guess I start S too later. So that thread has issues. We run 12.3 sec, while linux run 9.7 sec. 20 th , try seq+4GB data. Linux runs 314.4 sec . Lego runs 403 sec . But Lego has some clflush error messages. I don\u2019t know why actually. [ 794.604628 ] Processor : Processor manager is running . [ 796.884884 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 796.938032 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 796.997312 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq ] --- [ 797.108596 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_4GB . txt ] --- [ 980.640200 ] __clflush_one () : EFAULT : bad address [ 980.692315 ] __clflush_one () : EFAULT : bad address [ 980.746397 ] __clflush_one () : EFAULT : bad address [ 980.800478 ] __clflush_one () : EFAULT : bad address [ 980.854559 ] __clflush_one () : EFAULT : bad address [ 980.908642 ] __clflush_one () : EFAULT : bad address [ 980.962723 ] __clflush_one () : EFAULT : bad address [ 981.016804 ] __clflush_one () : EFAULT : bad address [ 981.070886 ] __clflush_one () : EFAULT : bad address [ 981.124968 ] __clflush_one () : EFAULT : bad address [ 981.179048 ] __clflush_one () : EFAULT : bad address [ 981.233129 ] __clflush_one () : EFAULT : bad address [ 981.287211 ] __clflush_one () : EFAULT : bad address [ 981.341293 ] __clflush_one () : EFAULT : bad address [ 981.395375 ] __clflush_one () : EFAULT : bad address [ 981.449456 ] __clflush_one () : EFAULT : bad address [ 981.503538 ] __clflush_one () : EFAULT : bad address [ 981.557619 ] __clflush_one () : EFAULT : bad address [ 981.611702 ] __clflush_one () : EFAULT : bad address [ 981.665782 ] __clflush_one () : EFAULT : bad address [ 981.719863 ] __clflush_one () : EFAULT : bad address [ 981.773945 ] __clflush_one () : EFAULT : bad address [ 981.828026 ] __clflush_one () : EFAULT : bad address [ 981.882108 ] __clflush_one () : EFAULT : bad address [ 981.936188 ] __clflush_one () : EFAULT : bad address [ 981.990271 ] __clflush_one () : EFAULT : bad address [ 982.044352 ] __clflush_one () : EFAULT : bad address [ 982.098434 ] __clflush_one () : EFAULT : bad address [ 982.152515 ] __clflush_one () : EFAULT : bad address [ 982.206596 ] __clflush_one () : EFAULT : bad address [ 1200.759741 ] STDOUT : --- [ Word - Count - Seq : Computation Completed 403.519401 sec ] --- ... [ 1200.989480 ] STDOUT : --- [ THE : 44602000 ... [ 1201.755779 ] do_group_exit () pid : 32 , tgid : 32 exit_code : 0x0 [ 1201.819136 ] do_exit () pid : 32 , tgid : 32 code : 0x0 [ 1201.872451 ] nr_pgfault : 1049525 [ 1201.908579 ] nr_pgfault_wp : 0 [ 1201.942899 ] nr_pgfault_wp_cow : 0 [ 1201.981380 ] nr_pgfault_wp_reuse : 0 [ 1202.021941 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 1202.081223 ] nr_pcache_fill_from_memory : 1045393 [ 1202.135304 ] nr_pcache_fill_from_victim : 4132 [ 1202.186265 ] nr_pcache_eviction : 525230 [ 1202.230987 ] nr_victim_eviction : 521090 21th run. Do not have time and energy to debug the clflush issue. I just want to run MT+2GB again. Well victim has issues! Some warning are triggered. Log is wuklab13:~/ys/0311-22 . Continue tomorrow! Good night world. (Such a lonly phd.) 03/10 Sat \u00b6 Running python hello world. Tried to make kmalloc use buddy directly. put_pcache in pcache_zap_pte \u00b6 So this time, python keep running for a long time. But P crashed when the first time eviction was triggered. Below is log from S side, those libraries do not exist, so these log are fine: S: [Mar10 10:39] handle_access_request /etc/ld.so.preload 4, -2 [Mar10 10:44] local_file_open : Cannot open required file [/usr/lib64/python2.7/site.so]. [ +0.352839] local_file_open : Cannot open required file [/usr/lib64/python2.7/sitemodule.so]. [ +22.254465] local_file_open : Cannot open required file [/usr/lib64/python2.7/os.so]. [ +0.350759] local_file_open : Cannot open required file [/usr/lib64/python2.7/osmodule.so]. [Mar10 10:45] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpath.so]. [ +0.358045] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpathmodule.so]. [ +13.421033] local_file_open : Cannot open required file [/usr/lib64/python2.7/stat.so]. [ +0.352838] local_file_open : Cannot open required file [/usr/lib64/python2.7/statmodule.so]. [Mar10 10:46] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpath.so]. [ +0.360126] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpathmodule.so]. [ +11.582165] local_file_open : Cannot open required file [/usr/lib64/python2.7/warnings.so]. [ +0.357003] local_file_open : Cannot open required file [/usr/lib64/python2.7/warningsmodule.so]. [ +11.989828] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecache.so]. [ +0.358043] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecachemodule.so]. [Mar10 10:47] local_file_open : Cannot open required file [/usr/lib64/python2.7/types.so]. [ +0.353879] local_file_open : Cannot open required file [/usr/lib64/python2.7/typesmodule.so]. Weird P\u2019s bug, seems like the pcm returned by evict_find_line has issue. Well, I\u2019m trying to debug what is going with this set. wuklab13 0310 - 2 [ 1046.880649 ] SYSC_read () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff6e117e0 [ 1046.959692 ] fd : 8 , buf : 00007ff ff7ffb000 , count : 4096 [ 1048.726624 ] pcache_evict_line () : pset : ffff88207f9ffec0 , for uva : 0x7ffff7ffb000 [ 1048.813053 ] ------------ [ cut here ] ------------ [ 1048.868174 ] BUG : failure at . / include / processor / pcache . h : 284 / pcache_meta_to_pcache_set () ! [ 1048.965937 ] Kernel Panic - not syncing : BUG ! [ 1049.016898 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 347 [ 1049.083460 ] Stack : [ 1049.107380 ] ffff88107e18fca8 ffffffff81026f1c 000000000000000 8 ffff88107e18fcb8 [ 1049.194743 ] ffff88107e18fc70 0000000021475542 0000000000000000 0000000000000000 [ 1049.282107 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.369468 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.456832 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.544193 ] Call Trace : [ 1049.573315 ] < TSK > [ 1049.596195 ] [ < ffffffff81026f28 > ] panic + 0xc2 / 0xeb [ 1049.651318 ] [ < ffffffff8101c3fc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1049.715799 ] [ < ffffffff81019a65 > ] ? scheduler_tick + 0x55 / 0x60 [ 1049.782360 ] [ < ffffffff81017035 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1049.855163 ] [ < ffffffff81006764 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1049.927966 ] [ < ffffffff8101c3fc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1049.992447 ] [ < ffffffff81019a65 > ] ? scheduler_tick + 0x55 / 0x60 [ 1050.059009 ] [ < ffffffff81017035 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1050.131812 ] [ < ffffffff8103c41a > ] ? put_dec + 0x1a / 0x80 [ 1050.191093 ] [ < ffffffff81006764 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1050.263895 ] [ < ffffffff8100e56a > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1050.341897 ] [ < ffffffff81012ded > ] ? printk + 0x11d / 0x1b0 [ 1050.402219 ] [ < ffffffff810340c5 > ] dump_pcache_meta + 0xc5 / 0xd0 [ 1050.468782 ] [ < ffffffff81034588 > ] pcache_evict_line + 0x158 / 0x220 [ 1050.538463 ] [ < ffffffff81030f5e > ] pcache_alloc + 0x22e / 0x2f0 [ 1050.602945 ] [ < ffffffff8103015a > ] common_do_fill_page + 0x2a / 0x430 [ 1050.673668 ] [ < ffffffff8102fb20 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1050.744389 ] [ < ffffffff81030702 > ] pcache_handle_fault + 0x1a2 / 0x6c0 [ 1050.816152 ] [ < ffffffff810103d2 > ] do_page_fault + 0xa2 / 0x1a0 [ 1050.880634 ] [ < ffffffff8100db9f > ] page_fault + 0x1f / 0x30 [ 1050.940955 ] [ < ffffffff8103bb82 > ] ? copy_user_enhanced_fast_string + 0x2 / 0x10 [ 1051.023118 ] [ < ffffffff81038423 > ] ? normal_p2m_read + 0x233 / 0x330 [ 1051.092800 ] [ < ffffffff810363ce > ] sys_read + 0x9e / 0x160 [ 1051.152081 ] [ < ffffffff810268d0 > ] ? strace_enter_default + 0x30 / 0x40 [ 1051.224884 ] [ < ffffffff8100e935 > ] do_syscall_64 + 0x45 / 0xd0 [ 1051.288326 ] [ < ffffffff8100d82c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 Interesting, added several debug messages. The bug is I forgot to put_pcache when a rmap was zapped. One rmap counts one refcount (effectively one process), thus when a rmap was zapped, we should decrease the refcount. I found I\u2019ve already done so for pcache_remove_rmap , and pcache_move_pte . But damn, forgot this one. I remember this code was written before fork+pcache. So.. I don\u2019t have a big picture at that time. Multithreaded system plus background reclaim really a very rigours design usage of refcount and lock . [ 1418.038411] CPU5 PID32 sys_read+0x0/0xa0 [ 1418.085227] pcache_evict_line(): pset: ffff88207f9ffec0, for uva: 0x7ffff7ffb000 [ 1418.173617] pset:ffff88207f9ffec0 set_idx: 32763 nr_lru:8 [ 1418.238105] pcache:ffff8801801ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880107ffb000 [ 1418.351476] pcache:ffff8801805ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880117ffb000 [ 1418.464847] pcache:ffff8801809ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880127ffb000 [ 1418.578220] pcache:ffff880180dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880137ffb000 [ 1418.691591] pcache:ffff8801811ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880147ffb000 [ 1418.804963] pcache:ffff8801815ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880157ffb000 [ 1418.918334] pcache:ffff8801819ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880167ffb000 [ 1419.031706] pcache:ffff880181dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880177ffb000 [ 1419.145077] After dump pset [ 1419.176280] pcache:ffff8801801ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880107ffb000 [ 1419.289652] pcache dumped because: evict_find_line_lru [ 1419.351018] pcache:ffff8801805ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880117ffb000 [ 1419.464389] pcache dumped because: evict_find_line_lru [ 1419.525757] pcache:ffff8801809ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880127ffb000 [ 1419.639127] pcache dumped because: evict_find_line_lru [ 1419.700494] pcache:ffff880180dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880137ffb000 [ 1419.813865] pcache dumped because: evict_find_line_lru [ 1419.875231] pcache:ffff8801811ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880147ffb000 [ 1419.988604] pcache dumped because: evict_find_line_lru [ 1420.049969] pcache:ffff8801815ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880157ffb000 [ 1420.163341] pcache dumped because: evict_find_line_lru [ 1420.224708] pcache:ffff8801819ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880167ffb000 [ 1420.338079] pcache dumped because: evict_find_line_lru [ 1420.399445] pcache:ffff880181dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880177ffb000 [ 1420.512817] pcache dumped because: evict_find_line_lru [ 1420.574183] evict_find_line_lru(): pcm: ffff88207f9ffea8 [ 1420.637631] ------------[ cut here ]------------ [ 1420.692756] BUG: failure at ./include/processor/pcache.h:340/pcache_meta_to_kva()! [ 1420.783245] Kernel Panic - not syncing: BUG! [ 1420.834210] CPU: 5 PID: 32 Comm: python 4.0.0-lego-ys+ #349 [ 1420.900777] Stack: python hello world run to end \u00b6 Glad to say, python hello world finished, even with some missed syscalls. Especially the stdin stuff, so the string is actually not printed out. Log is wuklab13:~/ys/0310-4 [ 3149.540308 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3149.588144 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3149.635982 ] CPU5 PID32 sys_write + 0x0 / 0xa0 [ 3149.683818 ] STDOUT : --- [ >>> ] --- [ 3149.726456 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff6d9aeb0 flags : 0x150 [ 3149.926247 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff6d9aeb0 flags : 0x150 ret : 0 ( OKAY ) [ 3150.033464 ] CPU5 PID32 sys_newfstat + 0x0 / 0x10 [ 3150.084420 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3150.132256 ] strace__mmap cpu5 addr = 0x0 , len = 0x1000 , prot ( 0x3 ) = PROT_READ | PROT_WRITE , flags ( 0x22 ) = MAP_PRIVATE | MAP_ANONYMOUS , fd = 18446744073709551615 ( ), off = 0x0 [ 3150.301772 ] CPU5 PID32 sys_read + 0x0 / 0xa0 [ 3150.348562 ] ------------ [ cut here ] ------------ [ 3150.403679 ] WARNING : CPU : 5 PID : 32 at managers / processor / fs / stdio . c : 24 stdio_file_read + 0x30 / 0x50 [ 3150.509751 ] Process wants STDIN ! [ 3150.546149 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 352 [ 3150.612705 ] Stack : [ 3150.636624 ] ffff88107e18fe90 ffffffff81012b15 ffffffff811464e0 00007ff ff7ffb000 [ 3150.723977 ] 0000000000000400 00007ff ff70e5640 ffff88107e18fef0 ffffffff81012bd2 [ 3150.811331 ] ffffffff81079d6b ffff881000000018 ffff88107e18ff00 ffff88107e18fec0 [ 3150.898687 ] 0000000000000020 ffffffff810346b0 0000000000000022 ffffffff811464f0 [ 3150.986040 ] 00007ff ff7fdf740 0000000000000000 ffff88107e18ff00 ffffffff81035ac0 [ 3151.073394 ] Call Trace : [ 3151.102514 ] < TSK > [ 3151.125392 ] [ < ffffffff81012b21 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 3151.194028 ] [ < ffffffff81012bd2 > ] warn_slowpath_fmt + 0x42 / 0x50 [ 3151.261623 ] [ < ffffffff810346b0 > ] ? sweep_pset_lru + 0x220 / 0x220 [ 3151.330259 ] [ < ffffffff81035ac0 > ] stdio_file_read + 0x30 / 0x50 [ 3151.395775 ] [ < ffffffff810346e3 > ] sys_read + 0x33 / 0xa0 [ 3151.454010 ] [ < ffffffff8100e875 > ] do_syscall_64 + 0x45 / 0xd0 [ 3151.517446 ] [ < ffffffff8100d76c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3151.593362 ] < EOT > [ 3151.616240 ] --- [ end trace 0000000000000000 ] --- [ 3151.671360 ] CPU5 PID32 sys_write + 0x0 / 0xa0 [ 3151.719194 ] STDOUT : --- [ ] --- [ 3151.759756 ] CPU5 PID32 sys_close + 0x0 / 0x140 [ 3151.808628 ] SYSC_close () : [ 3 ] -> [ / root / ys / py_hello . py ] [ 3151.871028 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a79380 flags : 0x150 [ 3152.070817 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a79380 flags : 0x150 ret : 0 ( OKAY ) [ 3152.178033 ] CPU5 PID32 sys_rt_sigaction + 0x0 / 0xb0 [ 3152.234151 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a77f60 flags : 0x150 [ 3152.432941 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a77f60 flags : 0x150 ret : 0 ( OKAY ) [ 3152.540242 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff73ee794 flags : 0x150 [ 3152.739952 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff73ee794 flags : 0x150 ret : 0 ( OKAY ) [ 3152.847171 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff715b278 flags : 0x150 [ 3153.046958 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff715b278 flags : 0x150 ret : 0 ( OKAY ) [ 3153.154179 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff6de74f0 flags : 0x150 [ 3153.353965 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff6de74f0 flags : 0x150 ret : 0 ( OKAY ) [ 3153.461180 ] CPU5 PID32 sys_exit_group + 0x0 / 0x10 Trying phoenix pthread again \u00b6 4GB pcache, 1GB dataset. 1 th run with CONFIG_STRACE on, 1GB dataset finished, result is correct. 2 th run without CONFIG_STRACE, 1GB dataset stuck. Two weird things: open/close dev/cpu/online file too many times than a normal linux run IB stucked So next I\u2019m going to try add a lock to ibapi, see if it is ib internal deadlock issue. wuklab13 0310 - 7 [ 702.895936 ] Processor : Processor manager is running . [ 722.400159 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 722.453307 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 722.512589 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread ] --- [ 722.628036 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt ] --- [ 722.759101 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 722.819406 ] STDOUT : --- [ ] --- [ 722.860139 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 722.940653 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.011483 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.084287 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.157090 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.229894 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.302698 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.375502 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.448306 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.521111 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.593914 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.666718 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.739522 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.812326 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.885130 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 766.701260 ] ibapi_send_reply () polling timeout ( 30010 ms ), caller : net_send_reply_timeout + 0x11b / 0x1ee [ 766.809538 ] net_send_reply_timeout () caller : __pcache_do_fill_page + 0x82 / 0x140 [ 766.895863 ] word_count - pthr [ 65 ] : segfault at 0x7fffb5eba000 ip 00000000004024 9 d sp 00007ff fb5e9ad80 error 6 [ 767.012348 ] CPU : 15 PID : 65 Comm : word_count - pthr 4.0.0 - lego - ys + # 359 [ 767.089312 ] RIP : 0033 : [ < 00000000004024 9 d > ] [ < 00000000004024 9 d > ] 0x40249d [ 767.170436 ] RSP : 002 b : 00007ff fb5e9ad80 EFLAGS : 00010216 [ 767.233879 ] RAX : 00007ff fb5eba000 RBX : 00000000000013 88 RCX : 000000000000004f [ 767.319164 ] RDX : 00007ff fe4ea92a4 RSI : 00007ff fe626fac9 RDI : 00007ff fe4ea92a4 [ 767.404449 ] RBP : 00000000007540e0 R08 : 0000000000000000 R09 : 0000000000014f a0 [ 767.489733 ] R10 : 0000000000427f b0 R11 : 0000000000000202 R12 : 0000000000012 b12 [ 767.575018 ] R13 : 00007ff f496ab890 R14 : 00007ff f48704fb0 R15 : 00000000000013 88 [ 767.660303 ] FS : 00007ff fb5e9b700 ( 0000 ) GS : ffff88207fce0000 ( 0000 ) knlGS : 0000000000000000 [ 767.757028 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 767.825671 ] CR2 : 00007ff fb5eba000 CR3 : 000000207f e3a000 CR4 : 00000000000406 a0 [ 767.910958 ] get_signal () : dequeue_signr : 11 , handler : ( null ) [ 767.987928 ] get_signal () : dequeue_signr : 9 , handler : ( null ) 3 th run, without STRACE, with locked ibapi, it finished, result is correct. Runtime: 18.692936 sec . [ 555.423623 ] nr_pgfault : 288100 [ 555.458042 ] nr_pgfault_wp : 0 [ 555.492360 ] nr_pgfault_wp_cow : 0 [ 555.530838 ] nr_pgfault_wp_reuse : 0 [ 555.571396 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 555.630673 ] nr_pcache_fill_from_memory : 288081 [ 555.683710 ] nr_pcache_fill_from_victim : 12 [ 555.732588 ] nr_pcache_eviction : 494 [ 555.774187 ] nr_victim_eviction : 474 4 th run, same setting with the 3 th run, same result. But the nr_pgfault differs, I guess it is due to runtime things. Runtime: 19.12861 sec . [ 469.891700 ] nr_pgfault : 288119 [ 469.926123 ] nr_pgfault_wp : 0 [ 469.960444 ] nr_pgfault_wp_cow : 0 [ 469.998924 ] nr_pgfault_wp_reuse : 0 [ 470.039484 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 470.098764 ] nr_pcache_fill_from_memory : 288093 [ 470.151805 ] nr_pcache_fill_from_victim : 12 [ 470.200684 ] nr_pcache_eviction : 513 [ 470.242285 ] nr_victim_eviction : 493 5 th run, same with 4 th , succeed, Runtime: 18.653879 sec . [ 313.202348] nr_pgfault: 288070 [ 313.236772] nr_pgfault_wp: 0 [ 313.271093] nr_pgfault_wp_cow: 0 [ 313.309575] nr_pgfault_wp_reuse: 0 [ 313.350139] nr_pgfault_due_to_concurrent_eviction: 0 [ 313.409421] nr_pcache_fill_from_memory: 288052 [ 313.462465] nr_pcache_fill_from_victim: 6 [ 313.510307] nr_pcache_eviction: 446 [ 313.551909] nr_victim_eviction: 432 6 th , setting is the same, but with 4GB dataset, crashed: [ 512.028141 ] Processor : Processor manager is running . [ 529.375605 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 529.435906 ] STDOUT : --- [ ] --- [ 529.476660 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 529.555983 ] ------------ [ cut here ] ------------ [ 529.609128 ] BUG : failure at managers / processor / pcache / rmap . c : 735 / pcache_zap_pte () ! [ 529.699613 ] Kernel Panic - not syncing : BUG ! [ 529.750576 ] CPU : 5 PID : 32 Comm : word_count - pthr 4.0.0 - lego - ys + # 361 [ 529.826500 ] Stack : [ 529.850422 ] ffff88107e1a3dd8 ffffffff810259b4 000000000000000 8 ffff88107e1a3de8 [ 529.937787 ] ffff88107e1a3da0 0000000021475542 0000000000000000 0000000000000000 [ 530.025152 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.112517 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.199882 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.287247 ] Call Trace : [ 530.316370 ] < TSK > [ 530.339251 ] [ < ffffffff810259c0 > ] panic + 0xc2 / 0xeb [ 530.394374 ] [ < ffffffff8106190a > ] ? client_internal_poll_sendcq + 0x2a / 0x80 [ 530.474458 ] [ < ffffffff8101bfcc > ] ? task_tick_rt + 0x2c / 0xd0 [ 530.538943 ] [ < ffffffff81019725 > ] ? scheduler_tick + 0x55 / 0x60 [ 530.605506 ] [ < ffffffff81016df5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 530.678311 ] [ < ffffffff8103768a > ] ? put_dec + 0x1a / 0x80 [ 530.737595 ] [ < ffffffff810066f4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 530.810398 ] [ < ffffffff8100e4aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 530.888403 ] [ < ffffffff81012ccd > ] ? printk + 0x11d / 0x1b0 [ 530.948726 ] [ < ffffffff81030429 > ] pcache_zap_pte + 0xf9 / 0x160 [ 531.014250 ] [ < ffffffff8102f090 > ] ? __pcache_move_pte_fastpath + 0x50 / 0x50 [ 531.093295 ] [ < ffffffff8102c8dc > ] unmap_page_range + 0x32c / 0x3b0 [ 531.161940 ] [ < ffffffff8102c97e > ] release_pgtable + 0x1e / 0x40 [ 531.227463 ] [ < ffffffff8102bfb3 > ] sys_munmap + 0xc3 / 0x120 [ 531.288827 ] [ < ffffffff8100e86d > ] do_syscall_64 + 0x3d / 0xc0 [ 531.352270 ] [ < ffffffff8100d76c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 7 th run, add debug info, does not seem that useful: ] --- [ 15755.579501 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 15755.672760 ] pte : ffff88107e1a3dd8 pfn : 0x8207e80b flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 15755.807015 ] pte dumped because : Invalid pte [ 15755.856932 ] address : 0x7ffefc638000 [ 15755.899569 ] ------------ [ cut here ] ------------ [ 15755.954684 ] BUG : failure at managers / processor / pcache / rmap . c : 747 / pcache_zap_pte () ! [ 15756.045159 ] Kernel Panic - not syncing : BUG ! [ 15756.096114 ] CPU : 5 PID : 32 Comm : word_count - pt Tried several times, even with mmap/munmap debug option on, it crashed at the same point. Key is address 0x7ffefc638000 , and the mmap() related to it. Close to find the bug. Latest log in 0310-18. 03/09 Fri \u00b6 Find bug in kmalloc \u00b6 Tried to print pud in every syscall and catch the criminal: wuklab13 030 9 - 1 [ 320.088684 ] CPU5 PID32 sys_close + 0x0 / 0x1f0 [ 320.137567 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 320.269657 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3c37 [ 320.349742 ] 3 [ 320.372624 ] SYSC_close () : [ 3 ] -> [ / lib64 / libpython2 .7 . so .1.0 ] [ 320.441268 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x0 ( 0 ) [ 320.510954 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 320.643043 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 [ 320.709607 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 320.798014 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 [ 320.995755 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 ret : 0 ( OKAY ) [ 321.101944 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 [ 321.168509 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 321.256914 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 [ 321.454651 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 ret : 0 ( OKAY ) [ 321.560845 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 [ 321.627409 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 321.715815 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 [ 321.913553 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 ret : 0 ( OKAY ) [ 322.019745 ] CPU5 PID32 sys_open + 0x0 / 0x10 [ 322.066548 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 322.198638 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3b27 [ 322.277683 ] f_name : / lib64 / libpthread . so .0 , flags : 80000 , mode : e150 [ 322.357780 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x3 ( 3 ) [ 322.426414 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 After printing more in pcache_handle_fault, I found who corrupted pgtable: wuklab13 030 9 - 5 [ 661.308584 ] CPU5 PID32 sys_close + 0x0 / 0x1f0 [ 661.357466 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 661.489557 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3c37 [ 661.569642 ] 3 [ 661.592525 ] SYSC_close () : [ 3 ] -> [ / lib64 / libpython2 .7 . so .1.0 ] [ 661.661170 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x0 ( 0 ) [ 661.730854 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 661.862944 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.001275 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 [ 662.067840 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 662.156247 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 [ 662.353985 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 ret : 0 ( OKAY ) [ 662.460176 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.600586 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.738916 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 [ 662.805481 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 662.893888 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 [ 663.091636 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 ret : 0 ( OKAY ) [ 663.197831 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 663.338242 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 663.476572 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 [ 663.543135 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 663.631543 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 [ 663.829279 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 ret : 0 ( OKAY ) [ 663.935472 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 664.075884 ] CPU5 PID32 sys_open + 0x0 / 0x10 [ 664.122686 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 664.254776 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3b27 [ 664.333821 ] f_name : / lib64 / libpthread . so .0 , flags : 80000 , mode : e150 [ 664.413918 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x3 ( 3 ) [ 664.482552 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 Then, try catching bug with address 0x7ffff7ff2fda fault. Printing still being the most effective way to debug. :-) Dig further, I found pgtable corrupted after pcache_add_rmap() , namely after alloc_pcache_rmap() : [ 5024.482570 ] pcache_add_rmap () 343 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 5024.613601 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 5024.686396 ] pcache_add_rmap () 358 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 Well, rmap: ffff88207fccefd0 & ffff90207fcce000 , clearly [ 843.916517 ] pcache_add_rmap () 372 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.047557 ] alloc_pcache_rmap () 60 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.179638 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 844.252438 ] alloc_pcache_rmap () 71 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.384517 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 844.457317 ] alloc_pcache_rmap () 85 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 [ 844.589398 ] pcache_add_rmap () 387 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 46 static struct pcache_rmap * alloc_pcache_rmap ( void ) 47 { 48 struct pcache_rmap * rmap ; 49 50 pgd_t * pgd ; 51 pud_t * pud ; 52 unsigned long addr ; 53 struct mm_struct * mm = current -> mm ; 54 55 if ( pall ) { 56 addr = 0x601008 ; 57 pgd = pgd_offset ( mm , addr ); 58 pud = pud_alloc ( mm , pgd , addr ); 59 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 60 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 61 } 62 63 rmap = kmalloc ( sizeof ( * rmap ), GFP_KERNEL ); 64 65 if ( pall ) { 66 addr = 0x601008 ; 67 pgd = pgd_offset ( mm , addr ); 68 pud = pud_alloc ( mm , pgd , addr ); 69 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 70 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 71 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 72 } 73 74 if ( rmap ) { 75 INIT_LIST_HEAD ( & rmap -> next ); 76 rmap -> flags = 0 ; 77 } 78 79 if ( pall ) { 80 addr = 0x601008 ; 81 pgd = pgd_offset ( mm , addr ); 82 pud = pud_alloc ( mm , pgd , addr ); 83 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 84 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 85 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 86 } 87 88 return rmap ; 89 } Narrow it down to INIT_LIST_HEAD : [ 1334.548682 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1334.621487 ] alloc_pcache_rmap () 71 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1334.753576 ] alloc_pcache_rmap () 76 & rmap -> next ffff88207fcceff8 & flags ffff88207fccefd8 [ 1334.922067 ] alloc_pcache_rmap () 86 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 [ 1335.126962 ] alloc_pcache_rmap () 98 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 74 if ( rmap ) { 75 pr_info ( \"%s() %d &rmap->next %p &flags %p \\n \" , 76 __func__ , __LINE__ , & rmap -> next , & rmap -> flags ); 77 78 INIT_LIST_HEAD ( & rmap -> next ); 79 80 if ( pall ) { 81 addr = 0x601008 ; 82 pgd = pgd_offset ( mm , addr ); 83 pud = pud_alloc ( mm , pgd , addr ); 84 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 85 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 86 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 87 } 88 89 rmap -> flags = 0 ; 90 } Seriously, if this is running on user-level on VM, I would be able to find the bug maybe in 30min. But I spent several hours to find it out with physical machine. Damn you physical machine. Hmm, this func is used A LOT. How can it fail at this point? Possible reasons: kmalloced area happen to intersect with pgtable? one physical page is mapped twice? one to pgtable, one by this rmap. tty/serial code has bug? Really ancient code. After add a few printk, IB seems stuck. And this happens just with few more lines of code! Why? code size matters? [ 722.381469 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 722.519778 ] addr : 0x7ffff7feffcc , pgd : ffff88207fccf7f8 [ 722.586334 ] addr : 0x7ffff7feffcc , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 722.674727 ] Before fill address = 0x7ffff7feffcc set_idx : 0x7fef [ 722.743362 ] pcache : ffff8801801ffbc0 mapcount : 0 refcount : 1 flags :( allocated | usable ) set_idx = 0x7fef kva : ffff880107fef000 [ 722.872312 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7feffcc flags : 0x50 [ 722.967985 ] __pcache_do_fill_page () : before net pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 last line Well, the following finding finally find the bug line. And it kind of explains the above bug. Probably kmalloc\u2019ed area has issues, so IB is touching wrong data. The following bug is related to kmalloc, the rmap is 56 bytes, and it should be within 1 single page, but it is not: [ 1862.307427 ] pcache_add_rmap () 413 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.438477 ] alloc_pcache_rmap () 86 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.570568 ] sp -> units : 50 SLOB_UNITS : 32 [ 1862.617372 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1862.690178 ] alloc_pcache_rmap () 97 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.822268 ] alloc_pcache_rmap () 104 & rmap -> next ffff88207fcceff8 & flags ffff88207fccefd8 [ 1862.918995 ] __INIT_LIST_HEAD () : next ffff88207fcceff8 prev ffff88207fccf000 [ 1863.002202 ] __INIT_LIST_HEAD () 63 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1863.133253 ] __INIT_LIST_HEAD () : next ffff88207fcceff8 prev ffff88207fccf000 [ 1863.216459 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1863.289265 ] alloc_pcache_rmap () 114 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 Analysis: The @prev field in line 7 has address ffff88207fccf000 , which happen to the pgd page ( pgd ffff88207fccf000 ). Thus when we do list->prev = list , it writes to the first 8 bytes of pgd page, corrupts the original pgd entry. That is why we see a corrupted pgd entry ( ffff90207fcce000 ). This roots from kmalloc, which should not allocate such an object that cross two pages. 03/08 Thur \u00b6 Took several days off. This morning finished the porting of wait4 and waitid , which actually has a lot code change. The concept and mechanism is fairly simple, but the legacy UNIX tradition make the implementation quite complex. Now, look back to finish debugging the pcache issue. It must be fixed this week. python \u00b6 Tried python hello_world.py , the program runs for a while and crashes at a deterministic point: wuklab13 and wuklab15 , ~/ ttyS1 [ 419097.929969 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a4b008 flags : 0x50 ret : 0 ( OKAY ) [ 419098.039145 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a4c010 flags : 0x50 [ 419098.306537 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a4c010 flags : 0x50 ret : 0 ( OKAY ) [ 419098.413756 ] CPU5 PID32 sys_mprotect + 0x0 / 0x90 [ 419098.465753 ] SYSC_mprotect () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3d27 [ 419098.549990 ] start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1 [ 419098.614469 ] BUG : unable to handle kernel paging request at ffff9001801ff000 [ 419098.698703 ] IP : [ < ffffffff8102f7a9 > ] pcache_handle_fault + 0x69 / 0x6c0 [ 419098.774621 ] PGD 0 [ 419098.799579 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 419098.848457 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 312 [ 419098.916054 ] RIP : 0010 : [ < ffffffff8102f7a9 > ] [ < ffffffff8102f7a9 > ] pcache_handle_fault + 0x69 / 0x6c0 [ 419099.021089 ] RSP : 0000 : ffff88107e857ed8 EFLAGS : 000102 86 [ 419099.085567 ] RAX : ffff9001801ff000 RBX : ffff9001801ff000 RCX : 00003ff ffffff000 [ 419099.171884 ] RDX : 00000801801ff 000 RSI : 000000000060100 8 RDI : ffff88107e83d648 [ 419099.258199 ] RBP : ffff88107e857f18 R08 : 00007ff ff7fe3000 R09 : 00007ff ff7fe3000 [ 419099.344516 ] R10 : 0000000000000000 R11 : 0000000000000206 R12 : 000000000060100 8 [ 419099.430832 ] R13 : ffff88107e83d648 R14 : 0000000000000050 R15 : 00007ff ff7ffe150 [ 419099.517149 ] FS : 00007ff ff7fdf740 ( 0000 ) GS : ffff88207fc40000 ( 0000 ) knlGS : 0000000000000000 [ 419099.614905 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 419099.684582 ] CR2 : ffff9001801ff000 CR3 : 000000207f ccf000 CR4 : 00000000000406 a0 [ 419099.770899 ] Stack : [ 419099.795858 ] 00007ff ff7d8c000 0000000000002000 0000000000000001 0000000000000004 [ 419099.884254 ] 000000000060100 8 ffff88107e857f58 0000000000000000 00007ff ff7ffe150 [ 419099.972650 ] ffff88107e857f48 ffffffff81010082 0000000000000000 0000000000000001 [ 419100.061047 ] 0003 92 c29c720ba2 0000000000000000 00007ff fffffdc40 ffffffff8100d91f [ 419100.149442 ] 00007ff ff7ffe150 0000000000000000 0003 92 c29c720ba2 0000000000000001 [ 419100.237839 ] Call Trace : [ 419100.267998 ] < TSK > [ 419100.291917 ] [ < ffffffff81010082 > ] do_page_fault + 0xa2 / 0x1a0 [ 419100.357434 ] [ < ffffffff8100d91f > ] page_fault + 0x1f / 0x30 [ 419100.418792 ] < EOT > M : ... [ 419142.163396 ] handle_p2m_pcache_miss () cpu 4 I nid : 0 pid : 32 tgid : 32 flags : 50 vaddr : 0x7ffff7a4c010 [ 419142.268460 ] handle_p2m_pcache_miss () cpu 4 O nid : 0 pid : 32 tgid : 32 flags : 50 vaddr : 0x7ffff7a4c010 ( Last Message ) Dig deeper: int pcache_handle_fault ( struct mm_struct * mm , unsigned long address , unsigned long flags ) { .. pgd = pgd_offset ( mm , address ); pr_info ( \" addr: %#lx, pgd: %p \\n \" , address , pgd ); pud = pud_alloc ( mm , pgd , address ); pr_info ( \" addr: %#lx, pgd: %p pud %p \\n \" , address , pgd , pud ); if ( ! pud ) return VM_FAULT_OOM ; pmd = pmd_alloc ( mm , pud , address ); if ( ! pmd ) .. } [ 21130.503314 ] strace__mprotect cpu5 start = 0x7ffff7d8c000 , len = 0x2000 , prot ( 0x1 ) = PROT_READ [ 21130.598994 ] SYSC_mprotect () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3d27 [ 21130.682193 ] start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1 [ 21130.745635 ] addr : 0x601008 , pgd : ffff88207fccf000 [ 21130.805954 ] addr : 0x601008 , pgd : ffff88207fccf000 pud ffff9001801ff000 [ 21130.888116 ] BUG : unable to handle kernel paging request at ffff9001801ff000 [ 21130.971314 ] IP : [ < ffffffff8102fa11 > ] pcache_handle_fault + 0x91 / 0x6f0 Print pgd and pud info, these three messages are related and the last one leads to panic: wuklab13 ~/ ys / 030 8 - 6 [ 479.375498 ] addr : 0x400040 , pgd : ffff88207fccf000 [ 479.435819 ] pud_alloc_one () : addr : 0x400040 , pud : ffff88207fc6f000 [ 479.511739 ] pud_alloc () : addr : 0x400040 pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 479.649021 ] addr : 0x400040 , pgd : ffff88207fccf000 pud ffff88207fc6f000 [ 480.016381 ] addr : 0x600dd8 , pgd : ffff88207fccf000 [ 480.076701 ] pud_alloc () : addr : 0x600dd8 pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 480.213982 ] addr : 0x600dd8 , pgd : ffff88207fccf000 pud ffff88207fc6f000 [ 680.072819 ] addr : 0x601008 , pgd : ffff88207fccf000 [ 680.133138 ] pud_alloc () : addr : 0x601008 pgd ffff88207fccf000 , pgd . cont_va ffff90107e834000 , pud_index = 0x0 pud : ffff90107e834000 [ 680.270422 ] addr : 0x601008 , pgd : ffff88207fccf000 pud ffff90107e834000 [ 680.352583 ] BUG : unable to handle kernel paging request at ffff90107e834000 [ 680.435783 ] IP : [ < ffffffff8102fc43 > ] pcache_handle_fault + 0xb3 / 0x770 [ 680.510664 ] PGD 0 I need to check what happens between 480s to 680s. Something in between corrupted pgtable. I doubt it can be: copy_to_user related syscalls pcache establish mapping, mempcy all other memcpy strcpy etc stuff 03/02 Fri \u00b6 TODO: -add vsyscall- -pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.- pcache_exit_thread? I don\u2019t think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread. check python bug use omnigraffle to draw the whole workflow of pcache. Phoenix, word_count-seq, 4G dataset, 4GB pcache: [ 273.268853] Processor: Processor manager is running. [ 573.272479] page:ffffea0071bb9660 count:0 mapcount:-128 [ 573.332903] flags: 0x200000000000300(slab|slob_free) [ 573.392182] page dumped because: VM_BUG_ON_PAGE(page_ref_count(page) == 0) [ 573.474340] ------------[ cut here ]------------ [ 573.529459] BUG: failure at ./include/lego/mm.h:251/put_page_testzero()! [ 573.609537] Kernel Panic - not syncing: BUG! [ 573.660496] CPU: 4 PID: 13 Comm: kvictim_flushd 4.0.0-lego+ #18 [ 573.731212] Stack: [ 573.755132] ffff88207e4bfe10 ffffffff81023644 0000000000000008 ffff88207e4bfe20 [ 573.842490] ffff88207e4bfdd8 0000000021475542 0000000000000000 0000000000000000 [ 573.929848] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.017205] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.104563] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.191921] Call Trace: [ 574.221039] <TSK> [ 574.243919] [<ffffffff81023650>] panic+0xc2/0xeb [ 574.299038] [<ffffffff8105a35a>] ? client_internal_poll_sendcq+0x2a/0x80 [ 574.379115] [<ffffffff8105a4fd>] ? client_send_message_with_rdma_write_with_imm_request+0x14d/0x360 [ 574.487273] [<ffffffff8101ac3c>] ? task_tick_rt+0x2c/0xd0 [ 574.551751] [<ffffffff81018395>] ? scheduler_tick+0x55/0x60 [ 574.618308] [<ffffffff81015a45>] ? tick_handle_periodic+0x45/0x70 [ 574.691107] [<ffffffff810064c4>] ? apic_timer_interrupt+0x54/0x90 [ 574.763905] [<ffffffff8100dbaa>] ? smp__apic_timer_interrupt+0x6a/0x70 [ 574.841903] [<ffffffff8101198d>] ? printk+0x11d/0x1b0 [ 574.902222] [<ffffffff81025c00>] __free_pages+0x2e0/0x3c0 [ 574.966699] [<ffffffff81028472>] kfree+0x62/0x480 [ 575.022858] [<ffffffff8102e6be>] victim_flush_func+0x15e/0x1e0 [ 575.092536] [<ffffffff8102e560>] ? victim_try_fill_pcache+0x390/0x390 [ 575.169494] [<ffffffff8101e446>] kthread+0xf6/0x120 [ 575.227733] [<ffffffff8101e350>] ? __kthread_parkme+0x70/0x70 [ 575.296371] [<ffffffff8100de32>] ret_from_fork+0x22/0x30 [ 575.359810] <EOT> 03/01 Thur \u00b6 Weird. [43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25 [43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25 [43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150 [43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e [43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150 [43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e After all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private. Lego\u2019s current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux\u2019s batch flush. Today\u2019s case is flush tlb after making pte read-only. And this really has to be performed one by one","title":"March 2018"},{"location":"lego/log/log-03-2018/#march-2018","text":"","title":"March 2018"},{"location":"lego/log/log-03-2018/#0331-sat","text":"Stay humble. Be real.","title":"03/31 Sat"},{"location":"lego/log/log-03-2018/#0330-fri","text":"Our scheduling, or IB do have issues. I must revisit this. The case is: in P, we boot only 12 cores, and three of them are used by flush, sweep, and IB. So there are 9 cores left for user. Phoenix create 24 threads. During the run, a lot ib timeout will happen. If we have a good scheduling, this should never happen. I probably need to check more on this. Anyway. Today I reorganized the opcode things. And now I\u2019m adding the final large piece of Lego: replication. It should be much simpler than the pcache part. I will first write down what code I need to add, e.g., opcode, handler, buffer mgmt etc. End of day. Want to write down some simple thoughts on building system. Building system is fun, but you have to know that devil is in the details. And, you may end up debugging for many many hours on a very very little issue. But that is how it is. Building system does not mean you are always working on fantastic beautiful ideas. It is always about those little bugs, little things, trivial fixes, that make your system robust and usable. For example, the patch Yilun sent me today is about handling special cases of stat and lseek. The patch does not improve any performance or adding fancy features, it is a minor fix to make user progam run. But this enable us to run TF. I think it is a great patch and it stands for 90% of building systems in middle or late stage. Of course, there are other trivial things on building systems: 1) initialize every possible used variables, can be local variables, malloced buffers. 2) have decent cleanup, which is a counterpart of your initialization, like dequeue list, decrease counter etc. 3) Clear coding style, write code for others, for yourself when you read the code two weeks later. This one is hard, need experience. But can be learned. I think Yilun and Yutong both improved a lot during this project. Me? I learned this from NVM emulator protect. It is a painful one, but also a valuable one. 4) Decent protect source file organization. 5) Remember, draw, the connections between subsystems. By adding this new feature to this subsystem A, will it broke subsystem B, which is using subsystem A. Something like this. 6) clear mind on lock usage, multithread issue. This is the most difficult one. I would say I learned this by coding pcache, or mm. I would say, mm is the most difficult multithread issue one can encounter.","title":"03/30 Fri"},{"location":"lego/log/log-03-2018/#0326-mon","text":"Spent several days on replication design. Now I\u2019m back on coding and debuging track. Fixed a bug introduced by per-pte lock. A one hided by previous one big giant page table lock. Also add an option to boot socket 0 only if Processor is configured. This is because pcache is normally registered at socket 0, if we schedule user threads to sockets other than socket 0, that will have bad performance.","title":"03/26 Mon"},{"location":"lego/log/log-03-2018/#0322-thur","text":"","title":"03/22 Thur"},{"location":"lego/log/log-03-2018/#clear-registers-for-execve","text":"Want to figure out execve problem today. Check if pcache is clean after process_exit. Check if pgtable is clean. Well. Checked, both are clean. The bug looks like the return of main, evevntually does not go to library\u2019s exit. Is it because library pages are not loaded properly? Since the number of pgfault equals to normal setting, I guess it may originate from Memory side. TLB is also flushed, so TLB should not be a hidden issue. Going to check checsum. Well, checsum is okay too. Syscall execve will change ip, sp, flags registers. So it will use iretq instead of sysexit to return to userspace. Got an insteresting IB bug after execve. The CPU5 seems fail to return to userspace, and the CPU0 has the IB bug followed: [ 1201.940681 ] CPU : 5 PID : 32 Comm : seq . o 4.0.0 - lego - ys + # 609 [ 1202.006200 ] RIP : 0033 : [ < 0000000000401 d1d > ] [ < 0000000000401 d1d > ] 0x401d1d [ 1202.087320 ] RSP : 002 b : 00007ff fffffedb0 EFLAGS : 00000200 [ 1202.150760 ] RAX : 0000000000000000 RBX : 00000000004002e0 RCX : 000000000043 b2c7 [ 1202.236041 ] RDX : 00007ff fffffedc8 RSI : 00007ff fffffeb40 RDI : 000000000048f9f 0 [ 1202.321320 ] RBP : 00007ff fffffeb60 R08 : 00000000006 ba4a0 R09 : 00000000006 bc880 [ 1202.406601 ] R10 : 000000000000000f R11 : 0000000000000246 R12 : 0000000000000000 [ 1202.491881 ] R13 : 0000000000401 930 R14 : 0000000000401 9 c0 R15 : 0000000000000006 [ 1202.577161 ] FS : 0000000000000000 ( 0000 ) GS : ffff88207fc40000 ( 0000 ) knlGS : 0000000000000000 [ 1202.673880 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1202.742521 ] CR2 : 000000000042 c9a0 CR3 : 000000207f c2f000 CR4 : 00000000000406 a0 [ 1220.465601 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 1220.557225 ] IP : [ < ffffffff810591ef > ] ib_mad_completion_handler + 0x6f / 0x7c0 [ 1220.638344 ] PGD 0 [ 1220.662265 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 1220.710105 ] CPU : 0 PID : 27 Comm : ib_mad_completi 4.0.0 - lego - ys + # 609 [ 1220.786025 ] RIP : 0010 : [ < ffffffff810591ef > ] [ < ffffffff810591ef > ] ib_mad_completion_handler + 0x6f / 0x7c0 [ 1220.896265 ] RSP : 0000 : ffff88103eea7e30 EFLAGS : 00010246 [ 1220.959704 ] RAX : 0000000000000000 RBX : ffff88103eeac728 RCX : 0000000000000001 [ 1221.044985 ] RDX : 000000002 8000000 RSI : ffff88103ee8f000 RDI : ffff88107ff841d8 [ 1221.130265 ] RBP : ffff88103eea7ec0 R08 : 0000000000000000 R09 : ffff88103eea03c0 [ 1221.215545 ] R10 : ffff88103eea7ea0 R11 : 0000000000000001 R12 : ffff88103ee8c3f0 [ 1221.300825 ] R13 : ffff88103ee8c4e8 R14 : ffff88103eeac620 R15 : ffff88103eeac5f8 [ 1221.386106 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 1221.482825 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1221.551466 ] CR2 : 0000000000000020 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 1221.636746 ] Stack : [ 1221.660666 ] ffff88103eeaac10 ffff881000000001 ffff88103eeaac10 ffff88103eeaab50 [ 1221.748026 ] ffff88107fc05d80 ffff88103eea0000 ffff88103eeac728 000000 8000000000 [ 1221.835386 ] 0000012 83 eea7ea8 ffff88103ee8c9a8 000000007f cf2000 ffff000000000000 [ 1221.922746 ] ffff88107fcf0000 ffff88207ff6cbd8 ffff88107fcf76e8 ffff88103ee8c3f0 [ 1222.010106 ] ffffffff81059180 0000000000000000 ffff88103eea7f48 ffffffff81020866 [ 1222.097466 ] Call Trace : [ 1222.126586 ] < TSK > [ 1222.149466 ] [ < ffffffff81059180 > ] ? ib_mad_send_done_handler . isra .21 + 0x1d0 / 0x1d0 [ 1222.236826 ] [ < ffffffff81020866 > ] kthread + 0xf6 / 0x120 [ 1222.295066 ] [ < ffffffff81020770 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1222.363707 ] [ < ffffffff8100e4b2 > ] ret_from_fork + 0x22 / 0x30 [ root @ wuklab12 : LegoOS git :( master )] $ addr2line - e vmImage - i ffffffff810591ef / root / ys / LegoOS / drivers / infiniband / core / mad . c : 1899 / root / ys / LegoOS / drivers / infiniband / core / mad . c : 2324 It is ib_mad_recv_done_handler () Well\u2026 Eventually, at 22:09, I figured out.. After I cleaned up all registers (except IP, SP, CS, SS, FLAGS) within start_thread, the execve\u2019ed program can run to end successfully. I did not clear the registers because linux does not clear it. I thought this is fine. Glibc should clear it anyway, right? But anyway, this works.","title":"Clear Registers for execve()"},{"location":"lego/log/log-03-2018/#0321-wed","text":"Task 1: add some checking in ib, flush, sweep thread. 1) If cpu changed, 2) if nr_threads on this core > 1. Had an issue while testing: execve(). I ran a exec.o first, then do execve to run seq.o: wuklab13 0321 - 10 [ 970.380252 ] STDOUT : --- [ uname () : --- [ 970.431212 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x44605d flags : 0x150 [ 1101.862429 ] mlx4_ib_handle_error_cqe syndrome 21 [ 1101.915570 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1101.969649 ] send request failed at connection 4 as 12 [ 1102.029968 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.084046 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.138125 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1102.192203 ] fit_poll_cq : failed status ( 5 ) for wr_id 1054 [ 1102.256681 ] fit_poll_cq : failed status ( 5 ) for wr_id 1055 [ 1102.321160 ] csum : 442 a97c0 , reply -> csum : 2 d352c33 [ 1102.377319 ] fit_poll_cq : connection 4 Recv weird event as - 1 [ 1102.444916 ] pcache : ffff880180011180 mapcount : 0 refcount : 1 flags :( allocated | usable ) kva : ffff880100446000 [ 1102.558273 ] fit_poll_cq : failed status ( 5 ) for wr_id 1056 [ 1102.622751 ] pcache dumped because : csum mismatch [ 1102.677871 ] fit_poll_cq : connection 4 Recv weird event as - 30704 [ 1102.749627 ] ------------ [ cut here ] ------------ [ 1102.804746 ] fit_poll_cq : failed status ( 5 ) for wr_id 1057 [ 1102.869225 ] BUG : failure at managers / processor / pcache / fault . c : 237 / __pcache_do_fill_page () ! [ 1102.968022 ] fit_poll_cq : connection 4 Recv weird event as - 30704 [ 1103.039780 ] Kernel Panic - not syncing : BUG ! [ 1103.090739 ] CPU : 5 PID : 32 Comm : seq . o 4.0.0 - lego - ys + # 599 [ 1103.156256 ] Stack : [ 1103.180177 ] ffff88103e85be18 ffffffff8102676c ffffffff00000008 ffff88103e85be28 [ 1103.267533 ] ffff88103e85bde0 0000000021475542 00000000000002 96 ffff88103e85ba10 [ 1103.354892 ] ffffffff810195c5 ffff88207fc44980 0000000000000005 ffff88103e85ba28 [ 1103.442249 ] ffffffff81016c75 ffff88103e85ba40 ffff88103e85ba50 ffffffff810065d4 [ 1103.529607 ] ffffffff811d36e0 000000000000003 9 ffffffff81081718 ffff88103e85bb80 [ 1103.616964 ] Call Trace :","title":"03/21 Wed"},{"location":"lego/log/log-03-2018/#0320-tue","text":"Task 1: calculate failure numbers Task 2: read 0319-4 Log Task 3: opt pte lock Hmm, I finished the per-pte per-pmd lock patch. I think it works. But I do found an issue. When I run MT+2GB, it will create 24 threads. Since I marked 3 CPUs inactive, so all new 24 threads will be scheduled to other cores (I may need to check this!). At some point, Lego P either stuck, or a lot ibapi_send_reply timeout. When I change the cpu_online to may 0-6 , it finished. When I change it to 0-18 , also succeed. I really doubt if actually those pinned threads are not pinned. Need to check.","title":"03/20 Tue"},{"location":"lego/log/log-03-2018/#ib-bug-again","text":"Running MT-phoenix, 2GB, somehow crashed in the middle: [ 60095.857381 ] SYSC_close () CPU6 PID : 33 [ fd : 4 ] -> [ / proc / stat ] [ 60286.127359 ] mlx4_ib_handle_error_cqe syndrome 21 [ 60286.180503 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.234582 ] send request failed at connection 4 as 12 [ 60286.294903 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.348981 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.403062 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.457141 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.511221 ] send request failed at connection 4 as 5 [ 60286.570500 ] fit_poll_cq : failed status ( 5 ) for wr_id 1056 [ 60286.634980 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.689059 ] fit_poll_cq : failed status ( 5 ) for wr_id 1057 [ 60286.753539 ] send request failed at connection 4 as 5 [ 60286.812819 ] fit_poll_cq : failed status ( 5 ) for wr_id 1058 [ 60286.877298 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60286.931378 ] fit_poll_cq : failed status ( 5 ) for wr_id 1059 [ 60286.995857 ] send request failed at connection 4 as 5 [ 60287.055138 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.109217 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.163297 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.217376 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.271456 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.325536 ] send request failed at connection 4 as 5 [ 60287.384815 ] fit_poll_cq : failed status ( 5 ) for wr_id 1060 [ 60287.449294 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.503375 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 60287.596973 ] IP : [ < ffffffff81063ffd > ] fit_poll_cq + 0x4dd / 0x530 [ 60287.664574 ] send request failed at connection 4 as 5 [ 60287.723853 ] PGD 0 [ 60287.747772 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60287.801852 ] Oops : 0002 [ # 1 ] PREEMPT SMP PROCESSOR [ 60287.858013 ] send request failed at connection 4 as 5 [ 60287.917292 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 569 [ 60287.988010 ] RIP : 0010 : [ < ffffffff81063ffd > ] [ < ffffffff81063ffd > ] fit_poll_cq + 0x4dd / 0x530 [ 60288.084731 ] RSP : 0000 : ffff88103e84fd88 EFLAGS : 00010206 [ 60288.148170 ] RAX : 000000000000100 8 RBX : ffff88103e848438 RCX : 0000000000000014 [ 60288.233450 ] RDX : 0000000000000000 RSI : ffffffff811d36e0 RDI : ffffffff811dac08 [ 60288.318728 ] RBP : ffff88103e84fea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 60288.404008 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 60288.489288 ] R13 : ffff88207fd6e008 R14 : 0000000000000004 R15 : ffff88103e84fda0 [ 60288.574568 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60288.628647 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 60288.725367 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 60288.794006 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 60288.879285 ] send request failed at connection 4 as 5 [ 60288.938565 ] Stack : [ 60288.962484 ] ffffffff810031d9 000 801 d43e84fda0 0000000000000007 0000000000000424 [ 60289.049844 ] 000000 8100000005 00001008000000f 9 ffff88103e848868 00616e6440000014 [ 60289.137204 ] 0020004000000002 ffff88207fc00000 0000000000000425 000000 8100000005 [ 60289.224563 ] 00001008000000f 9 ffff88103e848868 007370654000000 d 0010004000000002 [ 60289.311922 ] ffffffff81010000 0000000000000426 000000 8100000005 00001008000000f 9 [ 60289.399282 ] Call Trace : [ 60289.428402 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60289.482482 ] < TSK > [ 60289.505361 ] [ < ffffffff810031d9 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 60289.584400 ] send request failed at connection 4 as 5 [ 60289.643680 ] [ < ffffffff81010000 > ] ? __ioremap_caller + 0x170 / 0x570 [ 60289.714400 ] [ < ffffffff81060000 > ] ? cm_work_handler + 0x270 / 0x1450 [ 60289.785119 ] [ < ffffffff81064050 > ] ? fit_poll_cq + 0x530 / 0x530 [ 60289.850639 ] [ < ffffffff81064064 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 60289.917198 ] [ < ffffffff81020c06 > ] kthread + 0xf6 / 0x120 [ 60289.975438 ] mlx4_ib_handle_error_cqe syndrome 5 [ 60290.029518 ] [ < ffffffff81020b10 > ] ? __kthread_parkme + 0x70 / 0x70 [ 60290.098157 ] [ < ffffffff8100e722 > ] ret_from_fork + 0x22 / 0x30 Uuh: [ 1002.803051 ] mlx4_ib_handle_error_cqe syndrome 1 [ 1002.855153 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1002.909232 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1002.963310 ] mlx4_ib_handle_error_cqe syndrome 5 [ 1003.017390 ] fit_poll_cq : failed status ( 1 ) for wr_id 512 [ 1003.080829 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000200 [ 1003.174425 ] IP : [ < ffffffff8105d499 > ] fit_poll_cq + 0x179 / 0x510 [ 1003.242024 ] PGD 0 [ 1003.265943 ] Oops : 0000 [ # 1 ] SMP MEMORY [ 1003.310661 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 149 [ 1003.381380 ] RIP : 0010 : [ < ffffffff8105d499 > ] [ < ffffffff8105d499 > ] fit_poll_cq + 0x179 / 0x510 [ 1003.478098 ] RSP : 0000 : ffff88104e84fd88 EFLAGS : 00010246 [ 1003.541537 ] RAX : ffff880000000000 RBX : ffff88104e848008 RCX : 00000000000000 80 [ 1003.626814 ] RDX : 0000000000000200 RSI : ffffffff811c76e0 RDI : ffffffff811d0988 [ 1003.712092 ] RBP : ffff88104e84fea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 1003.797369 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 1003.882648 ] R13 : ffff88207ff75008 R14 : 0000000000000004 R15 : ffff88104e84fda0 [ 1003.967925 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 1004.064644 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1004.133282 ] CR2 : 0000000000000200 CR3 : 0000000001131000 CR4 : 00000000000406 a0 [ 1004.218559 ] Stack : [ 1004.242479 ] ffffffff810031a9 ffff88104e84fda0 ffffffff81018ef4 0000000000000200 [ 1004.329837 ] 000000 8000000001 0000004 8000000 d7 ffff88104e848c98 00000000 81019302 [ 1004.417194 ] 0014000000000000 ffff88207fc00000 0000000000000201 ffffffff00000005 [ 1004.504552 ] ffff8810000000f9 ffff88104e848c98 0000000000000000 ffff88104e84fe38 [ 1004.591910 ] ffffffff810195a4 0000000000000202 ffff881000000005 ffff8810000000f9 [ 1004.679268 ] Call Trace : [ 1004.708388 ] < TSK > [ 1004.731267 ] [ < ffffffff810031a9 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 1004.810305 ] [ < ffffffff81018ef4 > ] ? resched_curr + 0x34 / 0x40 [ 1004.874783 ] [ < ffffffff810195a4 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 1004.942382 ] [ < ffffffff8105f458 > ] ? __schedule + 0xf8 / 0x1e0 [ 1005.005820 ] [ < ffffffff8105d830 > ] ? fit_poll_cq + 0x510 / 0x510 [ 1005.071338 ] [ < ffffffff8105d844 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 1005.137897 ] [ < ffffffff8101fdc6 > ] kthread + 0xf6 / 0x120 [ 1005.196135 ] [ < ffffffff8101fcd0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1005.264773 ] [ < ffffffff8100e472 > ] ret_from_fork + 0x22 / 0x30","title":"IB Bug again"},{"location":"lego/log/log-03-2018/#0319-mon","text":"Not too many days left!!! Got to design full replication mechanism and algorithm today. Merged pull request for pipe , pipe2 and /dev/null from Yilun. Our simple file op mgmt concerns me. I left a note at kernel/fork.c. Got a bug report from Yilun, syscall execv failed. To be honest, I\u2019ve never tried this syscall, always call it directly within kernel. [ 943.650712 ] CPU6 PID17 sys_execve + 0x0 / 0x10 [ 943.701899 ] BUG : unable to handle kernel paging request at 00000000004 90523 [ 943.702776 ] IP : [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.711501 ] PGD 0 [ 943.711911 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 943.712433 ] CPU : 6 PID : 17 Comm : word_count - pthr 4.0.0 - lego + # 64 [ 943.713126 ] RIP : 0010 : [ < ffffffff8103db86 > ] [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.714090 ] RSP : 001 8 : ffff88083e4bfe98 EFLAGS : 00010246 [ 943.714724 ] RAX : 0000000000000000 RBX : ffff88083e4b3780 RCX : 0000000000000000 [ 943.715511 ] RDX : 00000000ff ffffff RSI : 000000000000002f RDI : 00000000004 90523 [ 943.716297 ] RBP : ffff88083e4bfe98 R08 : 0000160000000000 R09 : ffff88083e4b8400 [ 943.717085 ] R10 : ffff880000000000 R11 : 6 db6db6db6db6db7 R12 : ffff88083e4b8000 [ 943.717871 ] R13 : ffff88083e4e6290 R14 : 00000000004 90523 R15 : ffff88083e4b3920 [ 943.718683 ] FS : 0000000000000000 ( 0000 ) GS : ffff88083fd80000 ( 0000 ) knlGS : 0000000000000000 [ 943.719650 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 943.720319 ] CR2 : 00000000004 90523 CR3 : 000000083e4 e7000 CR4 : 00000000000406 a0 [ 943.721106 ] Stack : [ 943.721459 ] ffff88083e4bff18 ffffffff8102c6bf ffff880800000000 0000000000000e10 [ 943.722541 ] 00007ff fffffedb0 0000000000400 d0d ffff88083e4c0000 00000000004 90523 [ 943.723624 ] ffff88083e4b9008 00007ff fffffed30 000000 8400000084 ffff88083e4bff58 [ 943.724706 ] 000000000000003 b 0000000000401 9 d0 0000000000401 a60 0000000000000000 [ 943.725789 ] ffff88083e4bff28 ffffffff8102c989 ffff88083e4bff48 ffffffff8100e5f5 [ 943.726870 ] Call Trace : [ 943.727260 ] < TSK > [ 943.727619 ] [ < ffffffff8102c6bf > ] do_execve + 0x4af / 0x770 [ 943.728236 ] [ < ffffffff8102c989 > ] sys_execve + 0x9 / 0x10 [ 943.728868 ] [ < ffffffff8100e5f5 > ] do_syscall_64 + 0x45 / 0xd0 [ 943.729499 ] [ < ffffffff8100d4ec > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 943.730222 ] < EOT > [ 943.730570 ] Code : d2 74 18 40 38 f2 89 f1 75 06 eb 0f 38 ca 74 0 b 48 83 c0 01 0f b6 10 84 d2 75 f1 5 d c3 0f 1f 84 00 00 00 00 00 55 31 c0 48 89 e5 < 0f > b6 17 40 38 f2 48 0f 44 c7 48 83 c7 01 84 d2 75 ee 5 d c3 66 [ 943.735455 ] RIP [ < ffffffff8103db86 > ] strrchr + 0x6 / 0x20 [ 943.736120 ] RSP < ffff88083e4bfe98 > [ 943.736598 ] CR2 : 00000000004 90523 It is setup_new_exec() -> set_task_comm() . I passed the user pointer to set_task_comm() , which I should pass a kernel pointer. And I actually found we missed a function: do_close_on_exec() . I also add a note above.","title":"03/19 Mon"},{"location":"lego/log/log-03-2018/#random-ib-bug","text":"Another weird bug after pathing loader. Actually, I tried the same setting twice, the second time it works. I guess this is some random IB bug. (Setting: 1P, 1M, 1S. Running a simple exec.c testing program, this have not reach that point yet.) wuklab13 031 9 - 2 [ 496.288272 ] p2m_fork ( cpu0 ) : I cur : 1 - kernel_init new : 31 [ 496.349624 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000004 [ 496.443216 ] IP : [ < ffffffff81064935 > ] fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0 [ 496.538892 ] PGD 0 [ 496.562811 ] Oops : 0002 [ # 1 ] PREEMPT SMP PROCESSOR [ 496.618968 ] CPU : 0 PID : 1 Comm : kernel_init 4.0.0 - lego - ys + # 559 [ 496.689684 ] RIP : 0010 : [ < ffffffff81064935 > ] [ < ffffffff81064935 > ] fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0 [ 496.814478 ] RSP : 0000 : ffff88107fcf7d00 EFLAGS : 00010202 [ 496.877915 ] RAX : 000000000000004 c RBX : 0000000000000004 RCX : 000000000000002 c [ 496.963190 ] RDX : 0000000000000004 RSI : 0000000000000001 RDI : ffff88207ff6d008 [ 497.048466 ] RBP : ffff88107fcf7d98 R08 : ffff88107fcf7e3c R09 : 0000000000000004 [ 497.133742 ] R10 : ffffffff81145fe0 R11 : 000000000000001 c R12 : 000000000000002 c [ 497.219018 ] R13 : 0000000000000001 R14 : ffff88107fcf7e40 R15 : ffff88207ff6d008 [ 497.304293 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 497.401009 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 497.469645 ] CR2 : 0000000000000004 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 497.554922 ] Stack : [ 497.578840 ] ffff88107fcf7d08 0000000000000000 00000000000002 82 ffffffff81077b10 [ 497.666195 ] 000000000000003 a 000000047f cf7e18 ffff88107fcf7e3c ffff88107fd5ed88 [ 497.753552 ] 000000010000002 c ffffff9b00000040 0000000000000034 ffffffff81145fe0 [ 497.840906 ] ffff88107fcf7db0 00000000000002 97 ffff88107fd5ed88 000000000000002 c [ 497.928263 ] ffff88107fcf7e3c ffff88107fcf7e40 000000000000003 9 ffff88107fcf7dc8 [ 498.015618 ] Call Trace : [ 498.044736 ] < TSK > [ 498.067615 ] [ < ffffffff810622ff > ] ibapi_send_reply_timeout + 0x3f / 0x50 [ 498.142492 ] [ < ffffffff8103b0d4 > ] ? net_send_reply_timeout + 0x94 / 0x132 [ 498.218408 ] [ < ffffffff8103b0d4 > ] net_send_reply_timeout + 0x94 / 0x132 [ 498.292244 ] [ < ffffffff8102c683 > ] p2m_fork + 0xd3 / 0x200 [ 498.351521 ] [ < ffffffff8101f490 > ] do_fork + 0xf0 / 0x150 [ 498.409758 ] [ < ffffffff8101f514 > ] kernel_thread + 0x24 / 0x30 [ 498.473195 ] [ < ffffffff8115bf21 > ] processor_manager_init + 0x21 / 0x50 [ 498.545991 ] [ < ffffffff81000354 > ] kernel_init + 0x94 / 0x120 [ 498.608388 ] [ < ffffffff810002c0 > ] ? 0xffffffff810002c0 [ 498.668706 ] [ < ffffffff81019b0a > ] ? schedule_tail + 0xa / 0x40 [ 498.733182 ] [ < ffffffff810002c0 > ] ? 0xffffffff810002c0 [ 498.793499 ] [ < ffffffff8100e762 > ] ret_from_fork + 0x22 / 0x30 [ 498.856936 ] < EOT >","title":"Random IB Bug"},{"location":"lego/log/log-03-2018/#0318-sun","text":"Got a bug report after enable preempt and sweep thread [ 582.545444 ] pcache : ffff8801812cb680 mapcount : 1 refcount : 2 flags :( locked | allocated | usable | valid | reclaim ) kva : ffff88014b2da000 [ 582.678677 ] pcache dumped because : PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm )) [ 582.758760 ] rmap : ffff88207e5e37e8 flags : 0x0 owner - tgid : 33 user_va : 0x7fff0b2da000 ptep : ffff88207e4a86d0 [ 582.870046 ] pte : ffff88207e4a86d0 pfn : 0x0 flags :() [ 582.926210 ] ------------ [ cut here ] ------------ [ 582.981333 ] BUG : failure at managers / processor / pcache / victim . c : 604 / victim_finish_insert () ! [ 583.080137 ] Kernel Panic - not syncing : BUG ! ... ... [ 588.847239 ] nr_pgfault : 591101 [ 588.883641 ] nr_clflush : 66176 [ 588.919003 ] nr_pgfault_wp : 0 [ 588.953325 ] nr_pgfault_wp_cow : 0 [ 588.991806 ] nr_pgfault_wp_reuse : 0 [ 589.032368 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 589.091651 ] nr_pcache_fill_from_memory : 587057 [ 589.144694 ] nr_pcache_fill_from_victim : 4038 [ 589.195656 ] nr_pcache_eviction_triggered : 439562 [ 589.250780 ] nr_pcache_eviction_eagain_freeable : 373382 [ 589.312143 ] nr_pcache_eviction_eagain_concurrent : 0 [ 589.370386 ] nr_pcache_eviction_failure_find : 0 [ 589.423429 ] nr_pcache_eviction_failure_evict : 0 [ 589.477512 ] nr_pcache_eviction_succeed : 66176 [ 589.529514 ] nr_victim_eviction_triggered : 733361 [ 589.584638 ] nr_victim_eviction_eagain : 671227 [ 589.636640 ] nr_victim_eviction_succeed : 62134 [ 589.688642 ] nr_victim_prepare_insert : 66180 [ 589.738566 ] nr_victim_finish_insert : 66176 [ 589.787447 ] nr_victim_flush_submitted : 66176 [ 589.838411 ] nr_victim_flush_finished : 66176 [ 589.888332 ] nr_victim_flush_async_run : 0 [ 589.935135 ] nr_victim_flush_sync : 0 [ 589.976738 ] nr_sweep_run : 50580 [ 590.014179 ] nr_sweep_nr_pset : 116770383 [ 590.059943 ] nr_sweep_nr_moved_pcm : 100686435 This is an interesting bug. Two threads, one doing munmap or mremap, one doing eviction. They are using the same pcm. munmap and mremap will use pte_get_and_clear() to get the pcm. While eviction will call pcache_try_to_unamp , which will further call rmap_get_locked_pte() , in which we check if the pte is none, if it is, then we know this is under munmap or mremap, then we skip. This is absolutely wrong. When pcache_try_to_unamp is called by eviction, it should always unmap ALL rmap. The above case is triggered because both two threads skip the final __pcache_remove_rmap . Hmm, looks like open/close filename is wrong. I need to check. Last Log from MT+2GB, computation finished: wuklab13 031 8 - 10 [ 627.280016 ] **** ERROR : *** current : 32 : kevict_sweepd caller : ( null ) **** [ pte == rmap -> page_table ] && [ pcache_pfn != pte_pfn ] **** rmap -> owner_process : word_count - pthr uva : 0x7fff78f52000 ptep : ffff88107e87fa90 , rmap -> page_table : ffff88107e87fa90 **** pcache_pfn : 0x168f52 , pte_pfn : 0x178f52 [ 627.624239 ] rmap : ffff88107dc73740 flags : 0x0 owner - tgid : 33 user_va : 0x7fff78f52000 ptep : ffff88107e87fa90 [ 627.735513 ] pte : ffff88107e87fa90 pfn : 0x0 flags :() [ 627.791670 ] pcache_rmap dumped because : Corrupted RMAP [ 627.853026 ] pcache : ffff880181a3d480 mapcount : 1 refcount : 2 flags :( locked | allocated | usable | valid ) kva : ffff880168f52000 [ 627.979901 ] pcache dumped because : Corrupted RMAP [ 628.036057 ] ------------ [ cut here ] ------------ [ 628.091175 ] BUG : failure at managers / processor / pcache / rmap . c : 109 / report_bad_rmap () ! [ 628.182691 ] Kernel Panic - not syncing : BUG ! [ 628.233647 ] CPU : 5 PID : 32 Comm : kevict_sweepd 4.0.0 - lego - ys + # 543 [ 628.307483 ] Stack : [ 628.331401 ] ffff88107e85bd00 ffffffff81026d24 000000000000000 8 ffff88107e85bd10 [ 628.418756 ] ffff88107e85bcc8 0000000021475542 0000000000000000 0000000000000000 [ 628.506113 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.593468 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.680823 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 628.768179 ] Call Trace : [ 628.797299 ] < TSK > [ 628.820176 ] [ < ffffffff81026d30 > ] panic + 0xc2 / 0x102 [ 628.876334 ] [ < ffffffff8101c6ac > ] ? task_tick_rt + 0x2c / 0xd0 [ 628.940811 ] [ < ffffffff8101c6ac > ] ? task_tick_rt + 0x2c / 0xd0 [ 629.005288 ] [ < ffffffff81019bfc > ] ? scheduler_tick + 0x5c / 0x70 [ 629.071843 ] [ < ffffffff81017195 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 629.144639 ] [ < ffffffff81006704 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 629.217436 ] [ < ffffffff8100e4da > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 629.295432 ] [ < ffffffff81012d94 > ] ? printk + 0x124 / 0x1c0 [ 629.355748 ] [ < ffffffff8103ad1f > ] report_bad_rmap + 0x144 / 0x144 [ 629.423345 ] [ < ffffffff81031046 > ] pcache_referenced_trylock_one + 0x1c6 / 0x2c0 [ 629.505500 ] [ < ffffffff8100e4da > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 629.583497 ] [ < ffffffff810328a1 > ] rmap_walk + 0x71 / 0xe0 [ 629.642774 ] [ < ffffffff81033329 > ] pcache_referenced_trylock + 0x59 / 0xd0","title":"03/18 Sun"},{"location":"lego/log/log-03-2018/#0317-sat","text":"I\u2019m too tired today. Coding side, I will only optimize sweep. Besides, I will book tickets for Iceland trip.","title":"03/17 Sat"},{"location":"lego/log/log-03-2018/#0316-friday","text":"Task 1 : Add physical memory counter. It is a per-zone based counter, even though there is also some global counters. In Linux, per-cpu counter is first accumlated, global counter is updated only when per-cpu ones overflow. Lego\u2019s initial version save the trouble of per-cpu counter, I only port one global counter today, because I\u2019m not quite confident about our percpu_alloc\u2026 Anway, the info is reported in the format of manager_sysinfo . Do note this is different from the oirginal sysinfo structure, which is used by sysinfo syscall. Task 2 : Patch get_random_number and /dev/urandom /dev/random. Others wrote the code, but he did not stick to the tradition of format naming. So I have to rewrite some of them. Sigh. Task 3 : optimize sweep","title":"03/16 Friday"},{"location":"lego/log/log-03-2018/#0315-thur","text":"Forgot to write the log yesterday. I actually solved the major bug, the refcount and eviction one. That is really nasty. I basically used pte lock, pcache_lock, and refcount to synchronize between eviction routine and other users such as munmap, mremap, write-protected-handler. I\u2019m really not sure if this mode can be reproduced if I have any other similar systems. But I\u2019m glad that I find a way to do this. Today I got few tasks going on. First merge storage syscall branch, then add sched_yield syscall, add zone/node counters, and probably patch get_random_number. Task 1 : Merge Yilun\u2019s storage pull request, has bunch syscalls. I\u2019m reviewing now. truncate ftruncate getdents getcwd mkdir rmdir creat unlink unlinkat readlink statfs sync Task 2 : Add sched_yield() . Fairly simple. Task 3 : Add physical memory counter. Fairly complex. The underlying is built long time ago. Need to pick up some. Well some facts: pg_data_t (and zone) is allcoated by alloc_node_data if NUMA is configured. all zones are built and initliazed in memory_init() in Lego stats are reset to 0 when pg_data_t allocated (DUH?). Played directly in page_alloc.c Have to continue tomorrow. Task 4 : Patch get_random_number and /dev/urandom","title":"03/15 Thur"},{"location":"lego/log/log-03-2018/#0313-wed","text":"The slow victim flush issue is solved by pinning the thread to a core and remove that core from active_cpu mask. Today I\u2019m going to solve the SMP object issue. I\u2019m hoping by solving this, we can have a complete working pcache and victim cache. Continue yesterday\u2019s log: wuklab13 0313 - 12 [ 1073.616269 ] pcache : ffff880180777a80 mapcount : 0 refcount : 3 flags :( locked | allocated | usable ) kva : ffff88011ddea000 [ 1073.734941 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff4ddea000 [ 1073.822304 ] pcache dumped because : evict / ref bug [ 1073.987667 ] BUG : failure at managers / processor / pcache / evict . c : 301 / pcache_evict_line () ! [ 1074.082308 ] BUG : failure at managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () ! [ 1074.172789 ] Kernel Panic - not syncing : BUG ! [ 1074.223751 ] CPU : 23 PID : 50 Comm : word_count - pthr 4.0.0 - lego - ys + # 476 Time CPU0 CPU1 0 pcache_evict_line() zap_pte_range() 1 find @pcm to evict prepare to unmap pte which points to @pcm 2 lock_pcache() .. 3 pcache_try_to_unmap() pte_offset_lock() 4 try to lock pte pcache_zap_pte() 5 ..spin.. trylock_pcache (failed) 6 ..spin.. unlock pte 7 lock pte trylock pcache 8 clear pte ..spin.. 9 unlock pte ..spin.. 10 unlock pcache ..spin.. 11 .. lock pcache 12 .. lock pte 13 .. HERE, should check if pte changed! Huh, patched both eviction and other code. Use refcount, pcache lock, pte lock to synchronize between all users. Make sure a going-to-be-evicted pcm will not be used by others. And others will not have a chance to use such line.","title":"03/13 Wed"},{"location":"lego/log/log-03-2018/#0312-tue","text":"Continue victim cache. The current conclusion is victim has a unbalanced input and output rate. That is why some cores timeout and abort. Got some more clean log. The log told us that the flushd_victim is too slow at flushing content. Next I going to print the current flush queue content. Make sure that they are really not flushed. If so, I want to add code to flush sync. [ 318.193591 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 318.330986 ] -- Start Dump Victim Cache -- [ 318.388190 ] -- CPU4 [ word_count - pthr ][ pid = 54 , tgid = 32 ] -- [ 318.456835 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 318.627406 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90748000 [ 318.707492 ] pset : ffff88207f81d200 set_idx : 1864 nr_lru : 8 [ 318.775096 ] [ 318.792778 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 318.963349 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90749000 [ 319.043435 ] pset : ffff88207f81d240 set_idx : 1865 nr_lru : 8 [ 319.111040 ] [ 319.128721 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 319.299292 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90746000 [ 319.379378 ] pset : ffff88207f81d180 set_idx : 1862 nr_lru : 8 [ 319.446983 ] [ 319.464664 ] victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 319.635237 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 319.715321 ] pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 319.782927 ] [ 319.800608 ] victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 319.971179 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90745000 [ 320.051265 ] pset : ffff88207f81d140 set_idx : 1861 nr_lru : 8 [ 320.118870 ] [ 320.136551 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 320.307123 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074c000 [ 320.387208 ] pset : ffff88207f81d300 set_idx : 1868 nr_lru : 8 [ 320.454813 ] [ 320.472494 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 320.643066 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90747000 [ 320.723152 ] pset : ffff88207f81d1c0 set_idx : 1863 nr_lru : 8 [ 320.790756 ] [ 320.808438 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 320.979009 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 321.059096 ] pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 [ 321.126700 ] [ 321.144381 ] -- End Dump Victim Cache -- [ 321.200545 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 321.278552 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 321.511925 ] nr_pgfault : 551908 [ 321.546357 ] nr_clflush : 33449 [ 321.581718 ] nr_pgfault_wp : 0 [ 321.616040 ] nr_pgfault_wp_cow : 0 [ 321.654523 ] nr_pgfault_wp_reuse : 0 [ 321.695087 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 321.754371 ] nr_pcache_fill_from_memory : 546067 [ 321.807414 ] nr_pcache_fill_from_victim : 5750 [ 321.858378 ] nr_pcache_eviction_triggered : 38689 [ 321.912461 ] nr_pcache_eviction_eagain : 5239 [ 321.962385 ] nr_pcache_eviction_succeed : 33449 [ 322.014389 ] nr_victim_eviction_triggered : 41887455 [ 322.071592 ] nr_victim_eviction_eagain : 41859764 [ 322.125676 ] nr_victim_eviction_succeed : 27691 [ 322.177680 ] nr_victim_prepare_insert : 33450 [ 322.227603 ] nr_victim_finish_insert : 33449 [ 322.276487 ] nr_victim_flush_submitted : 33449 [ 322.327451 ] nr_victim_flush_finished : 33449 [ 322.377374 ] nr_victim_flush_async_run : 26989 [ 322.428338 ] nr_victim_flush_sync : 0 Yes, this victims are truly not being flushed. They are inside the flush_queue. No bug, hoo! Just some performance coding issues. But god why the flushd does not get a chance to run in 10 seconds? Hmm\u2026 [ 5520.236187 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff464fa000 [ 5530.404269 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 5530.541664 ] CPU4 PID54 -- Start Dump Victim Cache [ 0 ] [ 5530.606147 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 5530.789194 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90747000 [ 5530.880717 ] CPU4 PID54 rmap to pset : ffff88207f81d1c0 set_idx : 1863 nr_lru : 8 [ 5530.968080 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 5531.151128 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 5531.242652 ] CPU4 PID54 rmap to pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 5531.330015 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 5531.513063 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074c000 [ 5531.604586 ] CPU4 PID54 rmap to pset : ffff88207f81d300 set_idx : 1868 nr_lru : 8 [ 5531.691950 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 5531.874997 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 5531.966521 ] CPU4 PID54 rmap to pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 [ 5532.053885 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 5532.236932 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90748000 [ 5532.328456 ] CPU4 PID54 rmap to pset : ffff88207f81d200 set_idx : 1864 nr_lru : 8 [ 5532.415819 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 5532.598867 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90749000 [ 5532.690390 ] CPU4 PID54 rmap to pset : ffff88207f81d240 set_idx : 1865 nr_lru : 8 [ 5532.777753 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 5532.960802 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90746000 [ 5533.052325 ] CPU4 PID54 rmap to pset : ffff88207f81d180 set_idx : 1862 nr_lru : 8 [ 5533.139689 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 5533.322736 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff90745000 [ 5533.414259 ] CPU4 PID54 rmap to pset : ffff88207f81d140 set_idx : 1861 nr_lru : 8 [ 5533.501623 ] CPU4 PID54 -- End Dump Victim Cache [ 0 ] [ 5533.566106 ] CPU4 PID54 -- Start Dump Victim Flush Queue [ 0 ] [ 5533.635789 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 5533.818837 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 5534.001884 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 5534.184931 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 5534.367978 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 5534.551025 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 5534.734074 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 5534.917120 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 5535.100168 ] CPU4 PID54 -- End Dump Victim Flush Queue [ 0 ] [ 5535.169851 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 5535.247854 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 5535.480513 ] nr_pgfault : 549578 [ 5535.514943 ] nr_clflush : 31822 [ 5535.550304 ] nr_pgfault_wp : 0 [ 5535.584625 ] nr_pgfault_wp_cow : 0 [ 5535.623107 ] nr_pgfault_wp_reuse : 0 [ 5535.663669 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 5535.722952 ] nr_pcache_fill_from_memory : 544279 [ 5535.775993 ] nr_pcache_fill_from_victim : 5201 [ 5535.826955 ] nr_pcache_eviction_triggered : 37437 [ 5535.881038 ] nr_pcache_eviction_eagain : 5614 [ 5535.930960 ] nr_pcache_eviction_succeed : 31822 [ 5535.982963 ] nr_victim_eviction_triggered : 42000029 [ 5536.040165 ] nr_victim_eviction_eagain : 41973416 [ 5536.094247 ] nr_victim_eviction_succeed : 26613 [ 5536.146249 ] nr_victim_prepare_insert : 31823 [ 5536.196171 ] nr_victim_finish_insert : 31822 [ 5536.245052 ] nr_victim_flush_submitted : 31822 [ 5536.296015 ] nr_victim_flush_finished : 31822 [ 5536.345937 ] nr_victim_flush_async_run : 26718 [ 5536.396899 ] nr_victim_flush_sync : 0 Hmm, got some interesting bug, which never happened before. We did a unmap before finish_insert , so the mapcount must be zero. Since we have the Reclaim set for the candidate. But it looks like other code does not too much about the Reclaim bit. I need to check. [ 1009.676839 ] victim_flush_async CPU4 jobs 1 [ 1009.725830 ] victim_flush_async CPU4 jobs 1 [ 1009.774423 ] victim_flush_async CPU4 jobs 1 [ 1009.823147 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff465fc000 [ 1009.910479 ] pcache : ffff88018098d740 mapcount : 1 refcount : 3 flags :( locked | allocated | usable | valid | reclaim ) kva : ffff88012635d000 [ 1010.045652 ] pcache dumped because : PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm )) [ 1010.125725 ] victim_flush_async CPU4 jobs 1 [ 1010.174602 ] ------------ [ cut here ] ------------ [ 1010.229717 ] BUG : failure at managers / processor / pcache / victim . c : 601 / victim_finish_insert () ! [ 1010.328509 ] victim_flush_async CPU4 jobs 1 [ 1010.377385 ] Kernel Panic - not syncing : BUG ! [ 1010.428341 ] CPU : 20 PID : 47 Comm : word_count - pthr 4.0.0 - lego - ys + # 468 [ 1010.505294 ] Stack : [ 1010.529212 ] ffff881f2040fe08 ffffffff810259f4 000000000000000 8 ffff881f2040fe18 [ 1010.616565 ] ffff881f2040fdd0 0000000021475542 0000000000000000 0000000000000000 [ 1010.703918 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.791270 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.878623 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1010.965976 ] Call Trace : [ 1010.995095 ] < TSK > [ 1011.017972 ] [ < ffffffff81025a00 > ] panic + 0xc2 / 0x102 [ 1011.074127 ] [ < ffffffff81063a8a > ] ? client_internal_poll_sendcq + 0x2a / 0x80 [ 1011.154202 ] [ < ffffffff81063c2d > ] ? client_send_message_with_rdma_write_with_imm_request + 0x14d / 0x360 [ 1011.262351 ] [ < ffffffff8101bffc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1011.326827 ] [ < ffffffff81019755 > ] ? scheduler_tick + 0x55 / 0x60 [ 1011.393382 ] [ < ffffffff81016e25 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1011.466175 ] [ < ffffffff810066e4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1011.538969 ] [ < ffffffff8100e4aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1011.616964 ] [ < ffffffff81012cfd > ] ? printk + 0x11d / 0x1b0 [ 1011.677279 ] [ < ffffffff81032a19 > ] victim_finish_insert + 0x89 / 0x230 [ 1011.749032 ] [ < ffffffff81031a99 > ] pcache_evict_line + 0x79 / 0x280 [ 1011.817667 ] [ < ffffffff8102f00a > ] pcache_alloc + 0x23a / 0x340 [ 1011.882141 ] [ < ffffffff8102e4da > ] common_do_fill_page + 0x2a / 0x1b0 [ 1011.952856 ] [ < ffffffff8102e160 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1012.023570 ] [ < ffffffff8102e802 > ] pcache_handle_fault + 0x1a2 / 0x660 [ 1012.095324 ] [ < ffffffff810102b2 > ] do_page_fault + 0xa2 / 0x1a0 [ 1012.159799 ] [ < ffffffff8100dadf > ] page_fault + 0x1f / 0x30 Interesting. Memory consistency issue? Actually, I\u2019m not sure if it is the v->flags = 0 issue. Others use atomic bit operations to play with this flag, while the reset is a simple store. I checked the list operations, all of them are protected by spinlock. So the below should never happen in theory. I\u2019m changing the v->flags = 0 to smp_store_mb(v->flags, 0) , which is a xchg in x86. Same for pcache. [ 1773.814490] CPU17 PID44 victim:ffff88207ff71000 index:0 refcount:1 nr_fill:0 locked:0 flags:(allocated|usable) pcm: (null) pset: (null) [ 1773.979705] CPU17 PID44 hit[0] owner: [word_count-pthr][32] addr: 0x7fff95b1c000 [ 1774.072260] CPU17 PID44 rmap to pset:ffff88207f96c700 set_idx: 23324 nr_lru:8 [ 1774.161694] CPU17 PID44 victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimUsable(v)) [ 1774.259451] ------------[ cut here ]------------ [ 1774.314567] BUG: failure at managers/processor/pcache/victim.c:231/find_victim_to_evict()! [ 1774.413363] Kernel Panic - not syncing: BUG! [ 1774.464320] CPU: 17 PID: 44 Comm: word_count-pthr 4.0.0-lego-ys+ #47 ... [ 1781.363348] nr_pcache_fill_from_victim: 2 Did another run. I added an explicit wake_up_victim_flushd if victim failed to evict any line. But this fails with IB failure.. [ 2336.950087 ] CPU4 PID : 54 Abort victim alloc ( 20010 ms ) nr_usable_victims : 8 req from pset : ffff88207f81d340 , pset_idx : 1869 , nr_lru : 7 [ 2337.087474 ] CPU4 PID54 -- Start Dump Victim Cache [ 0 ] [ 2337.151955 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 2337.334999 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074a000 [ 2337.426521 ] CPU4 PID54 rmap to pset : ffff88207f81d280 set_idx : 1866 nr_lru : 8 [ 2337.513883 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 2337.696927 ] CPU4 PID54 hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff9074b000 [ 2337.788450 ] CPU4 PID54 rmap to pset : ffff88207f81d2c0 set_idx : 1867 nr_lru : 8 ... ... [ 2340.111861 ] CPU4 PID54 -- Start Dump Victim Flush Queue [ 0 ] [ 2340.181543 ] CPU4 PID54 victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d140 [ 2340.364587 ] CPU4 PID54 victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d180 [ 2340.547632 ] CPU4 PID54 victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d1c0 [ 2340.730675 ] CPU4 PID54 victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d200 [ 2340.913720 ] CPU4 PID54 victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d240 [ 2341.096763 ] CPU4 PID54 victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d280 [ 2341.279808 ] CPU4 PID54 victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d2c0 [ 2341.462851 ] CPU4 PID54 victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207f81d300 [ 2341.645895 ] CPU4 PID54 -- End Dump Victim Flush Queue [ 0 ] [ 2341.715577 ] CPU4 PID : 54 fail to allocate pcache or victim cache lines . [ 2341.793579 ] word_count - pthr [ 54 ] : segfault at 0x74d000 ip 00000000004024 9 d sp 00007ff f7674cd80 error 6 [ 2476.201442 ] mlx4_ib_handle_error_cqe syndrome 21 [ 2476.254590 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.308670 ] send request failed at connection 4 as 12 [ 2476.368991 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.423073 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.477153 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2476.531236 ] client_poll_cq : failed status ( 5 ) for wr_id 1051 [ 2476.598837 ] client_poll_cq : failed status ( 5 ) for wr_id 1052 [ 2476.666438 ] __clflush_one () : EPERM : Operation not permitted tsk : 32 user_va : 0x7fff90745000 [ 2476.765240 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2476.840122 ] client_poll_cq : failed status ( 5 ) for wr_id 1053 [ 2476.907724 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2476.982605 ] client_poll_cq : failed status ( 5 ) for wr_id 1054 [ 2477.050207 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2477.125089 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.179169 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.233251 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.287332 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.341414 ] client_poll_cq : failed status ( 5 ) for wr_id 1055 [ 2477.409016 ] client_poll_cq : failed status ( 5 ) for wr_id 1056 .. .. [ 2477.761583 ] client_poll_cq : connection 4 Recv weird event as - 30704 [ 2477.836464 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.890545 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.944626 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2477.998707 ] mlx4_ib_handle_error_cqe syndrome 5 [ 2478.052789 ] client_poll_cq : failed status ( 5 ) for wr_id 1059 [ 2478.120392 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 2478.213992 ] IP : [ < ffffffff81064894 > ] client_poll_cq + 0x1f4 / 0x6c0 [ 2478.284714 ] PGD 0 [ 2478.308635 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 2478.356476 ] CPU : 2 PID : 29 Comm : recvpollcq 4.0.0 - lego - ys + # 473 [ 2478.427197 ] RIP : 0010 : [ < ffffffff81064894 > ] [ < ffffffff81064894 > ] client_poll_cq + 0x1f4 / 0x6c0 [ 2478.527040 ] RSP : 0000 : ffff88107e143d90 EFLAGS : 00010246 [ 2478.590481 ] RAX : 0000000000000000 RBX : ffff88207fc6e000 RCX : 0000000000000000 [ 2478.675762 ] RDX : 000000000000100 8 RSI : ffffffff811d36e0 RDI : ffffffff811dab08 [ 2478.761044 ] RBP : ffff88107e143eb0 R08 : 0000000000000000 R09 : 0000000000000000 [ 2478.846327 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : ffff88207fd4f000 [ 2478.931609 ] R13 : 0000000000000004 R14 : ffff88107e143da8 R15 : 0000000000000000 [ 2479.016890 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc20000 ( 0000 ) knlGS : 0000000000000000 [ 2479.113613 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 2479.182254 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 2479.267536 ] Stack : [ 2479.291457 ] ffff88107e143da0 001012 9 c81019794 0000000000000001 0000000000000423 [ 2479.378818 ] 000000 8100000005 00001008000000f 9 ffff88207fd39000 0000000040000000 [ 2479.466180 ] 000f 004000000002 ffff88107e140000 0000000000000424 ffff881000000005 [ 2479.553542 ] 00000000000000f 9 ffff88207fd39000 ffff88107e143e38 ffffffff81019e44 [ 2479.640904 ] 0000000000000001 0000000000000425 ffff881000000005 ffffffff000000f9 [ 2479.728266 ] Call Trace : [ 2479.757386 ] < TSK > [ 2479.780268 ] [ < ffffffff81019e44 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 2479.847869 ] [ < ffffffff81066d78 > ] ? __schedule + 0xf8 / 0x1e0 [ 2479.911311 ] [ < ffffffff81064d60 > ] ? client_poll_cq + 0x6c0 / 0x6c0 [ 2479.979952 ] [ < ffffffff81064d70 > ] client_poll_cq_pass + 0x10 / 0x20 [ 2480.049634 ] [ < ffffffff81020336 > ] kthread + 0xf6 / 0x110 [ 2480.107875 ] [ < ffffffff81020240 > ] ? __kthread_parkme + 0x70 / 0x70 [ 2480.176516 ] [ < ffffffff8100e732 > ] ret_from_fork + 0x22 / 0x30 A classical SMP bug. Lucky to find this one. Let me try to describe this. There are two CPU1. CPU0 and CPU1. CPU0 is doing eviction while CPU1 is doing munmap->pcache_zap_pte. The CPU0 slected a pcm, while this pcm happen to be zapped at the same time by CPU1. There are not enough actions to either 1) prevent CPU0 from selecting this pcm, 2) prevent CPU1 from using this pcm. Both solutions might be work. But we need as least one. wuklab13 0313 - 12 [ 1073.616269 ] pcache : ffff880180777a80 mapcount : 0 refcount : 3 flags :( locked | allocated | usable ) kva : ffff88011ddea000 [ 1073.734941 ] __clflush_one () : EFAULT : bad address tsk : 32 user_va : 0x7fff4ddea000 [ 1073.822304 ] pcache dumped because : evict / ref bug [ 1073.987667 ] BUG : failure at managers / processor / pcache / evict . c : 301 / pcache_evict_line () ! [ 1074.082308 ] BUG : failure at managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () ! [ 1074.172789 ] Kernel Panic - not syncing : BUG ! [ 1074.223751 ] CPU : 23 PID : 50 Comm : word_count - pthr 4.0.0 - lego - ys + # 476","title":"03/12 Tue"},{"location":"lego/log/log-03-2018/#0311-mon","text":"","title":"03/11 Mon"},{"location":"lego/log/log-03-2018/#debug-victim-cache","text":"Morning. Today I will continue debugging victim and clflush, running with MT phoenix+2GB, seq+4GB. Sounds good. Digging into yesterday\u2019s 21th run log. The warning comes from victim_alloc_slowpath . The allocation abort after 10 seconds timeout. And interestingly, a lot threads abort. (The case is, pset is full, so pcache_alloc will try to evict one to victim cache. But victim cache is full as well. So it needs to evict one victim cache line too. Somehow this does not proceed as planned.) I guess somewhere deadlock happens. [ 1682.040428 ] WARNING : CPU : 7 PID : 34 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1682.161063 ] WARNING : CPU : 19 PID : 46 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1686.602779 ] WARNING : CPU : 10 PID : 37 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.384837 ] WARNING : CPU : 3 PID : 53 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.505474 ] WARNING : CPU : 21 PID : 48 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.737386 ] WARNING : CPU : 16 PID : 43 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1687.859063 ] WARNING : CPU : 4 PID : 54 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.034819 ] WARNING : CPU : 6 PID : 56 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.210574 ] WARNING : CPU : 14 PID : 41 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1688.488246 ] WARNING : CPU : 5 PID : 55 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1689.598935 ] WARNING : CPU : 22 PID : 49 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1689.953565 ] WARNING : CPU : 0 PID : 51 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1691.740234 ] WARNING : CPU : 13 PID : 40 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1691.861911 ] WARNING : CPU : 1 PID : 52 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 [ 1791.554552 ] WARNING : CPU : 11 PID : 38 at managers / processor / pcache / victim . c : 447 victim_prepare_insert + 0x322 / 0x4b0 1 st run. MT+2GB. Victim allocation as predicted. Somehow I already forgot how the code is designed. I need to take a detailed reread. Along the testing, fixed a bug in eviction code: handle failed evict_line properly. If eviction mechanism failed, we need to clear what the algorithm part has done. This is also related to yesterday\u2019s big idea: always do proper cleanup. Many thanks go to pcache free checking, help me to find this bug. Less is more. I printed too much useless info when pcache_alloc or victim_alloc fail. I removed all the dump_pset from the failing path. It can give me a much more clean message to debug. Hmm, it is really weird. I dump all victims once alloc timeout. You can see that all victim are not Flushed, that means none of them can be evicted. Take a look at the stat. Hmm, I probabaly should not do this per-cpu counter?? ... [ 4751.460819 ] -- Start Dump Victim Cache -- [ 4751.518022 ] -- CPU19 [ word_count - pthr ][ pid = 46 , tgid = 32 ] -- [ 4751.587706 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800440 [ 4751.747872 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20011000 [ 4751.827955 ] pset : ffff88207f800440 set_idx : 17 nr_lru : 8 [ 4751.893478 ] [ 4751.911159 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f8003c0 [ 4752.071326 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff2000f000 [ 4752.428060 ] pset : ffff88207f8003c0 set_idx : 15 nr_lru : 8 [ 4752.630868 ] [ 4752.931441 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800540 [ 4753.370339 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20015000 [ 4753.450422 ] pset : ffff88207f800540 set_idx : 21 nr_lru : 8 [ 4753.515945 ] [ 4753.533627 ] victim : ffff88207ff710d8 index : 3 refcount : 3 nr_fill : 1 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207fbdff40 [ 4753.693792 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fffbf7fd000 [ 4753.773875 ] pset : ffff88207fbdff40 set_idx : 63485 nr_lru : 7 [ 4753.842518 ] [ 4753.860199 ] victim : ffff88207ff71120 index : 4 refcount : 3 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800500 [ 4754.020367 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20014000 [ 4754.100449 ] pset : ffff88207f800500 set_idx : 20 nr_lru : 8 [ 4754.165971 ] [ 4754.183653 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800480 [ 4754.343819 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30012000 [ 4754.423902 ] pset : ffff88207f800480 set_idx : 18 nr_lru : 8 [ 4754.489426 ] [ 4754.507106 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f8004c0 [ 4754.808718 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30013000 [ 4754.888802 ] pset : ffff88207f8004c0 set_idx : 19 nr_lru : 8 [ 4754.954325 ] [ 4754.972006 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800400 [ 4755.132172 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff20010000 [ 4755.212255 ] pset : ffff88207f800400 set_idx : 16 nr_lru : 8 [ 4755.277778 ] [ 4755.295458 ] -- End Dump Victim Cache -- ... [ 4757.948641 ] nr_pgfault : 313898 [ 4757.983067 ] nr_clflush : 488 [ 4758.016347 ] nr_pgfault_wp : 0 [ 4758.050669 ] nr_pgfault_wp_cow : 0 [ 4758.089151 ] nr_pgfault_wp_reuse : 0 [ 4758.129713 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 4758.188995 ] nr_pcache_fill_from_memory : 313833 [ 4758.242038 ] nr_pcache_fill_from_victim : 54 [ 4758.290919 ] nr_pcache_eviction_triggered : 243280263 [ 4758.349161 ] nr_pcache_eviction_eagain : 243279763 [ 4758.404283 ] nr_pcache_eviction_succeed : 488 [ 4758.454207 ] nr_victim_eviction : 426 [ 4758.495807 ] nr_victim_prepare_insert : 500 [ 4758.543649 ] nr_victim_finish_insert : 488 [ 4758.590451 ] nr_victim_flush_submitted : 488 [ 4758.639333 ] nr_victim_flush_finished : 488 I counted it wrong. Below is the log. Since nr_victim_flushd_run * 8 = nr_victim_flush_finished , it basically means for every run, victim_flushd needs to flush all 8 victims, which implies eviction rate is much higher than the flushd running rate. nr_pcache_fill_from_victim: 21 , which means there are some succeed refills, but I don\u2019t know how it can improve performance. [ 475.468489 ] CPU4 PID : 54 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f800000 , pset_idx : 0 , nr_lru : 7 [ 475.602752 ] CPU3 PID : 53 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f900a00 , pset_idx : 16424 , nr_lru : 7 [ 476.029145 ] CPU5 PID : 55 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fbdff40 , pset_idx : 63485 , nr_lru : 7 [ 476.169542 ] CPU9 PID : 36 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207f900000 , pset_idx : 16384 , nr_lru : 7 [ 477.360322 ] CPU1 PID : 52 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fbfff80 , pset_idx : 65534 , nr_lru : 7 [ 479.206291 ] CPU18 PID : 45 Abort victim alloc ( 10010 ms ) nr_usable_victims : 8 req from pset : ffff88207fb00000 , pset_idx : 49152 , nr_lru : 7 [ 475.743150 ] -- Start Dump Victim Cache -- [ 475.800350 ] -- CPU4 [ word_count - pthr ][ pid = 54 , tgid = 32 ] -- [ 475.868989 ] victim : ffff88207ff71000 index : 0 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a80 [ 476.309940 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002a000 [ 476.390020 ] pset : ffff88207f800a80 set_idx : 42 nr_lru : 8 [ 476.455538 ] [ 476.473218 ] victim : ffff88207ff71048 index : 1 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800bc0 [ 476.633376 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff4002f000 [ 476.713453 ] pset : ffff88207f800bc0 set_idx : 47 nr_lru : 8 [ 476.778972 ] [ 476.796652 ] victim : ffff88207ff71090 index : 2 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b80 [ 476.956809 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002e000 [ 477.036889 ] pset : ffff88207f800b80 set_idx : 46 nr_lru : 8 [ 477.102406 ] [ 477.120086 ] victim : ffff88207ff710d8 index : 3 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a00 [ 477.280245 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30028000 [ 477.500721 ] pset : ffff88207f800a00 set_idx : 40 nr_lru : 8 [ 477.566239 ] [ 477.583918 ] victim : ffff88207ff71120 index : 4 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b40 [ 477.744077 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002d000 [ 477.824155 ] pset : ffff88207f800b40 set_idx : 45 nr_lru : 8 [ 477.889673 ] [ 477.907353 ] victim : ffff88207ff71168 index : 5 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800b00 [ 478.067511 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002c000 [ 478.147590 ] pset : ffff88207f800b00 set_idx : 44 nr_lru : 8 [ 478.213109 ] [ 478.230788 ] victim : ffff88207ff711b0 index : 6 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800a40 [ 478.390946 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff30029000 [ 478.471024 ] pset : ffff88207f800a40 set_idx : 41 nr_lru : 8 [ 478.536542 ] [ 478.554222 ] victim : ffff88207ff711f8 index : 7 refcount : 2 nr_fill : 0 locked : 0 flags :( allocated | usable | hasdata ) pcm : ( null ) pset : ffff88207f800ac0 [ 478.714380 ] hit [ 0 ] owner : [ word_count - pthr ][ 32 ] addr : 0x7fff3002b000 [ 478.794458 ] pset : ffff88207f800ac0 set_idx : 43 nr_lru : 8 [ 478.859977 ] [ 478.877657 ] -- End Dump Victim Cache -- [ 480.324070 ] nr_pgfault : 372353 [ 480.358494 ] nr_clflush : 336 [ 480.391774 ] nr_pgfault_wp : 0 [ 480.426093 ] nr_pgfault_wp_cow : 0 [ 480.464573 ] nr_pgfault_wp_reuse : 0 [ 480.505132 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 480.564410 ] nr_pcache_fill_from_memory : 372326 [ 480.617450 ] nr_pcache_fill_from_victim : 21 [ 480.666330 ] nr_pcache_eviction_triggered : 178320088 [ 480.724569 ] nr_pcache_eviction_eagain : 178319746 [ 480.779687 ] nr_pcache_eviction_succeed : 336 [ 480.829606 ] nr_victim_eviction_triggered : 20589049 [ 480.886805 ] nr_victim_eviction_eagain : 20588741 [ 480.940885 ] nr_victim_eviction_succeed : 308 [ 480.990804 ] nr_victim_prepare_insert : 342 [ 481.038643 ] nr_victim_finish_insert : 336 [ 481.085442 ] nr_victim_flush_submitted : 336 [ 481.134321 ] nr_victim_flush_finished : 336 [ 481.182161 ] nr_victim_flushd_run : 42","title":"Debug victim cache"},{"location":"lego/log/log-03-2018/#0310-sun","text":"","title":"03/10 Sun"},{"location":"lego/log/log-03-2018/#fix-bug-from-__unhash_procees","text":"[Summary]: a bug cause by laziness. When fork happens, the new thread is added into parent\u2019s thread_group list. However, we forgot to remove it when the new thread exit. Thus, the field in parent\u2019s thread_group will point to a freed page. To make it worse, the freed page got allocated again. In our case, the page was used by pgtable. So, when the parent tries to use that field, it simply corrupts pgtable. This bug is fixed by this commit: 64d43fc. Got something going on. Huh. Anyway, pick up what left last night. 8 th run, [ 426.595911] SYSC_mmap(cpu5): ret_addr:0x7ffefbeac000 pte page got allocated [ 426.653216] pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90 [ 426.734334] __pte_alloc(): for addr: 0x7ffefc6acd90 pte_index: ac [ 426.807132] pte is none index 0x38 line 574 from_addr 0x7ffefc6acd90 [ 427.304148] pte is none index 0x38 line 576 from_addr 0x7ffefc6acd90 this addr seems fine [ 427.382251] pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78 [ 427.462329] pte is none index 0x38 line 574 from_addr 0x7ffefc6abe78 [ 427.644439] pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78 Something happen in between corrupted pgtable [ 427.722547] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000) [ 427.858779] line: 567 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0 [ 427.938858] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000) [ 428.075095] line: 574 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0 9 th run, found actually it created another thread. And it exit. And it corrupted aftet the pid33 exit. Bang, it should be something wrong in exit(). wuklab13 0311 - 4 [ 813.127325 ] CPU6 pid : 33 pmd is none index 0x1e3 line 586 from_addr 0x4b0db0 [ 813.214683 ] CPU5 pid : 32 pmd is none index 0x1e3 line 593 from_addr 0x6f4768 [ 813.302042 ] CPU6 pid : 33 pmd is none index 0x1e3 line 593 from_addr 0x4b0db0 [ 813.397836 ] CPU5 pid : 32 pmd is none index 0x1e3 line 595 from_addr 0x6f4768 [ 813.593364 ] CPU6 pid : 33 pmd is none index 0x1e3 line 595 from_addr 0x4b0db0 [ 813.678751 ] do_exit () pid : 33 , tgid : 32 code : 0x0 [ 814.474321 ] CPU5 pid : 32 pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90 [ 814.567918 ] CPU5 pid : 32 pmd is none index 0x1e3 line 575 from_addr 0x7ffefc6acd90 [ 814.661516 ] CPU5 pid : 32 pmd is none index 0x1e3 line 583 from_addr 0x7ffefc6acd90 [ 814.755115 ] CPU5 pid : 32 pmd is none index 0x1e3 line 586 from_addr 0x7ffefc6acd90 [ 814.848714 ] __pte_alloc () : for addr : 0x7ffefc6acd90 pte_index : ac [ 814.921511 ] CPU5 pid : 32 pte is none index 0x38 line 593 from_addr 0x7ffefc6acd90 [ 815.125249 ] CPU5 pid : 32 pte is none index 0x38 line 595 from_addr 0x7ffefc6acd90 [ 815.215833 ] After pcache_handle_fault [ 815.259511 ] CPU5 pid : 32 pte is none index 0x38 line 726 from_addr 0x7ffefc6acd90 [ 815.352071 ] CPU5 pid : 32 pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78 [ 815.444627 ] CPU5 pid : 32 pte is none index 0x38 line 575 from_addr 0x7ffefc6abe78 [ 815.537186 ] CPU5 pid : 32 pte is none index 0x38 line 583 from_addr 0x7ffefc6abe78 [ 815.629744 ] CPU5 pid : 32 pte is none index 0x38 line 586 from_addr 0x7ffefc6abe78 [ 815.722303 ] CPU5 pid : 32 pte is none index 0x38 line 593 from_addr 0x7ffefc6abe78 [ 815.916890 ] CPU5 pid : 32 pte is none index 0x38 line 595 from_addr 0x7ffefc6abe78 [ 816.007471 ] After pcache_handle_fault [ 816.051151 ] CPU5 pid : 32 pte is none index 0x38 line 726 from_addr 0x7ffefc6abe78 [ 816.143715 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 816.279946 ] do_exit () pid : 34 , tgid : 32 code : 0x0 [ 816.331945 ] CPU5 pid : 32 line : 567 from_addr : 0x6fc6d8 pte . cont : 0xffff88207e8c31c0 10 th run, actually 2 threads are created. When pid 33 exit, everything stays okay. But after fork of pid 34. It went wrong: wuklab13 0311 - 8 [ 609.490893 ] do_exit () pid : 33 , tgid : 32 code : 0x0 [ 609.542894 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 401 from_addr 0x0 [ 609.640661 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 443 from_addr 0x0 [ 609.738429 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 465 from_addr 0x0 [ 609.836197 ] exit_mm : 378 mm -> users 2 mm -> count 1 [ 609.891320 ] exit_mm : 380 mm -> users 1 mm -> count 1 [ 609.946445 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 468 from_addr 0x0 [ 610.044212 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 471 from_addr 0x0 [ 610.141979 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 474 from_addr 0x0 [ 610.239747 ] SYSC_mmap ( cpu5 ) : ret_addr : 0x7ffefbeac000 [ 610.299031 ] CPU6 pid : 33 caller : do_exit pmd is none index 0x1e3 line 482 from_addr 0x0 [ 610.396798 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 568 from_addr 0x7ffefc6acd90 [ 610.518489 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 576 from_addr 0x7ffefc6acd90 [ 610.640178 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 584 from_addr 0x7ffefc6acd90 [ 610.761866 ] CPU5 pid : 32 caller : pcache_handle_fault pmd is none index 0x1e3 line 587 from_addr 0x7ffefc6acd90 [ 610.883557 ] __pte_alloc () : for addr : 0x7ffefc6acd90 pte_index : ac [ 610.956362 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 594 from_addr 0x7ffefc6acd90 [ 611.179051 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 596 from_addr 0x7ffefc6acd90 [ 611.297723 ] After pcache_handle_fault [ 611.341406 ] CPU5 pid : 32 caller : do_page_fault pte is none index 0x38 line 726 from_addr 0x7ffefc6acd90 [ 611.455816 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 568 from_addr 0x7ffefc6abe78 [ 611.576464 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78 [ 611.697113 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 584 from_addr 0x7ffefc6abe78 [ 611.817762 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 587 from_addr 0x7ffefc6abe78 [ 611.938412 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 594 from_addr 0x7ffefc6abe78 [ 612.161103 ] CPU5 pid : 32 caller : pcache_handle_fault pte is none index 0x38 line 596 from_addr 0x7ffefc6abe78 [ 612.279778 ] After pcache_handle_fault [ 612.323461 ] CPU5 pid : 32 caller : do_page_fault pte is none index 0x38 line 726 from_addr 0x7ffefc6abe78 [ 612.437875 ] do_fork : current : 32 new : 34 [ 612.484676 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 612.620924 ] do_exit () pid : 34 , tgid : 32 code : 0x0 [ 612.672928 ] CPU5 pid : 32 caller : pcache_handle_faultline : 568 from_addr : 0x6fc6d8 pte . cont : 0xffff88207e8c31c0 [ 612.793577 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 612.929828 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 613.066078 ] CPU7 pid : 34 caller : do_exitline : 401 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 11 th run, found it orignate from copy_process() . Good. [ 869.591729 ] CPU5 pid : 32 caller : do_fork pte is none index 0x38 line 886 from_addr 0x0 [ 869.688449 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 869.824681 ] CPU5 pid : 32 caller : do_fork line : 894 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 12 th run, found the opeation that corrupt pgtable: [ 1099.974106 ] CPU5 pid : 32 caller : copy_process pte is none index 0x38 line 897 from_addr 0x0 [ 1100.076032 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 1100.212282 ] CPU5 pid : 32 caller : copy_process line : 902 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 896 if ( current -> tgid == 32 ) 897 jasmine ( 0 , __LINE__ , __func__ ); 898 899 list_add_tail ( & p -> thread_group , 900 & p -> group_leader -> thread_group ); 901 if ( current -> tgid == 32 ) 902 jasmine ( 0 , __LINE__ , __func__ ); 13 th run, interesting, the list_add_tail write to the pgtable. pte.cont = 0xffff88207e8c31c0, p->thread_group: 0xffff88207e8c31c0 . [ 916.269942 ] CPU5 pid : 32 caller : copy_process pte is none index 0x38 line 898 from_addr 0x0 [ 916.371863 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e190000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_grou : ffff88107e1901c0 [ 916.523705 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 916.659947 ] CPU5 pid : 32 caller : copy_process line : 906 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 [ 916.769148 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e190000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_grou : ffff88107e1901c0 14 th run, got an log like this. Clearly, the pte is written the value of p->thread_group. But the leader\u2019s pointer is correct. Weird, going to dig deeper. p: ffff88207e8c3000 p->group_leader: ffff88107e189000(32) p->thread_group: ffff88207e8c31c0 leader->thread_group: ffff88107e1891c0 pte page: ffff88207e8b5000 pte: ffff88207e8b51c0 pte.cont: ffff88207e8c31c0 15 th run, found the bug. wuklab13 0311 - 15 [ 1474.477687 ] dup_task_struct () : current : 32 new : ffff88207e8b5000 .. while pid 33 exit so the ffff88207e8b5000 is freed but allocated again by pte_alloc [ 1481.420200 ] __pte_alloc () : CPU5 for addr : 0x7ffefc6acd90 pte_index : ac new pte page : ffff88207e8b5000 However , we forgot to remove it from group_leader ' s thread_group [ 1485.895938 ] p : ffff88207e8c3000 p -> group_leader : ffff88107e19b000 ( 32 ) p -> thread_group : ffff88207e8c31c0 leader -> thread_group : ffff88107e19b1c0 [ 1486.047784 ] tg -> next : ffff88207e8c31c8 tg -> prev : ffff88207e8c31c0 leader -> tg -> next ffff88107e19b1c8 leader -> tg -> prev ffff88107e19b1c0 [ 1486.191311 ] next ffff88107e19b1c0 prev ffff88207e8b51c0 next ffff88107e19b1c0 [ 1486.276594 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 61 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.401399 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 65 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.526203 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 69 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.651010 ] CPU5 pid : 32 caller : __list_add pte is none index 0x38 line 73 from_addr 0x0 page : 0xffff88207e8b5000 [ 1486.775814 ] pte : ffff88207e8b51c0 pfn : 0x8207e8c3 flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 1486.912060 ] CPU5 pid : 32 caller : __list_add line : 77 from_addr : 0x0 pte . cont : 0xffff88207e8c31c0 16 th run, damn, after patching __unhash_process() , it finally works. Going to workout, see you tonight.","title":"Fix bug from __unhash_procees()"},{"location":"lego/log/log-03-2018/#victim-report-error","text":"17 th run. The phoenix program has bug itself, it is not able to run with 4GB dataset. So try it with 2GB dataset. Uuh, the log is too long. __put_vicim report a victim that has wrong flags. Going to disable the evict log and try again. 18 th run. Happen to run seq with 100MB\u2026 It actually half finished. But the printf of phoenix has funny chars. I guess memory is corrupted. The log shows it is ib_mad_completion. [ 2244.018806 ] Processor : Processor manager is running . [ 2246.394568 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 2246.447719 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 2246.507003 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq ] --- [ 2246.618289 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt ] --- [ 2258.805633 ] STDOUT : --- [ Word - Count - Seq : Computation Completed 12.46633 sec ] --- [ 2258.923180 ] SYSC_close () : [ 4 ] -> [ / proc / meminfo ] [ 2258.995743 ] STDOUT : --- [ Use len is 123748 [ 2263.484774 ] STDOUT : --- [ THE : 1115050 ] --- [ 2263.666785 ] STDOUT : --- [ OF : 615296 ] --- [ 2266.103660 ] STDOUT : --- [ AND : 545303 ( a lot funny chars , deleted .) ] --- [ 2267.016837 ] Code : [ 2267.038680 ] STDOUT : --- [ TO : 475179 +> \u00d5\u00fe\u00da\u00e9\u00d8 ^ G \u00a7 < 87 > k < 80 > z ^ T < 86 > ruJ \u00b7\u00bf\u00bb < 9 e > \u00e9\u00de\u00ed\u00d1 r\u00dc\u00d5 ^ W\u00e5 ^ W *^ _ {( \u00ca ? R\u00f9a\u00e9\u00f6 \u00f7 8 \u00ed < 91 > \u00dc\u00e8 < 8f > \u00f2\u00bf i ^? \u00e8 4 < 94 > \u00d7\u00b2\u00c9\u00b5 ^ V \u00bf\u00ab\u00eb P ] \u00ed\u00ef h ^ G\u00ca\u00eb < 98 >^ T \u00d7 Qp\u00b9O \u00ae\u00ef ^ \\\u00da ^?^ A\u00ed < 91 > \u00d9 v\u00ddBy ^ _\u00e9iwP ^ r < 97 > \u00eb\u00f9\u00ef\u00df ] \u00a3\u00df\u00ad < 98 >< 81 > \u00f8 < 85 > \u00ce Ey ^ Y\u00e5 ^? V\u00f9\u00ba ^ Y\u00de\u00f5\u00cb ] r5\u00c9\u00f0 ^^ ' < 92 > \u00c9 ] ^ ] P ^ \u00c7 i \u00bb z : \u00d4 ^ S \u00ae e < 8 a >+ \\\u00e9 < 8 a > \u00ae\u00b1\u00e0 E\u00d5\u00ce , \u00f0\u00d2\u00e2 3 \u00c1 _ ^ P_ ^ H ^ [ | \u00b8\u00ae\u00e1 s\u00edF \u00bf m < 95 >< 9 d >?< 82 > \u00f2 : \u00be\u00de\u00f5 3 \u00ca\u00d7 T\u00fc \u00ae ] --- [ 2263.339165 ] BUG : unable to handle kernel paging request at ffffffffffff8100 [ 2263.422369 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 2263.570058 ] PGD 1140067 PUD 1142067 PMD 0 [ 2263.618942 ] Oops : 0010 [ # 1 ] SMP PROCESSOR [ 2264.705811 ] CPU : 0 PID : 27 Comm : ib_mad_completi 4.0.0 - lego - ys + # 408 [ 2264.781736 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 2264.873262 ] RSP : 0000 : ffff88107efabc90 EFLAGS : 00010046 [ 2264.936705 ] RAX : 5636000000000098 RBX : db5affffffffffff RCX : 0000000000000001 [ 2265.021990 ] RDX : ffff88107efabd38 RSI : 0000000000000000 RDI : 4460ff ffffff8114 [ 2265.107277 ] RBP : ffff88107efabce0 R08 : 000000000000001f R09 : ffff88107efa43c0 [ 2265.192561 ] R10 : ffff88107efabe68 R11 : 0000000000000001 R12 : ac02000004ecbdbd [ 2265.277847 ] R13 : 0000000000000000 R14 : ffff88107efa4228 R15 : ffff88107e1ab000 [ 2265.363133 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 2265.459858 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 2265.528503 ] CR2 : ffffffffffff8100 CR3 : 000000000113 d000 CR4 : 00000000000406 b0 [ 2265.613789 ] Stack : [ 2265.637710 ] ffffffff810151a7 00000000000000 82 ffff88107fc04980 0000000000000000 [ 2265.725075 ] ffff88107efabcc8 ffff88107fc04980 0000000000000000 0000000000000000 [ 2265.812441 ] ffff88107efa4228 ffff88107e1ab000 ffff88107efabcf8 ffffffff81016e17 [ 2265.899806 ] 000000007 efabe20 ffff88107efabd20 ffffffff810066f4 ffffffff81072f20 [ 2265.987172 ] ffff88107fc05e00 ffff88107efa4000 ffff88107efabe08 ffffffff8100e4aa [ 2266.074538 ] Call Trace : [ 2266.206626 ] < TSK > [ 2266.229507 ] [ < ffffffff810151a7 > ] ? update_wall_time + 0x47 / 0x6b0 [ 2266.299192 ] [ < ffffffff81016e17 > ] tick_handle_periodic + 0x67 / 0x70 [ 2266.369916 ] [ < ffffffff810066f4 > ] apic_timer_interrupt + 0x54 / 0x90 [ 2266.440641 ] [ < ffffffff8100e4aa > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 2266.516565 ] [ < ffffffff810663b8 > ] ? __schedule + 0xf8 / 0x1e0 [ 2266.580010 ] [ < ffffffff810664b3 > ] schedule + 0x13 / 0x30 [ 2266.638254 ] [ < ffffffff81058c97 > ] ib_mad_completion_handler + 0x2b7 / 0x860 [ 2266.716258 ] [ < ffffffff810589e0 > ] ? ib_mad_send_done_handler . isra .22 + 0x1d0 / 0x1d0 [ 2266.803624 ] [ < ffffffff81020376 > ] kthread + 0xf6 / 0x110 [ 2266.861867 ] [ < ffffffff81020280 > ] ? __kthread_parkme + 0x70 / 0x70 [ 2266.930512 ] [ < ffffffff8100e732 > ] ret_from_fork + 0x22 / 0x30 [ 2266.993955 ] < EOT > 19 th , try seq+100MB again. Well succeed. I guess I start S too later. So that thread has issues. We run 12.3 sec, while linux run 9.7 sec. 20 th , try seq+4GB data. Linux runs 314.4 sec . Lego runs 403 sec . But Lego has some clflush error messages. I don\u2019t know why actually. [ 794.604628 ] Processor : Processor manager is running . [ 796.884884 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 796.938032 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 796.997312 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq ] --- [ 797.108596 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_4GB . txt ] --- [ 980.640200 ] __clflush_one () : EFAULT : bad address [ 980.692315 ] __clflush_one () : EFAULT : bad address [ 980.746397 ] __clflush_one () : EFAULT : bad address [ 980.800478 ] __clflush_one () : EFAULT : bad address [ 980.854559 ] __clflush_one () : EFAULT : bad address [ 980.908642 ] __clflush_one () : EFAULT : bad address [ 980.962723 ] __clflush_one () : EFAULT : bad address [ 981.016804 ] __clflush_one () : EFAULT : bad address [ 981.070886 ] __clflush_one () : EFAULT : bad address [ 981.124968 ] __clflush_one () : EFAULT : bad address [ 981.179048 ] __clflush_one () : EFAULT : bad address [ 981.233129 ] __clflush_one () : EFAULT : bad address [ 981.287211 ] __clflush_one () : EFAULT : bad address [ 981.341293 ] __clflush_one () : EFAULT : bad address [ 981.395375 ] __clflush_one () : EFAULT : bad address [ 981.449456 ] __clflush_one () : EFAULT : bad address [ 981.503538 ] __clflush_one () : EFAULT : bad address [ 981.557619 ] __clflush_one () : EFAULT : bad address [ 981.611702 ] __clflush_one () : EFAULT : bad address [ 981.665782 ] __clflush_one () : EFAULT : bad address [ 981.719863 ] __clflush_one () : EFAULT : bad address [ 981.773945 ] __clflush_one () : EFAULT : bad address [ 981.828026 ] __clflush_one () : EFAULT : bad address [ 981.882108 ] __clflush_one () : EFAULT : bad address [ 981.936188 ] __clflush_one () : EFAULT : bad address [ 981.990271 ] __clflush_one () : EFAULT : bad address [ 982.044352 ] __clflush_one () : EFAULT : bad address [ 982.098434 ] __clflush_one () : EFAULT : bad address [ 982.152515 ] __clflush_one () : EFAULT : bad address [ 982.206596 ] __clflush_one () : EFAULT : bad address [ 1200.759741 ] STDOUT : --- [ Word - Count - Seq : Computation Completed 403.519401 sec ] --- ... [ 1200.989480 ] STDOUT : --- [ THE : 44602000 ... [ 1201.755779 ] do_group_exit () pid : 32 , tgid : 32 exit_code : 0x0 [ 1201.819136 ] do_exit () pid : 32 , tgid : 32 code : 0x0 [ 1201.872451 ] nr_pgfault : 1049525 [ 1201.908579 ] nr_pgfault_wp : 0 [ 1201.942899 ] nr_pgfault_wp_cow : 0 [ 1201.981380 ] nr_pgfault_wp_reuse : 0 [ 1202.021941 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 1202.081223 ] nr_pcache_fill_from_memory : 1045393 [ 1202.135304 ] nr_pcache_fill_from_victim : 4132 [ 1202.186265 ] nr_pcache_eviction : 525230 [ 1202.230987 ] nr_victim_eviction : 521090 21th run. Do not have time and energy to debug the clflush issue. I just want to run MT+2GB again. Well victim has issues! Some warning are triggered. Log is wuklab13:~/ys/0311-22 . Continue tomorrow! Good night world. (Such a lonly phd.)","title":"victim report error"},{"location":"lego/log/log-03-2018/#0310-sat","text":"Running python hello world. Tried to make kmalloc use buddy directly.","title":"03/10 Sat"},{"location":"lego/log/log-03-2018/#put_pcache-in-pcache_zap_pte","text":"So this time, python keep running for a long time. But P crashed when the first time eviction was triggered. Below is log from S side, those libraries do not exist, so these log are fine: S: [Mar10 10:39] handle_access_request /etc/ld.so.preload 4, -2 [Mar10 10:44] local_file_open : Cannot open required file [/usr/lib64/python2.7/site.so]. [ +0.352839] local_file_open : Cannot open required file [/usr/lib64/python2.7/sitemodule.so]. [ +22.254465] local_file_open : Cannot open required file [/usr/lib64/python2.7/os.so]. [ +0.350759] local_file_open : Cannot open required file [/usr/lib64/python2.7/osmodule.so]. [Mar10 10:45] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpath.so]. [ +0.358045] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpathmodule.so]. [ +13.421033] local_file_open : Cannot open required file [/usr/lib64/python2.7/stat.so]. [ +0.352838] local_file_open : Cannot open required file [/usr/lib64/python2.7/statmodule.so]. [Mar10 10:46] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpath.so]. [ +0.360126] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpathmodule.so]. [ +11.582165] local_file_open : Cannot open required file [/usr/lib64/python2.7/warnings.so]. [ +0.357003] local_file_open : Cannot open required file [/usr/lib64/python2.7/warningsmodule.so]. [ +11.989828] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecache.so]. [ +0.358043] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecachemodule.so]. [Mar10 10:47] local_file_open : Cannot open required file [/usr/lib64/python2.7/types.so]. [ +0.353879] local_file_open : Cannot open required file [/usr/lib64/python2.7/typesmodule.so]. Weird P\u2019s bug, seems like the pcm returned by evict_find_line has issue. Well, I\u2019m trying to debug what is going with this set. wuklab13 0310 - 2 [ 1046.880649 ] SYSC_read () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff6e117e0 [ 1046.959692 ] fd : 8 , buf : 00007ff ff7ffb000 , count : 4096 [ 1048.726624 ] pcache_evict_line () : pset : ffff88207f9ffec0 , for uva : 0x7ffff7ffb000 [ 1048.813053 ] ------------ [ cut here ] ------------ [ 1048.868174 ] BUG : failure at . / include / processor / pcache . h : 284 / pcache_meta_to_pcache_set () ! [ 1048.965937 ] Kernel Panic - not syncing : BUG ! [ 1049.016898 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 347 [ 1049.083460 ] Stack : [ 1049.107380 ] ffff88107e18fca8 ffffffff81026f1c 000000000000000 8 ffff88107e18fcb8 [ 1049.194743 ] ffff88107e18fc70 0000000021475542 0000000000000000 0000000000000000 [ 1049.282107 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.369468 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.456832 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 1049.544193 ] Call Trace : [ 1049.573315 ] < TSK > [ 1049.596195 ] [ < ffffffff81026f28 > ] panic + 0xc2 / 0xeb [ 1049.651318 ] [ < ffffffff8101c3fc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1049.715799 ] [ < ffffffff81019a65 > ] ? scheduler_tick + 0x55 / 0x60 [ 1049.782360 ] [ < ffffffff81017035 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1049.855163 ] [ < ffffffff81006764 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1049.927966 ] [ < ffffffff8101c3fc > ] ? task_tick_rt + 0x2c / 0xd0 [ 1049.992447 ] [ < ffffffff81019a65 > ] ? scheduler_tick + 0x55 / 0x60 [ 1050.059009 ] [ < ffffffff81017035 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1050.131812 ] [ < ffffffff8103c41a > ] ? put_dec + 0x1a / 0x80 [ 1050.191093 ] [ < ffffffff81006764 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1050.263895 ] [ < ffffffff8100e56a > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1050.341897 ] [ < ffffffff81012ded > ] ? printk + 0x11d / 0x1b0 [ 1050.402219 ] [ < ffffffff810340c5 > ] dump_pcache_meta + 0xc5 / 0xd0 [ 1050.468782 ] [ < ffffffff81034588 > ] pcache_evict_line + 0x158 / 0x220 [ 1050.538463 ] [ < ffffffff81030f5e > ] pcache_alloc + 0x22e / 0x2f0 [ 1050.602945 ] [ < ffffffff8103015a > ] common_do_fill_page + 0x2a / 0x430 [ 1050.673668 ] [ < ffffffff8102fb20 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1050.744389 ] [ < ffffffff81030702 > ] pcache_handle_fault + 0x1a2 / 0x6c0 [ 1050.816152 ] [ < ffffffff810103d2 > ] do_page_fault + 0xa2 / 0x1a0 [ 1050.880634 ] [ < ffffffff8100db9f > ] page_fault + 0x1f / 0x30 [ 1050.940955 ] [ < ffffffff8103bb82 > ] ? copy_user_enhanced_fast_string + 0x2 / 0x10 [ 1051.023118 ] [ < ffffffff81038423 > ] ? normal_p2m_read + 0x233 / 0x330 [ 1051.092800 ] [ < ffffffff810363ce > ] sys_read + 0x9e / 0x160 [ 1051.152081 ] [ < ffffffff810268d0 > ] ? strace_enter_default + 0x30 / 0x40 [ 1051.224884 ] [ < ffffffff8100e935 > ] do_syscall_64 + 0x45 / 0xd0 [ 1051.288326 ] [ < ffffffff8100d82c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 Interesting, added several debug messages. The bug is I forgot to put_pcache when a rmap was zapped. One rmap counts one refcount (effectively one process), thus when a rmap was zapped, we should decrease the refcount. I found I\u2019ve already done so for pcache_remove_rmap , and pcache_move_pte . But damn, forgot this one. I remember this code was written before fork+pcache. So.. I don\u2019t have a big picture at that time. Multithreaded system plus background reclaim really a very rigours design usage of refcount and lock . [ 1418.038411] CPU5 PID32 sys_read+0x0/0xa0 [ 1418.085227] pcache_evict_line(): pset: ffff88207f9ffec0, for uva: 0x7ffff7ffb000 [ 1418.173617] pset:ffff88207f9ffec0 set_idx: 32763 nr_lru:8 [ 1418.238105] pcache:ffff8801801ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880107ffb000 [ 1418.351476] pcache:ffff8801805ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880117ffb000 [ 1418.464847] pcache:ffff8801809ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880127ffb000 [ 1418.578220] pcache:ffff880180dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880137ffb000 [ 1418.691591] pcache:ffff8801811ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880147ffb000 [ 1418.804963] pcache:ffff8801815ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880157ffb000 [ 1418.918334] pcache:ffff8801819ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880167ffb000 [ 1419.031706] pcache:ffff880181dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880177ffb000 [ 1419.145077] After dump pset [ 1419.176280] pcache:ffff8801801ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880107ffb000 [ 1419.289652] pcache dumped because: evict_find_line_lru [ 1419.351018] pcache:ffff8801805ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880117ffb000 [ 1419.464389] pcache dumped because: evict_find_line_lru [ 1419.525757] pcache:ffff8801809ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880127ffb000 [ 1419.639127] pcache dumped because: evict_find_line_lru [ 1419.700494] pcache:ffff880180dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880137ffb000 [ 1419.813865] pcache dumped because: evict_find_line_lru [ 1419.875231] pcache:ffff8801811ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880147ffb000 [ 1419.988604] pcache dumped because: evict_find_line_lru [ 1420.049969] pcache:ffff8801815ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880157ffb000 [ 1420.163341] pcache dumped because: evict_find_line_lru [ 1420.224708] pcache:ffff8801819ffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880167ffb000 [ 1420.338079] pcache dumped because: evict_find_line_lru [ 1420.399445] pcache:ffff880181dffec0 mapcount:0 refcount:1 flags:(allocated|usable) kva: ffff880177ffb000 [ 1420.512817] pcache dumped because: evict_find_line_lru [ 1420.574183] evict_find_line_lru(): pcm: ffff88207f9ffea8 [ 1420.637631] ------------[ cut here ]------------ [ 1420.692756] BUG: failure at ./include/processor/pcache.h:340/pcache_meta_to_kva()! [ 1420.783245] Kernel Panic - not syncing: BUG! [ 1420.834210] CPU: 5 PID: 32 Comm: python 4.0.0-lego-ys+ #349 [ 1420.900777] Stack:","title":"put_pcache in pcache_zap_pte"},{"location":"lego/log/log-03-2018/#python-hello-world-run-to-end","text":"Glad to say, python hello world finished, even with some missed syscalls. Especially the stdin stuff, so the string is actually not printed out. Log is wuklab13:~/ys/0310-4 [ 3149.540308 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3149.588144 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3149.635982 ] CPU5 PID32 sys_write + 0x0 / 0xa0 [ 3149.683818 ] STDOUT : --- [ >>> ] --- [ 3149.726456 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff6d9aeb0 flags : 0x150 [ 3149.926247 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff6d9aeb0 flags : 0x150 ret : 0 ( OKAY ) [ 3150.033464 ] CPU5 PID32 sys_newfstat + 0x0 / 0x10 [ 3150.084420 ] CPU5 PID32 sys_ioctl + 0x0 / 0x10 [ 3150.132256 ] strace__mmap cpu5 addr = 0x0 , len = 0x1000 , prot ( 0x3 ) = PROT_READ | PROT_WRITE , flags ( 0x22 ) = MAP_PRIVATE | MAP_ANONYMOUS , fd = 18446744073709551615 ( ), off = 0x0 [ 3150.301772 ] CPU5 PID32 sys_read + 0x0 / 0xa0 [ 3150.348562 ] ------------ [ cut here ] ------------ [ 3150.403679 ] WARNING : CPU : 5 PID : 32 at managers / processor / fs / stdio . c : 24 stdio_file_read + 0x30 / 0x50 [ 3150.509751 ] Process wants STDIN ! [ 3150.546149 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 352 [ 3150.612705 ] Stack : [ 3150.636624 ] ffff88107e18fe90 ffffffff81012b15 ffffffff811464e0 00007ff ff7ffb000 [ 3150.723977 ] 0000000000000400 00007ff ff70e5640 ffff88107e18fef0 ffffffff81012bd2 [ 3150.811331 ] ffffffff81079d6b ffff881000000018 ffff88107e18ff00 ffff88107e18fec0 [ 3150.898687 ] 0000000000000020 ffffffff810346b0 0000000000000022 ffffffff811464f0 [ 3150.986040 ] 00007ff ff7fdf740 0000000000000000 ffff88107e18ff00 ffffffff81035ac0 [ 3151.073394 ] Call Trace : [ 3151.102514 ] < TSK > [ 3151.125392 ] [ < ffffffff81012b21 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 3151.194028 ] [ < ffffffff81012bd2 > ] warn_slowpath_fmt + 0x42 / 0x50 [ 3151.261623 ] [ < ffffffff810346b0 > ] ? sweep_pset_lru + 0x220 / 0x220 [ 3151.330259 ] [ < ffffffff81035ac0 > ] stdio_file_read + 0x30 / 0x50 [ 3151.395775 ] [ < ffffffff810346e3 > ] sys_read + 0x33 / 0xa0 [ 3151.454010 ] [ < ffffffff8100e875 > ] do_syscall_64 + 0x45 / 0xd0 [ 3151.517446 ] [ < ffffffff8100d76c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 3151.593362 ] < EOT > [ 3151.616240 ] --- [ end trace 0000000000000000 ] --- [ 3151.671360 ] CPU5 PID32 sys_write + 0x0 / 0xa0 [ 3151.719194 ] STDOUT : --- [ ] --- [ 3151.759756 ] CPU5 PID32 sys_close + 0x0 / 0x140 [ 3151.808628 ] SYSC_close () : [ 3 ] -> [ / root / ys / py_hello . py ] [ 3151.871028 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a79380 flags : 0x150 [ 3152.070817 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a79380 flags : 0x150 ret : 0 ( OKAY ) [ 3152.178033 ] CPU5 PID32 sys_rt_sigaction + 0x0 / 0xb0 [ 3152.234151 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a77f60 flags : 0x150 [ 3152.432941 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a77f60 flags : 0x150 ret : 0 ( OKAY ) [ 3152.540242 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff73ee794 flags : 0x150 [ 3152.739952 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff73ee794 flags : 0x150 ret : 0 ( OKAY ) [ 3152.847171 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff715b278 flags : 0x150 [ 3153.046958 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff715b278 flags : 0x150 ret : 0 ( OKAY ) [ 3153.154179 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff6de74f0 flags : 0x150 [ 3153.353965 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff6de74f0 flags : 0x150 ret : 0 ( OKAY ) [ 3153.461180 ] CPU5 PID32 sys_exit_group + 0x0 / 0x10","title":"python hello world run to end"},{"location":"lego/log/log-03-2018/#trying-phoenix-pthread-again","text":"4GB pcache, 1GB dataset. 1 th run with CONFIG_STRACE on, 1GB dataset finished, result is correct. 2 th run without CONFIG_STRACE, 1GB dataset stuck. Two weird things: open/close dev/cpu/online file too many times than a normal linux run IB stucked So next I\u2019m going to try add a lock to ibapi, see if it is ib internal deadlock issue. wuklab13 0310 - 7 [ 702.895936 ] Processor : Processor manager is running . [ 722.400159 ] STDOUT : --- [ envp [ 0 ] HOME =/ ] --- [ 722.453307 ] STDOUT : --- [ envp [ 1 ] TERM = linux ] --- [ 722.512589 ] STDOUT : --- [ argv [ 0 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread ] --- [ 722.628036 ] STDOUT : --- [ argv [ 1 ] / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt ] --- [ 722.759101 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 722.819406 ] STDOUT : --- [ ] --- [ 722.860139 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 722.940653 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.011483 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.084287 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.157090 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.229894 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.302698 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.375502 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.448306 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.521111 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.593914 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.666718 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.739522 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.812326 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 723.885130 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 766.701260 ] ibapi_send_reply () polling timeout ( 30010 ms ), caller : net_send_reply_timeout + 0x11b / 0x1ee [ 766.809538 ] net_send_reply_timeout () caller : __pcache_do_fill_page + 0x82 / 0x140 [ 766.895863 ] word_count - pthr [ 65 ] : segfault at 0x7fffb5eba000 ip 00000000004024 9 d sp 00007ff fb5e9ad80 error 6 [ 767.012348 ] CPU : 15 PID : 65 Comm : word_count - pthr 4.0.0 - lego - ys + # 359 [ 767.089312 ] RIP : 0033 : [ < 00000000004024 9 d > ] [ < 00000000004024 9 d > ] 0x40249d [ 767.170436 ] RSP : 002 b : 00007ff fb5e9ad80 EFLAGS : 00010216 [ 767.233879 ] RAX : 00007ff fb5eba000 RBX : 00000000000013 88 RCX : 000000000000004f [ 767.319164 ] RDX : 00007ff fe4ea92a4 RSI : 00007ff fe626fac9 RDI : 00007ff fe4ea92a4 [ 767.404449 ] RBP : 00000000007540e0 R08 : 0000000000000000 R09 : 0000000000014f a0 [ 767.489733 ] R10 : 0000000000427f b0 R11 : 0000000000000202 R12 : 0000000000012 b12 [ 767.575018 ] R13 : 00007ff f496ab890 R14 : 00007ff f48704fb0 R15 : 00000000000013 88 [ 767.660303 ] FS : 00007ff fb5e9b700 ( 0000 ) GS : ffff88207fce0000 ( 0000 ) knlGS : 0000000000000000 [ 767.757028 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 767.825671 ] CR2 : 00007ff fb5eba000 CR3 : 000000207f e3a000 CR4 : 00000000000406 a0 [ 767.910958 ] get_signal () : dequeue_signr : 11 , handler : ( null ) [ 767.987928 ] get_signal () : dequeue_signr : 9 , handler : ( null ) 3 th run, without STRACE, with locked ibapi, it finished, result is correct. Runtime: 18.692936 sec . [ 555.423623 ] nr_pgfault : 288100 [ 555.458042 ] nr_pgfault_wp : 0 [ 555.492360 ] nr_pgfault_wp_cow : 0 [ 555.530838 ] nr_pgfault_wp_reuse : 0 [ 555.571396 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 555.630673 ] nr_pcache_fill_from_memory : 288081 [ 555.683710 ] nr_pcache_fill_from_victim : 12 [ 555.732588 ] nr_pcache_eviction : 494 [ 555.774187 ] nr_victim_eviction : 474 4 th run, same setting with the 3 th run, same result. But the nr_pgfault differs, I guess it is due to runtime things. Runtime: 19.12861 sec . [ 469.891700 ] nr_pgfault : 288119 [ 469.926123 ] nr_pgfault_wp : 0 [ 469.960444 ] nr_pgfault_wp_cow : 0 [ 469.998924 ] nr_pgfault_wp_reuse : 0 [ 470.039484 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 470.098764 ] nr_pcache_fill_from_memory : 288093 [ 470.151805 ] nr_pcache_fill_from_victim : 12 [ 470.200684 ] nr_pcache_eviction : 513 [ 470.242285 ] nr_victim_eviction : 493 5 th run, same with 4 th , succeed, Runtime: 18.653879 sec . [ 313.202348] nr_pgfault: 288070 [ 313.236772] nr_pgfault_wp: 0 [ 313.271093] nr_pgfault_wp_cow: 0 [ 313.309575] nr_pgfault_wp_reuse: 0 [ 313.350139] nr_pgfault_due_to_concurrent_eviction: 0 [ 313.409421] nr_pcache_fill_from_memory: 288052 [ 313.462465] nr_pcache_fill_from_victim: 6 [ 313.510307] nr_pcache_eviction: 446 [ 313.551909] nr_victim_eviction: 432 6 th , setting is the same, but with 4GB dataset, crashed: [ 512.028141 ] Processor : Processor manager is running . [ 529.375605 ] STDOUT : --- [ Wordcount : Running ... ] --- [ 529.435906 ] STDOUT : --- [ ] --- [ 529.476660 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 529.555983 ] ------------ [ cut here ] ------------ [ 529.609128 ] BUG : failure at managers / processor / pcache / rmap . c : 735 / pcache_zap_pte () ! [ 529.699613 ] Kernel Panic - not syncing : BUG ! [ 529.750576 ] CPU : 5 PID : 32 Comm : word_count - pthr 4.0.0 - lego - ys + # 361 [ 529.826500 ] Stack : [ 529.850422 ] ffff88107e1a3dd8 ffffffff810259b4 000000000000000 8 ffff88107e1a3de8 [ 529.937787 ] ffff88107e1a3da0 0000000021475542 0000000000000000 0000000000000000 [ 530.025152 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.112517 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.199882 ] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 530.287247 ] Call Trace : [ 530.316370 ] < TSK > [ 530.339251 ] [ < ffffffff810259c0 > ] panic + 0xc2 / 0xeb [ 530.394374 ] [ < ffffffff8106190a > ] ? client_internal_poll_sendcq + 0x2a / 0x80 [ 530.474458 ] [ < ffffffff8101bfcc > ] ? task_tick_rt + 0x2c / 0xd0 [ 530.538943 ] [ < ffffffff81019725 > ] ? scheduler_tick + 0x55 / 0x60 [ 530.605506 ] [ < ffffffff81016df5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 530.678311 ] [ < ffffffff8103768a > ] ? put_dec + 0x1a / 0x80 [ 530.737595 ] [ < ffffffff810066f4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 530.810398 ] [ < ffffffff8100e4aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 530.888403 ] [ < ffffffff81012ccd > ] ? printk + 0x11d / 0x1b0 [ 530.948726 ] [ < ffffffff81030429 > ] pcache_zap_pte + 0xf9 / 0x160 [ 531.014250 ] [ < ffffffff8102f090 > ] ? __pcache_move_pte_fastpath + 0x50 / 0x50 [ 531.093295 ] [ < ffffffff8102c8dc > ] unmap_page_range + 0x32c / 0x3b0 [ 531.161940 ] [ < ffffffff8102c97e > ] release_pgtable + 0x1e / 0x40 [ 531.227463 ] [ < ffffffff8102bfb3 > ] sys_munmap + 0xc3 / 0x120 [ 531.288827 ] [ < ffffffff8100e86d > ] do_syscall_64 + 0x3d / 0xc0 [ 531.352270 ] [ < ffffffff8100d76c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 7 th run, add debug info, does not seem that useful: ] --- [ 15755.579501 ] SYSC_close () : [ 4 ] -> [ / sys / devices / system / cpu / online ] [ 15755.672760 ] pte : ffff88107e1a3dd8 pfn : 0x8207e80b flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 ) [ 15755.807015 ] pte dumped because : Invalid pte [ 15755.856932 ] address : 0x7ffefc638000 [ 15755.899569 ] ------------ [ cut here ] ------------ [ 15755.954684 ] BUG : failure at managers / processor / pcache / rmap . c : 747 / pcache_zap_pte () ! [ 15756.045159 ] Kernel Panic - not syncing : BUG ! [ 15756.096114 ] CPU : 5 PID : 32 Comm : word_count - pt Tried several times, even with mmap/munmap debug option on, it crashed at the same point. Key is address 0x7ffefc638000 , and the mmap() related to it. Close to find the bug. Latest log in 0310-18.","title":"Trying phoenix pthread again"},{"location":"lego/log/log-03-2018/#0309-fri","text":"","title":"03/09 Fri"},{"location":"lego/log/log-03-2018/#find-bug-in-kmalloc","text":"Tried to print pud in every syscall and catch the criminal: wuklab13 030 9 - 1 [ 320.088684 ] CPU5 PID32 sys_close + 0x0 / 0x1f0 [ 320.137567 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 320.269657 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3c37 [ 320.349742 ] 3 [ 320.372624 ] SYSC_close () : [ 3 ] -> [ / lib64 / libpython2 .7 . so .1.0 ] [ 320.441268 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x0 ( 0 ) [ 320.510954 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 320.643043 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 [ 320.709607 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 320.798014 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 [ 320.995755 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 ret : 0 ( OKAY ) [ 321.101944 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 [ 321.168509 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 321.256914 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 [ 321.454651 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 ret : 0 ( OKAY ) [ 321.560845 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 [ 321.627409 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 pud ffff88207fcaeff8 [ 321.715815 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 [ 321.913553 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 ret : 0 ( OKAY ) [ 322.019745 ] CPU5 PID32 sys_open + 0x0 / 0x10 [ 322.066548 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 322.198638 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3b27 [ 322.277683 ] f_name : / lib64 / libpthread . so .0 , flags : 80000 , mode : e150 [ 322.357780 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x3 ( 3 ) [ 322.426414 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 After printing more in pcache_handle_fault, I found who corrupted pgtable: wuklab13 030 9 - 5 [ 661.308584 ] CPU5 PID32 sys_close + 0x0 / 0x1f0 [ 661.357466 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 661.489557 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3c37 [ 661.569642 ] 3 [ 661.592525 ] SYSC_close () : [ 3 ] -> [ / lib64 / libpython2 .7 . so .1.0 ] [ 661.661170 ] SYSC_close () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x0 ( 0 ) [ 661.730854 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 661.862944 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.001275 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 [ 662.067840 ] addr : 0x7ffff7a101f0 , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 662.156247 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 [ 662.353985 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a101f0 flags : 0x50 ret : 0 ( OKAY ) [ 662.460176 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.600586 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 662.738916 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 [ 662.805481 ] addr : 0x7ffff7a21749 , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 662.893888 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 [ 663.091636 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a21749 flags : 0x50 ret : 0 ( OKAY ) [ 663.197831 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 663.338242 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 663.476572 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 [ 663.543135 ] addr : 0x7ffff7ff2fda , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 663.631543 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 [ 663.829279 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7ff2fda flags : 0x50 ret : 0 ( OKAY ) [ 663.935472 ] pcache_handle_fault () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 664.075884 ] CPU5 PID32 sys_open + 0x0 / 0x10 [ 664.122686 ] do_syscall_64 () : enter pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 [ 664.254776 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3b27 [ 664.333821 ] f_name : / lib64 / libpthread . so .0 , flags : 80000 , mode : e150 [ 664.413918 ] SYSC_open () cpu ( 5 ) tsk ( 32 / 32 / python ) ret : 0x3 ( 3 ) [ 664.482552 ] do_syscall_64 () : leave pgd ffff88207fccf000 , pgd . cont_va ffff9001801ff000 , pud_index = 0x0 pud : ffff9001801ff000 Then, try catching bug with address 0x7ffff7ff2fda fault. Printing still being the most effective way to debug. :-) Dig further, I found pgtable corrupted after pcache_add_rmap() , namely after alloc_pcache_rmap() : [ 5024.482570 ] pcache_add_rmap () 343 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 5024.613601 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 5024.686396 ] pcache_add_rmap () 358 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 Well, rmap: ffff88207fccefd0 & ffff90207fcce000 , clearly [ 843.916517 ] pcache_add_rmap () 372 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.047557 ] alloc_pcache_rmap () 60 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.179638 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 844.252438 ] alloc_pcache_rmap () 71 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 844.384517 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 844.457317 ] alloc_pcache_rmap () 85 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 [ 844.589398 ] pcache_add_rmap () 387 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 46 static struct pcache_rmap * alloc_pcache_rmap ( void ) 47 { 48 struct pcache_rmap * rmap ; 49 50 pgd_t * pgd ; 51 pud_t * pud ; 52 unsigned long addr ; 53 struct mm_struct * mm = current -> mm ; 54 55 if ( pall ) { 56 addr = 0x601008 ; 57 pgd = pgd_offset ( mm , addr ); 58 pud = pud_alloc ( mm , pgd , addr ); 59 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 60 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 61 } 62 63 rmap = kmalloc ( sizeof ( * rmap ), GFP_KERNEL ); 64 65 if ( pall ) { 66 addr = 0x601008 ; 67 pgd = pgd_offset ( mm , addr ); 68 pud = pud_alloc ( mm , pgd , addr ); 69 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 70 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 71 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 72 } 73 74 if ( rmap ) { 75 INIT_LIST_HEAD ( & rmap -> next ); 76 rmap -> flags = 0 ; 77 } 78 79 if ( pall ) { 80 addr = 0x601008 ; 81 pgd = pgd_offset ( mm , addr ); 82 pud = pud_alloc ( mm , pgd , addr ); 83 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 84 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 85 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 86 } 87 88 return rmap ; 89 } Narrow it down to INIT_LIST_HEAD : [ 1334.548682 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1334.621487 ] alloc_pcache_rmap () 71 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1334.753576 ] alloc_pcache_rmap () 76 & rmap -> next ffff88207fcceff8 & flags ffff88207fccefd8 [ 1334.922067 ] alloc_pcache_rmap () 86 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 [ 1335.126962 ] alloc_pcache_rmap () 98 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 74 if ( rmap ) { 75 pr_info ( \"%s() %d &rmap->next %p &flags %p \\n \" , 76 __func__ , __LINE__ , & rmap -> next , & rmap -> flags ); 77 78 INIT_LIST_HEAD ( & rmap -> next ); 79 80 if ( pall ) { 81 addr = 0x601008 ; 82 pgd = pgd_offset ( mm , addr ); 83 pud = pud_alloc ( mm , pgd , addr ); 84 pr_info ( \"%s(): size: %zu, rmap: %p \\n \" , __func__ , sizeof ( * rmap ), rmap ); 85 pr_info ( \"%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n \" , 86 __func__ , __LINE__ , pgd , pgd_page_vaddr ( * pgd ), pud_index ( addr ), ( void * ) pud ); 87 } 88 89 rmap -> flags = 0 ; 90 } Seriously, if this is running on user-level on VM, I would be able to find the bug maybe in 30min. But I spent several hours to find it out with physical machine. Damn you physical machine. Hmm, this func is used A LOT. How can it fail at this point? Possible reasons: kmalloced area happen to intersect with pgtable? one physical page is mapped twice? one to pgtable, one by this rmap. tty/serial code has bug? Really ancient code. After add a few printk, IB seems stuck. And this happens just with few more lines of code! Why? code size matters? [ 722.381469 ] pcache_handle_fault () : enter pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 722.519778 ] addr : 0x7ffff7feffcc , pgd : ffff88207fccf7f8 [ 722.586334 ] addr : 0x7ffff7feffcc , pgd : ffff88207fccf7f8 pud ffff88207fcafff8 [ 722.674727 ] Before fill address = 0x7ffff7feffcc set_idx : 0x7fef [ 722.743362 ] pcache : ffff8801801ffbc0 mapcount : 0 refcount : 1 flags :( allocated | usable ) set_idx = 0x7fef kva : ffff880107fef000 [ 722.872312 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7feffcc flags : 0x50 [ 722.967985 ] __pcache_do_fill_page () : before net pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 last line Well, the following finding finally find the bug line. And it kind of explains the above bug. Probably kmalloc\u2019ed area has issues, so IB is touching wrong data. The following bug is related to kmalloc, the rmap is 56 bytes, and it should be within 1 single page, but it is not: [ 1862.307427 ] pcache_add_rmap () 413 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.438477 ] alloc_pcache_rmap () 86 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.570568 ] sp -> units : 50 SLOB_UNITS : 32 [ 1862.617372 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1862.690178 ] alloc_pcache_rmap () 97 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1862.822268 ] alloc_pcache_rmap () 104 & rmap -> next ffff88207fcceff8 & flags ffff88207fccefd8 [ 1862.918995 ] __INIT_LIST_HEAD () : next ffff88207fcceff8 prev ffff88207fccf000 [ 1863.002202 ] __INIT_LIST_HEAD () 63 pgd ffff88207fccf000 , pgd . cont_va ffff88207fcae000 , pud_index = 0x0 pud : ffff88207fcae000 [ 1863.133253 ] __INIT_LIST_HEAD () : next ffff88207fcceff8 prev ffff88207fccf000 [ 1863.216459 ] alloc_pcache_rmap () : size : 56 , rmap : ffff88207fccefd0 [ 1863.289265 ] alloc_pcache_rmap () 114 pgd ffff88207fccf000 , pgd . cont_va ffff90207fcce000 , pud_index = 0x0 pud : ffff90207fcce000 Analysis: The @prev field in line 7 has address ffff88207fccf000 , which happen to the pgd page ( pgd ffff88207fccf000 ). Thus when we do list->prev = list , it writes to the first 8 bytes of pgd page, corrupts the original pgd entry. That is why we see a corrupted pgd entry ( ffff90207fcce000 ). This roots from kmalloc, which should not allocate such an object that cross two pages.","title":"Find bug in kmalloc"},{"location":"lego/log/log-03-2018/#0308-thur","text":"Took several days off. This morning finished the porting of wait4 and waitid , which actually has a lot code change. The concept and mechanism is fairly simple, but the legacy UNIX tradition make the implementation quite complex. Now, look back to finish debugging the pcache issue. It must be fixed this week.","title":"03/08 Thur"},{"location":"lego/log/log-03-2018/#python","text":"Tried python hello_world.py , the program runs for a while and crashes at a deterministic point: wuklab13 and wuklab15 , ~/ ttyS1 [ 419097.929969 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a4b008 flags : 0x50 ret : 0 ( OKAY ) [ 419098.039145 ] __pcache_do_fill_page () : I pid : 32 tgid : 32 address : 0x7ffff7a4c010 flags : 0x50 [ 419098.306537 ] __pcache_do_fill_page () : O pid : 32 tgid : 32 address : 0x7ffff7a4c010 flags : 0x50 ret : 0 ( OKAY ) [ 419098.413756 ] CPU5 PID32 sys_mprotect + 0x0 / 0x90 [ 419098.465753 ] SYSC_mprotect () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3d27 [ 419098.549990 ] start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1 [ 419098.614469 ] BUG : unable to handle kernel paging request at ffff9001801ff000 [ 419098.698703 ] IP : [ < ffffffff8102f7a9 > ] pcache_handle_fault + 0x69 / 0x6c0 [ 419098.774621 ] PGD 0 [ 419098.799579 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 419098.848457 ] CPU : 5 PID : 32 Comm : python 4.0.0 - lego - ys + # 312 [ 419098.916054 ] RIP : 0010 : [ < ffffffff8102f7a9 > ] [ < ffffffff8102f7a9 > ] pcache_handle_fault + 0x69 / 0x6c0 [ 419099.021089 ] RSP : 0000 : ffff88107e857ed8 EFLAGS : 000102 86 [ 419099.085567 ] RAX : ffff9001801ff000 RBX : ffff9001801ff000 RCX : 00003ff ffffff000 [ 419099.171884 ] RDX : 00000801801ff 000 RSI : 000000000060100 8 RDI : ffff88107e83d648 [ 419099.258199 ] RBP : ffff88107e857f18 R08 : 00007ff ff7fe3000 R09 : 00007ff ff7fe3000 [ 419099.344516 ] R10 : 0000000000000000 R11 : 0000000000000206 R12 : 000000000060100 8 [ 419099.430832 ] R13 : ffff88107e83d648 R14 : 0000000000000050 R15 : 00007ff ff7ffe150 [ 419099.517149 ] FS : 00007ff ff7fdf740 ( 0000 ) GS : ffff88207fc40000 ( 0000 ) knlGS : 0000000000000000 [ 419099.614905 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 419099.684582 ] CR2 : ffff9001801ff000 CR3 : 000000207f ccf000 CR4 : 00000000000406 a0 [ 419099.770899 ] Stack : [ 419099.795858 ] 00007ff ff7d8c000 0000000000002000 0000000000000001 0000000000000004 [ 419099.884254 ] 000000000060100 8 ffff88107e857f58 0000000000000000 00007ff ff7ffe150 [ 419099.972650 ] ffff88107e857f48 ffffffff81010082 0000000000000000 0000000000000001 [ 419100.061047 ] 0003 92 c29c720ba2 0000000000000000 00007ff fffffdc40 ffffffff8100d91f [ 419100.149442 ] 00007ff ff7ffe150 0000000000000000 0003 92 c29c720ba2 0000000000000001 [ 419100.237839 ] Call Trace : [ 419100.267998 ] < TSK > [ 419100.291917 ] [ < ffffffff81010082 > ] do_page_fault + 0xa2 / 0x1a0 [ 419100.357434 ] [ < ffffffff8100d91f > ] page_fault + 0x1f / 0x30 [ 419100.418792 ] < EOT > M : ... [ 419142.163396 ] handle_p2m_pcache_miss () cpu 4 I nid : 0 pid : 32 tgid : 32 flags : 50 vaddr : 0x7ffff7a4c010 [ 419142.268460 ] handle_p2m_pcache_miss () cpu 4 O nid : 0 pid : 32 tgid : 32 flags : 50 vaddr : 0x7ffff7a4c010 ( Last Message ) Dig deeper: int pcache_handle_fault ( struct mm_struct * mm , unsigned long address , unsigned long flags ) { .. pgd = pgd_offset ( mm , address ); pr_info ( \" addr: %#lx, pgd: %p \\n \" , address , pgd ); pud = pud_alloc ( mm , pgd , address ); pr_info ( \" addr: %#lx, pgd: %p pud %p \\n \" , address , pgd , pud ); if ( ! pud ) return VM_FAULT_OOM ; pmd = pmd_alloc ( mm , pud , address ); if ( ! pmd ) .. } [ 21130.503314 ] strace__mprotect cpu5 start = 0x7ffff7d8c000 , len = 0x2000 , prot ( 0x1 ) = PROT_READ [ 21130.598994 ] SYSC_mprotect () cpu ( 5 ) tsk ( 32 / 32 / python ) user - ip : 0x7ffff7df3d27 [ 21130.682193 ] start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1 [ 21130.745635 ] addr : 0x601008 , pgd : ffff88207fccf000 [ 21130.805954 ] addr : 0x601008 , pgd : ffff88207fccf000 pud ffff9001801ff000 [ 21130.888116 ] BUG : unable to handle kernel paging request at ffff9001801ff000 [ 21130.971314 ] IP : [ < ffffffff8102fa11 > ] pcache_handle_fault + 0x91 / 0x6f0 Print pgd and pud info, these three messages are related and the last one leads to panic: wuklab13 ~/ ys / 030 8 - 6 [ 479.375498 ] addr : 0x400040 , pgd : ffff88207fccf000 [ 479.435819 ] pud_alloc_one () : addr : 0x400040 , pud : ffff88207fc6f000 [ 479.511739 ] pud_alloc () : addr : 0x400040 pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 479.649021 ] addr : 0x400040 , pgd : ffff88207fccf000 pud ffff88207fc6f000 [ 480.016381 ] addr : 0x600dd8 , pgd : ffff88207fccf000 [ 480.076701 ] pud_alloc () : addr : 0x600dd8 pgd ffff88207fccf000 , pgd . cont_va ffff88207fc6f000 , pud_index = 0x0 pud : ffff88207fc6f000 [ 480.213982 ] addr : 0x600dd8 , pgd : ffff88207fccf000 pud ffff88207fc6f000 [ 680.072819 ] addr : 0x601008 , pgd : ffff88207fccf000 [ 680.133138 ] pud_alloc () : addr : 0x601008 pgd ffff88207fccf000 , pgd . cont_va ffff90107e834000 , pud_index = 0x0 pud : ffff90107e834000 [ 680.270422 ] addr : 0x601008 , pgd : ffff88207fccf000 pud ffff90107e834000 [ 680.352583 ] BUG : unable to handle kernel paging request at ffff90107e834000 [ 680.435783 ] IP : [ < ffffffff8102fc43 > ] pcache_handle_fault + 0xb3 / 0x770 [ 680.510664 ] PGD 0 I need to check what happens between 480s to 680s. Something in between corrupted pgtable. I doubt it can be: copy_to_user related syscalls pcache establish mapping, mempcy all other memcpy strcpy etc stuff","title":"python"},{"location":"lego/log/log-03-2018/#0302-fri","text":"TODO: -add vsyscall- -pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.- pcache_exit_thread? I don\u2019t think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread. check python bug use omnigraffle to draw the whole workflow of pcache. Phoenix, word_count-seq, 4G dataset, 4GB pcache: [ 273.268853] Processor: Processor manager is running. [ 573.272479] page:ffffea0071bb9660 count:0 mapcount:-128 [ 573.332903] flags: 0x200000000000300(slab|slob_free) [ 573.392182] page dumped because: VM_BUG_ON_PAGE(page_ref_count(page) == 0) [ 573.474340] ------------[ cut here ]------------ [ 573.529459] BUG: failure at ./include/lego/mm.h:251/put_page_testzero()! [ 573.609537] Kernel Panic - not syncing: BUG! [ 573.660496] CPU: 4 PID: 13 Comm: kvictim_flushd 4.0.0-lego+ #18 [ 573.731212] Stack: [ 573.755132] ffff88207e4bfe10 ffffffff81023644 0000000000000008 ffff88207e4bfe20 [ 573.842490] ffff88207e4bfdd8 0000000021475542 0000000000000000 0000000000000000 [ 573.929848] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.017205] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.104563] 0000000000000000 0000000000000000 0000000000000000 0000000000000000 [ 574.191921] Call Trace: [ 574.221039] <TSK> [ 574.243919] [<ffffffff81023650>] panic+0xc2/0xeb [ 574.299038] [<ffffffff8105a35a>] ? client_internal_poll_sendcq+0x2a/0x80 [ 574.379115] [<ffffffff8105a4fd>] ? client_send_message_with_rdma_write_with_imm_request+0x14d/0x360 [ 574.487273] [<ffffffff8101ac3c>] ? task_tick_rt+0x2c/0xd0 [ 574.551751] [<ffffffff81018395>] ? scheduler_tick+0x55/0x60 [ 574.618308] [<ffffffff81015a45>] ? tick_handle_periodic+0x45/0x70 [ 574.691107] [<ffffffff810064c4>] ? apic_timer_interrupt+0x54/0x90 [ 574.763905] [<ffffffff8100dbaa>] ? smp__apic_timer_interrupt+0x6a/0x70 [ 574.841903] [<ffffffff8101198d>] ? printk+0x11d/0x1b0 [ 574.902222] [<ffffffff81025c00>] __free_pages+0x2e0/0x3c0 [ 574.966699] [<ffffffff81028472>] kfree+0x62/0x480 [ 575.022858] [<ffffffff8102e6be>] victim_flush_func+0x15e/0x1e0 [ 575.092536] [<ffffffff8102e560>] ? victim_try_fill_pcache+0x390/0x390 [ 575.169494] [<ffffffff8101e446>] kthread+0xf6/0x120 [ 575.227733] [<ffffffff8101e350>] ? __kthread_parkme+0x70/0x70 [ 575.296371] [<ffffffff8100de32>] ret_from_fork+0x22/0x30 [ 575.359810] <EOT>","title":"03/02 Fri"},{"location":"lego/log/log-03-2018/#0301-thur","text":"Weird. [43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25 [43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25 [43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150 [43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e [43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150 [43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e After all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private. Lego\u2019s current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux\u2019s batch flush. Today\u2019s case is flush tlb after making pte read-only. And this really has to be performed one by one","title":"03/01 Thur"},{"location":"lego/log/log-04-2018/","text":"April 2018 \u00b6 05/04 Fri \u00b6 We made it. We\u2019ve done our part, now, it depends on reviewers. Please, be mercy, our hardworking deserves something good. 04/29 Sun \u00b6 Rolling. 04/26 Thus \u00b6 Fix the victim pte_same issue in SMP race cases. SMP is really pain in the ass, how many times? But\u2026 another victim ref count bug show up in SMP. First log in 0426-w15-\u00bd 0426 - w15 - 1 / 3 [ 206.381646 ] CPU12 PID28 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x2e )( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207ff72000 [ 206.416658 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffd0000000 [ 206.433431 ] CPU12 PID28 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.468429 ] CPU12 PID28 rmap to pset : ffff88207ff72000 set_idx : 0 nr_lru : 63 [ 206.484425 ] CPU12 PID28 victim dumped because : PCACHE_BUG_ON_VICTIM ( ! VictimAllocated ( v ) || ! VictimUsable ( v ) || ! VictimFlushed ( v ) || VictimWriteback ( v ) || VictimLocked ( v )) [ 206.543952 ] CPU : 12 PID : 28 Comm : python 4.0.0 - lego + # 274 [ 206.521849 ] WARNING : CPU : 12 PID : 28 at managers / processor / pcache / victim . c : 196 __put_victim_nolist + 0xa5 / 0xd0 [ 206.722631 ] [ < ffffffff8103b555 > ] __put_victim_nolist + 0xa5 / 0xd0 [ 206.729127 ] [ < ffffffff8103c419 > ] victim_try_fill_pcache + 0x2d9 / 0x460 [ 206.736107 ] [ < ffffffff8103b740 > ] ? victim_insert_hit_entry + 0x170 / 0x170 [ 206.743378 ] [ < ffffffff810371ea > ] pcache_handle_fault + 0x18a / 0x750 [ 206.399206 ] CPU8 PID19 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.425092 ] CPU8 PID19 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffd0000000 [ 206.450977 ] CPU8 PID19 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.476475 ] CPU8 PID19 rmap to pset : ffff88207ff72000 set_idx : 0 nr_lru : 63 [ 206.501779 ] CPU8 PID19 victim dumped because : PCACHE_BUG_ON_VICTIM ( victim_ref_count ( v ) == 0 ) [ 206.549963 ] CPU : 8 PID : 19 Comm : kvictim_flushd 4.0.0 - lego + # 274 [ 206.532803 ] WARNING : CPU : 8 PID : 19 at . / include / processor / pcache_victim . h : 119 __victim_flush_func + 0x1e4 / 0x1f0 04/25 \u00b6 Stay humble. Be real. 04/22 Sun \u00b6 Testing. Hardworking! 04/21 Sat \u00b6 Another major bug report in 0421-w15-19. Rmapped corrupted. lock issue? Fixed. It is handle_m2m_fork bug. pcache_miss_error + 0x20 Keep it going. I can not remember how many times I have seen this bug issue. And I have no idea. [ 714.144354 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.150171 ] PGD 115 c067 PUD 115e067 PMD 0 [ 714.154729 ] Oops : 0010 [ # 1 ] SMP PROCESSOR [ 714.159189 ] CPU : 0 PID : 15 Comm : ib_mad_completi 4.0.0 - lego + # 245 [ 714.165976 ] BUG : unable to handle kernel paging request at ffffffffffff8100 [ 714.173732 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.179549 ] PGD 115 c067 PUD 115e067 PMD 0 [ 714.184106 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.192638 ] RSP : 0000 : ffff88103e88fc80 EFLAGS : 00010046 [ 714.198552 ] RAX : 6e82000000000098 RBX : 7 b0bffffffffffff RCX : 0000000000000001 [ 714.206503 ] RDX : ffff88103e88fd28 RSI : 0000000000000000 RDI : 44 c0ffffffff8116 [ 714.214453 ] RBP : ffff88103e88fcd0 R08 : 000000000000001f R09 : ffff88103e8643c0 [ 714.222403 ] R10 : ffff88103e88fe68 R11 : 0000000000000001 R12 : a9670000018d71ba [ 714.230354 ] R13 : 0000000000000000 R14 : ffff88103e85d0f8 R15 : ffff88103dd58000 [ 714.238304 ] Oops : 0010 [ # 2 ] SMP PROCESSOR [ 714.242763 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 714.251781 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 714.258180 ] CR2 : ffffffffffff8100 CR3 : 000000000115 9000 CR4 : 00000000000406 b0 [ 714.266130 ] CPU : 10 PID : 20 Comm : python 4.0.0 - lego + # 245 [ 714.272141 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.280673 ] RSP : 001 8 : ffff88103dd8fe10 EFLAGS : 00010202 [ 714.286588 ] RAX : ffff88101fa54270 RBX : 00000000000 c92a6 RCX : 0000000000000002 [ 714.294538 ] RDX : 00000000ff ffffff RSI : 0000000000000000 RDI : 44 c0ffffffff8116 [ 714.302488 ] RBP : ffff88103dd8fe20 R08 : ffff88101fa6f000 R09 : ffff88101fa54400 [ 714.310439 ] R10 : ffff880000000000 R11 : 00000000407e9 c00 R12 : ffff88101fa54000 [ 714.318389 ] R13 : ffff88103dd68000 R14 : ffff88101fa60000 R15 : ffff88101fa54000 [ 714.326339 ] Stack : [ 714.328569 ] FS : 00007ff ff7fdf740 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 714.337585 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 714.343984 ] CR2 : ffffffffffff8100 CR3 : 000000103 dd9a000 CR4 : 00000000000406 a0 [ 714.351936 ] Stack : [ 714.354165 ] ffffffff810157f9 00000000003 d0f00 ffff88103dd8fec0 ffffffff8101dde5 [ 714.362309 ] ffff88103dd8fe68 ffffffff81036788 000000000000003 8 000000000000003 8 [ 714.370453 ] 00007ff fd89a79c0 ffff88101fa541c0 ffff88101fa54188 0000000000000000 [ 714.378598 ] 000000101f a60000 00007ff fd89a79d0 00007ff fd89a7700 0000000000000000 [ 714.386742 ] 00007ff fd89a6fb0 ffff88103dd8ff58 000000000000003 8 00000000003 d0f00 [ 714.394886 ] Call Trace : [ 714.397600 ] ffffffff81014f37 00000000000000 86 ffff88107fc05d80 ffff88103e864000 [ 714.405745 ] 0000000000000000 ffff88107fc04980 0000000000000000 0000000000000000 [ 714.413889 ] ffff88103e85d0f8 ffff88103dd58000 ffff88103e88fce8 ffffffff81016bb7 [ 714.422034 ] 000000007f c05d80 ffff88103e88fd10 ffffffff81006754 ffffffffffff0000 [ 714.430177 ] ffff88107fc05d80 ffff88103e864000 ffff88103e88fe00 ffffffff8100e4ea [ 714.438321 ] Call Trace : [ 714.441037 ] < TSK > [ 714.443169 ] [ < ffffffff810157f9 > ] ? ktime_get + 0x19 / 0x60 [ 714.448890 ] [ < ffffffff8101dde5 > ] copy_process + 0x2c5 / 0x1170 [ 714.454998 ] [ < ffffffff81036788 > ] ? strace_printflags + 0x88 / 0xc0 [ 714.461495 ] < TSK > [ 714.463627 ] [ < ffffffff81014f37 > ] ? update_wall_time + 0x47 / 0x6b0 [ 714.470123 ] [ < ffffffff81016bb7 > ] tick_handle_periodic + 0x67 / 0x70 [ 714.476716 ] [ < ffffffff81006754 > ] apic_timer_interrupt + 0x55 / 0x90 [ 714.483309 ] [ < ffffffff8101ecb6 > ] do_fork + 0x26 / 0x160 [ 714.488738 ] [ < ffffffff8101eea9 > ] sys_clone + 0x29 / 0x30 [ 714.494265 ] [ < ffffffff8100e8ad > ] do_syscall_64 + 0x3d / 0xd0 [ 714.500180 ] [ < ffffffff8100d7ac > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 714.507257 ] [ < ffffffff8100e4ea > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 714.514335 ] < EOT > 04/20 Fri \u00b6 Glad TF finally working now! Keep seeing this message from kernel. It have been many many times. Very deterministic. BUG : unable to handle kernel paging request at ffffffffffff8100 04/19 Thur \u00b6 Patched clflush to use tgid, n_nid directly without task_struct. In a 256M excache today (0419-w15-4), a timeout happen first, which will be handled as segfault to kill all threads. In an eviction->victim_prepare_hits, the get_memory_nodes() encounter the NULL again. Looks like the thread_group->mm got cleared before. \u00b6 04/18 Wed \u00b6 Try best to fix the pipe bug. (I found it by using my old way of debugging. By writing a function that test if PTE is corrupted or not. I put that function around the sycall enter/exit. So it help to find which syscall corrupt memory. I have used this stupid technique to find so many hard-to-find memory corruption bugs.....) do_close_on_exec dup2 Re-read Yutong\u2019s patch again. It touches a lot handler code. This has to be verified before using any nowait reply. pipe\u2019s wakeup may have issue? 0418-w15-41. 39sec 04/17 Tue \u00b6 Checking list: -pcache: ibapi use va or pa, does it matter?- No, I change it to use the VA. Then we don\u2019t have the need to use PA reply any more. =ib_mad, does it really corrupt Memory= Still not sure. Should be something come from the ib_poll_cq . M side per PTE lock, check if the lock is really the same lock! -Mail I20. Check CPT.- Dist-VMA First make sure, TF+no-dist-vma work on my own setting. Though sometimes random bug happen (I doubt it is IB). Then turn on dist-vma w/wo zerofill w/wo kfree w/wo all-zero Debug. w/wo M side per PTE lock Change most handlers to use TX buffer. Reduce the random mismatched reply case. P side watchdog patch: what to print -It looks like it is more easier to have bug when I turn on those debug counter printing. I probably should check those buffer mgmt. All next test have zerofill:- w print F 0417-w15-2(rmap_walk list_for_each_entrry #GP) F 0417-w15-3(pcache_copy_page_range corrupted PTE) F 0417-w15-4(fit_poll_cq+0x39 ib_poll_cq() \u2026) F 0417-w15-5(pcache_copy_page_range corrupted PTE) wo strace exit: S 0417-w15-6(each 100 step take ~39s/ Linux is ~34s) S 0417-w15-7(filling shuffle data, that works) F 0417-w15-8(pcache_copy_page_range+0x5d1) F 0417-w15-9(rmap_walk+0x47 #GP) disable strace: F 0417-w15-10(pcache_copy_page_range+0x5d1) Conclusion it has nothing to do with the strace thing. most of them fail around nr_reqs=19103 Why the pcache_copy_page_range always happen, after some fork, execve. w strace (fork, vfork, clone, execve) F 0417-w15-11 (pcache_cp_pg_range). Understand its flow. Back to make sure P side per PTE lock is correct. If it is pcache_cp fault, it always fail at nr_reqs=19103 . And it is: 1) python fork, 2) execve sh. S 0417-w15-12. With global PTE lock. Passed the failed stage above. F 0417-W15-13. With global PTE lock. Failed at pcache_cp. Same place. (Since global PTE lock also fail, so it is not the lock issue. Still someone write to wrong memory.) F 0417-w15-14. With global PTE lock. Same place. Found that I printed a misleading debug info. Modified a little bit to print the actual pte content. Hope can get some valid info next round. F 0417-w15-15. Same place. copy: addr: 0x7fffdca07000, ptecont: 0x8800000000000 . zap: ptent: 0x340 address: 0x7fffdca08000 . F 0417-w15-16. Well. BUG in ib_mad_send handler. I add the same checking in ib_mad_receive. This is really just used to catch it. Not fixing it. F 0417-w15-17. Again, addr: 0x7fffdc207000, ptecont: 0x8800000000000 F 0417-w15-18. addr: 0x7fffdca07000, ptecont: 0x8800000000000 Conclusion Only these two addresses addr: 0x7fffdca07000, ptecont: 0x8800000000000 pte:ffff88103ea87038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000) addr: 0x7fffdc207000, ptecont: 0x8800000000000 pte:ffff88103ea97038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000) Bug found. In pipe_read/write. It somehow corrupted memory. Damn. -Another first thing, check this weird log.. : Hmm, this log should be fine. mad_post is after recv_done_handler. So even if we detect corrupted memory in handler, it has nothing to do with mad_post. The root cause should come from ib_poll_cq, that is where we pass wc to, and where the wc.wr_id was filled in.- [ 3850.911144 ] ib_mad_recv_done_handler () : c1 : 2060 c2 : 12 wc -> wr_id : 0xffff88103eea1398 [ 3850.921881 ] ib_mad_post_receive_mads () : c1 : 2060 c2 : 13 recv_wr . wr_id : 0xffff88103eea1008 recv_queue : ffff88103ee42520 [ 3850.933620 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 13 [ 3850.942346 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 14 [ 3850.951266 ] ib_mad_recv_done_handler () : c1 : 2061 c2 : 13 wc -> wr_id : 0xffff88103eea1560 [ 3850.961999 ] ib_mad_post_receive_mads () : c1 : 2061 c2 : 14 recv_wr . wr_id : 0xffff88103eea11d0 recv_queue : ffff88103ee42520 [ 3850.973737 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 14 [ 3851.257563 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 15 [ 3851.266295 ] ib_mad_recv_done_handler () : c1 : 2062 c2 : 14 wc -> wr_id : 0xffff88103eea1728 [ 3851.277029 ] ib_mad_post_receive_mads () : c1 : 2062 c2 : 15 recv_wr . wr_id : 0xffff88103eea1398 recv_queue : ffff88103ee42520 [ 3851.288767 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 15 [ 3851.297493 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 16 [ 3851.306413 ] ib_mad_recv_done_handler () : c1 : 2063 c2 : 15 wc -> wr_id : 0xffff88103eea18f0 [ 3851.317147 ] ib_mad_post_receive_mads () : c1 : 2063 c2 : 16 recv_wr . wr_id : 0xffff88103eea1560 recv_queue : ffff88103ee42520 [ 3851.328886 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 16 [ 3851.903180 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 17 [ 3851.911913 ] ib_mad_recv_done_handler () : c1 : 2064 c2 : 16 wc -> wr_id : 0xffff88103eea1ab8 [ 3851.922646 ] ib_mad_post_receive_mads () : c1 : 2064 c2 : 17 recv_wr . wr_id : 0xffff88103eea1728 recv_queue : ffff88103ee42520 [ 3851.934384 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 17 [ 3851.943110 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 18 [ 3851.952030 ] ib_mad_recv_done_handler () : c1 : 2065 c2 : 17 wc -> wr_id : 0xffff88103eea1c80 [ 3851.962764 ] ib_mad_post_receive_mads () : c1 : 2065 c2 : 18 recv_wr . wr_id : 0xffff88103eea18f0 recv_queue : ffff88103ee42520 [ 3851.974502 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 18 [ 3864.723128 ] *** FIT layer ready to go ! [ 3864.727206 ] *** [ 3867.339488 ] Processor LLC Configurations : [ 3867.343760 ] PhysStart : 0x100000000 [ 3867.348705 ] VirtStart : 0xffff880100000000 [ 3867.354329 ] Registered Size : 0x400000000 [ 3867.359274 ] Actual Used Size : 0x208000000 [ 3867.364219 ] NR cachelines : 2097152 [ 3867.368776 ] Associativity : 8 [ 3867.372751 ] NR Sets : 262144 [ 3867.377210 ] Cacheline size : 4096 B [ 3867.381672 ] Metadata size : 64 B [ 3867.385937 ] NR cacheline bits : 12 [ 0 - 11 ] 0x0000000000000fff [ 3867.392821 ] NR set - index bits : 18 [ 12 - 29 ] 0x000000003ffff000 [ 3867.399705 ] NR tag bits : 34 [ 30 - 63 ] 0xffffffffc0000000 [ 3867.406588 ] NR pages for data : 2097152 [ 3867.411147 ] NR pages for meta : 32768 [ 3867.415509 ] Cacheline ( pa ) range : [ 0x100000000 - 0x2ffffffff ] [ 3867.423848 ] Metadata ( pa ) range : [ 0x300000000 - 0x307ffffff ] [ 3867.432186 ] Cacheline ( va ) range : [ 0xffff880100000000 - 0xffff8802ffffffff ] [ 3867.440524 ] Metadata ( va ) range : [ ffff880300000000 - 0xffff880307ffffff ] [ 3867.448862 ] pcache_set_map ( 064 B ) : [ ffff88207ec00000 - 0xffff88207fbfffff ] [ 3867.457201 ] Way cache stride : 0x40000000 [ 3867.462048 ] Memmap $ semantic : memblock reserved [ 3867.468156 ] NR victim $ entries : 8 [ 3867.472725 ] newpid : 1 home : 1 replica : 1 [ 3867.476980 ] p2m_fork ( cpu0 ) : I cur : 1 - kernel_init new : 20 [ 3867.482718 ] p2m_fork ( cpu0 ) : O succeed cur : 1 - kernel_init new : 20 [ 3867.489197 ] Processor : Processor manager is running . [ 3867.494724 ] Online CPU : 0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 3867.500444 ] Active CPU : 0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 3867.505777 ] [ 0 ] Thread [ kvictim_flushd : 19 ] pinned at CPU 8 [ 3867.511982 ] [ 1 ] Thread [ recvpollcq : 17 ] pinned at CPU 4 [ 3867.539217 ] do_close_on_exec () : TODO , not implemented . [ 3867.549209 ] STDOUT : --- [ Before execv ^ V ] --- [ 3867.553870 ] STDOUT : --- [ e --- [ 3867.557880 ] newpid : 20 home : 1 replica : 1 [ 3867.562248 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 3867.567560 ] p2m_fork ( cpu10 ) : O succeed cur : 20 - exe . o new : 21 [ 3867.573670 ] CPU12 PID21 sys_execve [ 3867.578681 ] do_close_on_exec () : TODO , not implemented . [ 3867.584215 ] CPU12 PID21 sys_execve = 0 , 0x0 [ 3867.599867 ] BUG : unable to handle kernel paging request at 000000040 8446080 [ 3867.607436 ] IP : [ < ffffffff8101bbbf > ] task_tick_rt + 0x1f / 0xd0 04/16 Mon \u00b6 Make dist-vma work with TF first. Tough work. 0416-w14-7 : 1) do_wp_page triggered, 2) dealock on per pte lock. This really should not happen. It is single worker. Basically means the page->lock is not intialized. Probabaly our per PTE lock implementation is wrong. [ 5220.250552 ] hb : worker [ 0 ] CPU 4 stucked [ 5220.254819 ] hb : common_header [ op = 0x20000000 src_nid : 0 ] [ 5220.260734 ] hb : msg [ pid = 21 , tgid = 21 , flags = 0x51 , vaddr = 0x7fff7b7fdfb8 ] [ 5220.267911 ] CPU : 4 PID : 31 Comm : thpool - worker0 4.0.0 - lego - ys + # 237 [ 5220.274890 ] RIP : 0010 : [ < ffffffff81031aa3 > ] [ < ffffffff81031aa3 > ] handle_lego_mm_fault + 0x373 / 0x4f0 handle_lego_mm_fault + 0x373 / 0x4ee : arch_spin_lock at arch / x86 / include / asm / spinlock . h : 21 ( inlined by ) spin_lock at include / lego / spinlock . h : 72 ( inlined by ) do_anonymous_page at managers / memory / vm / fault . c : 115 ( inlined by ) handle_pte_fault at managers / memory / vm / fault . c : 142 ( inlined by ) handle_lego_mm_fault at managers / memory / vm / fault . c : 225 A IB bug during normal run (P M S TF), this is REALLY weird: [ 395.259560 ] CPU12 PID21 sys_execve [ 395.263345 ] BUG : unable to handle kernel NULL pointer dereference at 00000000000001 a0 [ 395.272068 ] IP : [ < ffffffff81064c09 > ] fit_poll_cq + 0x39 / 0x530 fit_poll_cq + 0x39 / 0x523 : git :( test_vma )] $ . / scripts / faddr2line vmImage fit_poll_cq + 0x39 ib_poll_cq at include / rdma / ib_verbs . h : 1614 ( inlined by ) fit_poll_cq at net / lego / fit_internal . c : 1671 Catch the ib_mad bug once.. and mlx4_error follows. I added more checking to where the mad_queue was assigned. [ 787.471385 ] ib_mad_completion_handler 2365 got successful recv cq op 128 mad_got_one 15 [ 787.480124 ] BUG ! mad_list : ffff88103eea1728 mad_queue : ( null ) [ 787.487491 ] ------------ [ cut here ] ------------ [ 787.492630 ] WARNING : CPU : 0 PID : 15 at drivers / infiniband / core / mad . c : 1909 ib_mad_completion_handler + 0xa56 / 0xab0 04/15 Sun \u00b6 Trying TF myself. Had a bug report on 0415-w15-5, on fork, execve etc. [ 317.436811 ] newpid : 22 home : 1 replica : 1 [ 317.477701 ] pte : ffff88103e94a038 pfn : 0x0 flags :() [ 317.482752 ] pte dumped because : corrupted [ 317.487213 ] ------------ [ cut here ] ------------ [ 317.492352 ] WARNING : CPU : 14 PID : 22 at managers / processor / pgtable . c : 365 pcache_copy_page_range + 0x5d1 / 0x6c0 [ 317.503213 ] CPU : 14 PID : 22 Comm : python 4.0.0 - lego + # 93 [ 317.552082 ] Call Trace : [ 317.554799 ] < TSK > [ 317.556930 ] [ < ffffffff810123a1 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 317.563330 ] [ < ffffffff8101246f > ] warn_slowpath_null + 0xf / 0x20 [ 317.569634 ] [ < ffffffff8102d401 > ] pcache_copy_page_range + 0x5d1 / 0x6c0 [ 317.576615 ] [ < ffffffff81037ed7 > ] fork_dup_pcache + 0x27 / 0x30 [ 317.582723 ] [ < ffffffff8101e514 > ] copy_process + 0xcf4 / 0x1140 [ 317.588833 ] [ < ffffffff8101e986 > ] do_fork + 0x26 / 0x160 [ 317.594264 ] [ < ffffffff8101eb89 > ] sys_clone + 0x29 / 0x30 [ 317.599789 ] [ < ffffffff8100e66d > ] do_syscall_64 + 0x3d / 0xd0 [ 317.605705 ] [ < ffffffff8100d56c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 317.612782 ] < EOT > [ 317.614917 ] --- [ end trace 0000000000000000 ] --- [ 317.625561 ] p2m_fork ( cpu14 ) : I cur : 22 - python new : 36 [ 330.209312 ] p2m_fork ( cpu14 ) : O succeed cur : 22 - python new : 36 [ 330.310909 ] ------------ [ cut here ] ------------ [ 330.315864 ] BUG : failure at managers / processor / pcache / rmap . c : 804 / pcache_zap_pte () ! [ 330.324302 ] Kernel Panic - not syncing : BUG ! [ 330.329050 ] CPU : 0 PID : 36 Comm : python 4.0.0 - lego + # 93 [ 330.377824 ] Call Trace : [ 330.380540 ] < TSK > [ 330.382672 ] [ < ffffffff81026493 > ] panic + 0xc2 / 0x105 [ 330.387908 ] [ < ffffffff8101bbcc > ] ? task_tick_rt + 0x2c / 0xd0 [ 330.393920 ] [ < ffffffff81019245 > ] ? scheduler_tick + 0x55 / 0x60 [ 330.400126 ] [ < ffffffff810168f5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 330.406913 ] [ < ffffffff81006684 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 330.413700 ] [ < ffffffff8100e2aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 330.420973 ] [ < ffffffff810125ad > ] ? printk + 0x11d / 0x1b0 [ 330.426597 ] [ < ffffffff810375bc > ] pcache_zap_pte + 0x14c / 0x190 [ 330.432802 ] [ < ffffffff81035db0 > ] ? __pcache_remove_rmap_one + 0x70 / 0x70 [ 330.439978 ] [ < ffffffff8102cd25 > ] unmap_page_range + 0x325 / 0x3f0 [ 330.446379 ] [ < ffffffff8102ce0e > ] release_pgtable + 0x1e / 0x40 [ 330.452487 ] [ < ffffffff81037ef8 > ] pcache_process_exit + 0x18 / 0x20 [ 330.458984 ] [ < ffffffff8101d3c4 > ] mmput + 0x34 / 0xb0 [ 330.464123 ] [ < ffffffff8102c38d > ] do_execve + 0x42d / 0x760 [ 330.469845 ] [ < ffffffff8102c6c9 > ] sys_execve + 0x9 / 0x10 [ 330.475371 ] [ < ffffffff8100e66d > ] do_syscall_64 + 0x3d / 0xd0 [ 330.481286 ] [ < ffffffff8100d56c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 330.488364 ] < EOT > [ 330.490501 ] --- [ end Kernel panic - not syncing : BUG ! one more [ 369.223161] newpid: 22 home:1 replica: 1 [ 369.264307] pte:ffff88103ea41038 (0x0) pfn:0x0 flags:() [ 369.269938] pte dumped because: corrupted [ 369.274399] ------------[ cut here ]------------ [ 369.279538] WARNING: CPU: 14 PID: 22 at managers/processor/pgtable.c:365 pcache_copy_page_range+0x5d1/0x6c0 [ 369.290398] CPU: 14 PID: 22 Comm: python 4.0.0-lego+ #94 [ 369.296310] Stack: [ 369.341976] <TSK> [ 369.344107] [<ffffffff810123a1>] __warn.constprop.0+0x91/0xd0 [ 369.350508] [<ffffffff8101246f>] warn_slowpath_null+0xf/0x20 [ 369.356809] [<ffffffff8102d401>] pcache_copy_page_range+0x5d1/0x6c0 [ 369.363790] [<ffffffff81037f07>] fork_dup_pcache+0x27/0x30 [ 369.369897] [<ffffffff8101e514>] copy_process+0xcf4/0x1140 [ 369.376006] [<ffffffff8101e986>] do_fork+0x26/0x160 [ 369.381435] [<ffffffff8101eb89>] sys_clone+0x29/0x30 [ 369.386960] [<ffffffff8100e66d>] do_syscall_64+0x3d/0xd0 [ 369.392875] [<ffffffff8100d56c>] entry_SYSCALL64_slow_path+0x25/0x25 [ 369.399952] <EOT> [ 369.402086] ---[ end trace 0000000000000000 ]--- [ 369.412750] p2m_fork(cpu14): I cur:22-python new:36 [ 369.418215] p2m_fork(cpu14): O succeed cur:22-python new:36 [ 369.500829] ptent: 0x340 address: 0x7fffe2408000 [ 369.505783] pte:ffff88103dbe5040 (0x340) pfn:0x0 flags:(dirty|global|softw1) [ 369.513637] pte dumped because: corrupted [ 369.518095] ------------[ cut here ]------------ [ 369.523236] BUG: failure at managers/processor/pcache/rmap.c:808/pcache_zap_pte()! [ 369.531672] Kernel Panic - not syncing: BUG! 04/14 Sat \u00b6 Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm. Will they do harm Implement replication Add IB counter 04/13 Fri \u00b6 Patched M side pgtable to use per PTE/PMD lock. So thpool in M will not be bottlnecked by the page_table_lock. ib_mad_recv_done_handler may corrupt memory, again. \u00b6 Somehow, during testing of this patch. Running with MT-Phoenix 1GB, the P side has reported bad pgd entries. I\u2019m using fork+execve way. The child(phoenix) already exit. This msg is printed when parent exit_mm. The pgd table should either be 0, or valid pud va. Memory corruption happened\u2026 [ 2551.687806 ] Kernel strace [ 2551.690715 ] Task : 21 : 21 nr_accumulated_threads : 1 [ 2551.696327 ] % time seconds usecs / call calls errors syscall [ 2551.703704 ] ------ -------------- ----------- --------- --------- ---------------- [ 2551.712141 ] 98.63 66.942660568 66942661 1 0 sys_wait4 [ 2551.719898 ] 0.45 0.457060789 457061 1 0 sys_clone [ 2551.727654 ] 0.20 0.204320071 51081 4 0 sys_brk [ 2551.735216 ] 0.40 0.040378189 40379 1 0 sys_mmap [ 2551.742876 ] 0.13 0.013682424 4561 3 0 sys_write [ 2551.750633 ] 0.10 0.000001039 2 1 0 sys_newfstat [ 2551.758681 ] 0.88 0.000000888 1 2 0 sys_rt_sigaction [ 2551.767114 ] 0.79 0.000000792 1 2 0 sys_futex [ 2551.774871 ] 0.77 0.000000770 1 1 0 sys_rt_sigprocmask [ 2551.783501 ] 0.54 0.000000548 1 1 0 sys_arch_prctl [ 2551.791742 ] 0.49 0.000000499 1 1 0 sys_newuname [ 2551.799789 ] 0.46 0.000000469 1 1 0 sys_getrlimit [ 2551.807933 ] 0.19 0.000000195 1 1 0 sys_set_tid_address [ 2551.816659 ] 0.19 0.000000190 1 1 0 sys_set_robust_list [ 2551.825386 ] 0.18 0.000000181 1 1 0 sys_ioctl [ 2551.833143 ] ------ -------------- ----------- --------- --------- ---------------- [ 2551.841577 ] 100.00 67.658107612 22 0 total [ 2551.848945 ] [ 2551.850591 ] [ 2551.852240 ] Kernel Profile Points [ 2551.855924 ] status name total nr avg . ns [ 2551.865621 ] ------- -------------------- ---------------- ---------------- ---------------- [ 2551.875317 ] off flush_tlb_others 0.000204992 58 3535 [ 2551.885014 ] off __do_kmalloc_node 0.300783843 281501 1069 [ 2551.894709 ] off __pcache_zerofill 0.009844770 16558 595 [ 2551.904404 ] off pcache_miss 54.414457906 257869 211016 [ 2551.914100 ] off pcache_flush 0.000000000 0 0 [ 2551.923795 ] ------- -------------------- ---------------- ---------------- ---------------- [ 2551.933490 ] [ 2552.074985 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956028 ( ffffffff81146ca0 ) [ 2552.084206 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956030 ( ffff88103e956030 ) [ 2552.093611 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956038 ( ffff88103e956030 ) [ 2552.103016 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956048 ( ffff88103cc48740 ) [ 2552.112421 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956050 ( 00000000000001 c0 ) [ 2552.121825 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956058 ( ffff88103eea2008 ) [ 2552.131230 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956060 ( ffff88103eea17d8 ) [ 2552.140635 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956068 ( ffff88103ee42520 ) [ 2552.150040 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9560e8 ( 000000103e9560 f0 ) [ 2552.159444 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956118 ( 010200 8081018101 ) [ 2552.168849 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956120 ( 3 c010b0012000000 ) [ 2552.178254 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956128 ( 0000000000001100 ) [ 2552.187659 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956138 ( 00000000ff ffffff ) [ 2552.197064 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956158 ( 0307 8 a2402010101 ) [ 2552.206467 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956160 ( 0307 8 a2453946600 ) [ 2552.215872 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956168 ( 0307 8 a2450946600 ) [ 2552.225277 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956170 ( 0310 800051946600 ) [ 2552.234682 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956178 ( c902000100000000 ) [ 2552.244088 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956198 ( bfd0cc054a122000 ) [ 2552.253492 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561a0 ( 0000000000 98 b9c8 ) [ 2552.262897 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561a8 ( bfe0fe0610914e01 ) [ 2552.272302 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561b0 ( 000000000050f 2 c7 ) [ 2552.281706 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561b8 ( bfd9a30000ec5100 ) [ 2552.291111 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561c0 ( bffc91d40f20f2c7 ) [ 2552.300516 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561c8 ( 0f 20 cd054a20f2c7 ) [ 2552.309920 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561d0 ( 1094 edcf0f60edcf ) [ 2552.319325 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561d8 ( 0000000000000100 ) [ 2552.328730 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956218 ( 0000000000005 aa5 ) [ 2552.338151 ] nr_pgfault : 26 Second run, saw this invalid pointer deference again! Combined with the above log, I think ib_mad is definitely corrupting memory! I have to take a look. qp_info = mad_list->mad_queue->qp_info; Patching the handlers to use tx buffer. \u00b6 Patched. Once race condition: pcache_handle_miss use the page itself as the reply buffer. Assume later on, it changes to use nowait reply. When the reply is buffered in the queue and has not been sent. Another munmap comes in and invalidate this area, then the page will be freed. The data is invalidate. But this case seems abnormal. The application will not do so I guess. Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm. \u00b6 Tonight task. Think about how to do the VMA replication, how to combine with the $ line replicaiton. \u00b6 04/12 Thur \u00b6 Patched zerofill. All done. Testing new driver fix with Phoenix - 1 st run, the mismatch reply is still there. mmap() replied address is different from the one printed. So segfault follows. (0412-w15-4) - 2st run, 3st run, succeed. 0412-w15-9 0412-w14-9 First time testing phoenix with zerofill (no net). Somehow, P has pcache timeout, but M\u2019s watchdog show there is no pending requests. This happen once before I remember\u2026 0412-w15-10. Have not seen this ib mad thing for a long time. Indeed somewhere is wrong. [ 297.794969] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15 [ 297.803706] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020 [ 297.812431] IP: [<ffffffff81058937>] ib_mad_completion_handler+0xc7/0x810 2 04/11 Wed \u00b6 Adding anon first touch opt. 0411-p/m-9 : this log indicate M does not have any unhandled requests, but P side has 1 __pcache_fill timeout. It seems the message is lost somewhere. 0411-p/m-11 : catch one with the debug msg Yiying added. She says the M side send queue has 2 reqs. But poll does not return any error. Weird. Help debugging IB issue. 04/10 Tue \u00b6 Found. IB stuck. Damn. [ 2240.294960] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2242.694733] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2245.094524] RIP: 0010:[<ffffffff8104a6e3>] [<ffffffff8104a6e3>] mlx4_ib_poll_cq+0x383/0x6a0 [ 2247.494306] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2249.894088] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2252.293870] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2254.693651] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2257.093431] RIP: 0010:[<ffffffff8104a6e3>] [<ffffffff8104a6e3>] mlx4_ib_poll_cq+0x383/0x6a0 04/09 Mon \u00b6 thpool testing. 4 workers. MT-phoenix: [ root @ wuklab05 ys ] # cat 040 9 - p | grep __munmap [ 227.054974 ] CPU14 PID22 strace__munmap ([ 0x7fffb0ba9000 - 0x7fffb4000000 ], 54882304 ) = 0 , 0x0 [ 227.093466 ] CPU16 PID23 strace__munmap ([ 0x7fffab7ff000 - 0x7fffac000000 ], 8392704 ) = 0 , 0x0 [ 227.102773 ] CPU14 PID22 strace__munmap ([ 0x7fffb8000000 - 0x7fffb8ba9000 ], 12226560 ) = 0 , 0x0 [ 227.141265 ] CPU18 PID24 strace__munmap ([ 0x7fffa8000000 - 0x7fffac000000 ], 67108864 ) = 0 , 0x0 [ 227.150669 ] CPU16 PID23 strace__munmap ([ 0x7fffb0000000 - 0x7fffb37ff000 ], 58716160 ) = 0 , 0x0 [ 227.218248 ] CPU22 PID26 strace__munmap ([ 0x7fffa0000000 - 0x7fffa4000000 ], 67108864 ) = 0 , 0x0 [ 227.285826 ] CPU2 PID28 strace__munmap ([ 0x7fff98000000 - 0x7fff9c000000 ], 67108864 ) = 0 , 0x0 [ 227.440567 ] CPU14 PID31 strace__munmap ([ 0x7fff8a7fd000 - 0x7fff8c000000 ], 25178112 ) = 0 , 0x0 [ 227.449972 ] CPU12 PID30 strace__munmap ([ 0x7fff88000000 - 0x7fff8c000000 ], 67108864 ) = 0 , 0x0 [ 227.459376 ] CPU14 PID31 strace__munmap ([ 0x7fff90000000 - 0x7fff927fd000 ], 41930752 ) = 0 , 0x0 [ 227.490109 ] CPU18 PID33 strace__munmap ([ 0x7fff80000000 - 0x7fff84000000 ], 67108864 ) = 0 , 0x0 [ 227.723140 ] word_count - pthr [ 29 ] : segfault at 0x4e842010 ip 0000000000420354 sp 00007ff fb17f9bc0 error 6 0x4e842010 Print mmap on M, if segfault. Printed, the 0x4e842010 is never a valid address. thpool makes Memory side SMP. Probably bring some issues. Found: P CPU22 PID26 strace__mmap(addr=0x0, len=0xfb000, prot(0x3)=PROT_READ|PROT_WRITE, flags(0x22)=MAP_PRIVATE|MAP_ANONYMOUS, fd=18446744073709551615( ), off=0x0) = 1317351432, 0x4e853008 word_count-pthr[26]: segfault at 0x4e853010 ip 0000000000420354 sp 00007fff972a8bc0 error 6 M [ 583.120615] 00400000-004d9000 r-xp 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 583.131578] 006d9000-006dc000 rw-p 000d9000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 583.142729] 006dc000-00755000 rw-p 00000000 [heap] [ 583.148254] 7fff529c9000-7fffb93aa000 rw-p 00000000 [ 583.153974] 7fffb93aa000-7ffff7fff000 rw-p 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_1GB.txt [ 583.167355] 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] [ 583.173753] ------------[ cut here ]------------ [ 583.178892] WARNING: CPU: 4 PID: 31 at managers/memory/handle_pcache/fault.c:55 handle_p2m_pcache_miss+0x18e/0x1d0 [ 583.190430] src_nid:0,pid:21,vaddr:0x4e853010 [ 583.195279] CPU: 4 PID: 31 Comm: thpool-worker0 4.0.0-lego-ys+ #90 Confirmed. I printed added a number to mmap requests. And the compare the results of both P and M. The data is wrong. Btw, I\u2019m only running 1 worker thread at M, which makes it single thread handling. So, I\u2019m going to, 1) first use kmalloc to get the reply buffer, and 2) revert back the IB MAX_OUT config, remove the #ifdef COMP_MEMORY. See if it is this patch\u2019s issue. P : CPU18 PID24 strace__mmap ( 30 ..) = - 1325940736 , 0x7fffb0f7c000 CPU22 PID26 strace__mmap ( 31 ..) = 2144269992 , 0x7fcef6a8 M : ... handle_p2m_mmap () : 30 7ff fb0f7c000 handle_p2m_mmap () : 31 7ff fb0efe000 ... Anyway, this is temporary fixed by using kmalloced reply buffer. Spent whole afternoon and whole night. Finally figure out why timeout happen in P. It is because somewhere in the middle, M has 1 or more requests stucked/unhandled. Deadlock happen in the middle. Like this one. 5 requests queued waiting, 1 is being handled. And that 1 handler stuck. And it is handle_pcache_miss. Now, I need to find out where it stuck! thpool-worker0 nr_queued: 5 1 Oh, I really hope we can have some soft/hw lockdep, watchdog stuff . This should make out life much much much much much easier! 04/08 Sun \u00b6 Trying the fit_nowait patch. First try fit_nowait patch, without any chanegs to other code. See if this patch can work. Second, modify pcache to use reply_message_nowait. See if this can work. and improve performance. Third, if 2) can improve, perf. Move on to modify thpool patch. 1 st , P fail at ib_mad, during boot: [ 349.239220 ] Online CPU : 0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 349.244940 ] Active CPU : 0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 349.250272 ] [ 0 ] Thread [ kvictim_flushd : 19 ] pinned at CPU 8 [ 349.256478 ] [ 1 ] Thread [ recvpollcq : 17 ] pinned at CPU 4 [ 356.188819 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 13 [ 356.197545 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 356.206270 ] IP : [ < ffffffff81058287 > ] ib_mad_completion_handler + 0xc7 / 0x810 2st run, P side, config MAX_OUT to 1. Then single-thread pheonix with 1GB data finished. But forgot to turn on the profile point. Run one more time. 3st run. Same with 2st run setting. But with profile on. Bug shows. Ugh. I still think it is because of ib_mad_handler. It must write to wrong memory locations, and corrupt things randomly. [ 456.237913 ] do_close_on_exec () : TODO , not implemented . ... [ 456.263274 ] BUG : unable to handle kernel paging request at 00000002f 4 bfbf58 [ 456.270843 ] IP : [ < ffffffff8101bbff > ] task_tick_rt + 0x1f / 0xd0 [ 456.277048 ] PGD 0 [ 456.279279 ] Thread overran stack , or stack corrupted [ 456.284804 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 456.289265 ] CPU : 10 PID : 20 Comm : kevict_sweepd 4.0.0 - lego + # 40 [ 456.295858 ] RIP : 0010 : [ < ffffffff8101bbff > ] [ < ffffffff8101bbff > ] task_tick_rt + 0x1f / 0xd0 4st run, succeed. But it looks like the perf is very bad. Oh. but 99% of the pcache miss are file-backed, which will go to storage. So the number is actually doubled. With fit_nowait patch : [ 308.660051 ] Kernel Profile Points [ 308.663734 ] status name total nr avg . ns [ 308.673431 ] ------- -------------------- ---------------- ---------------- ---------------- [ 308.683128 ] off flush_tlb_others 0.000130715 53 2467 [ 308.692824 ] off __do_kmalloc_node 0.097344056 265647 367 [ 308.702521 ] off pcache_miss 4.504660891 258211 17446 [ 308.712218 ] off pcache_flush 0.000000000 0 0 [ 308.721914 ] ------- -------------------- ---------------- ---------------- ---------------- 5st run. Just run large malloc test. Looks better than yesterday\u2019s result. But I\u2019m using 15 as P today, instead of 13. So, let me try one more time to see if it is the machine. With fit_nowait patch : [ 674.382592 ] Kernel Profile Points [ 674.386277 ] status name total nr avg . ns [ 674.395974 ] ------- -------------------- ---------------- ---------------- ---------------- [ 674.405670 ] off flush_tlb_others 0.000130838 53 2469 [ 674.415366 ] off __do_kmalloc_node 1.604700641 1584917 1013 [ 674.425062 ] off pcache_miss 6.467938547 786571 8223 [ 674.434758 ] off pcache_flush 3.342783614 262225 12748 [ 674.444455 ] ------- -------------------- ---------------- ---------------- ---------------- [ 674.554497 ] nr_pgfault : 786513 [ 674.557706 ] nr_clflush : 262225 [ 674.561099 ] nr_pgfault_wp : 0 [ 674.564299 ] nr_pgfault_wp_cow : 0 [ 674.567887 ] nr_pgfault_wp_reuse : 0 [ 674.571668 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 674.577195 ] nr_pcache_fill_from_memory : 786511 [ 674.582139 ] nr_pcache_fill_from_victim : 2 6st run. Looks like the above fit_nowait can have 400ns improvement. But how come? I did not even change the pcache handling to use ibapi_nowait!!! Maybe random variation. Let me run more. Without fit_nowait patches [ 428.546738 ] Kernel Profile Points [ 428.550424 ] status name total nr avg . ns [ 428.560119 ] ------- -------------------- ---------------- ---------------- ---------------- [ 428.569815 ] off flush_tlb_others 0.000131140 53 2475 [ 428.579510 ] off __do_kmalloc_node 1.758704197 1331927 1321 [ 428.589205 ] off pcache_miss 6.807601189 786575 8655 [ 428.598899 ] off pcache_flush 3.699044847 262227 14107 [ 428.608594 ] ------- -------------------- ---------------- ---------------- ---------------- [ 428.618289 ] [ 428.718670 ] nr_pgfault : 786515 [ 428.721878 ] nr_clflush : 262227 [ 428.725272 ] nr_pgfault_wp : 0 [ 428.728470 ] nr_pgfault_wp_cow : 0 [ 428.732058 ] nr_pgfault_wp_reuse : 0 [ 428.735840 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 428.741365 ] nr_pcache_fill_from_memory : 786515 [ 428.746310 ] nr_pcache_fill_from_victim : 0 7 th run. without fit_nowait. without fit_nowait . [ 901.223090 ] Kernel Profile Points [ 901.226775 ] status name total nr avg . ns [ 901.236472 ] ------- -------------------- ---------------- ---------------- ---------------- [ 901.246168 ] off flush_tlb_others 0.000130802 53 2468 [ 901.255865 ] off __do_kmalloc_node 1.862575608 1331923 1399 [ 901.265560 ] off pcache_miss 6.814540477 786572 8664 [ 901.275257 ] off pcache_flush 3.699187003 262224 14107 [ 901.284953 ] ------- -------------------- ---------------- ---------------- ---------------- 8 th run. without fit_nowait. [ 321.514564 ] Kernel Profile Points [ 321.518250 ] status name total nr avg . ns [ 321.527945 ] ------- -------------------- ---------------- ---------------- ---------------- [ 321.537639 ] off flush_tlb_others 0.000130934 53 2471 [ 321.547335 ] off __do_kmalloc_node 2.216772665 1331939 1665 [ 321.557031 ] off pcache_miss 6.806060415 786573 8653 [ 321.566726 ] off pcache_flush 3.725455841 262231 14207 [ 321.576421 ] ------- -------------------- ---------------- ---------------- ---------------- 9 th run. with fit_nowait [ 374.847912 ] Kernel Profile Points [ 374.851597 ] status name total nr avg . ns [ 374.861293 ] ------- -------------------- ---------------- ---------------- ---------------- [ 374.870989 ] off flush_tlb_others 0.000130858 53 2470 [ 374.880684 ] off __do_kmalloc_node 1.485304454 1331934 1116 [ 374.890381 ] off pcache_miss 6.615317677 786582 8411 [ 374.900076 ] off pcache_flush 3.508328900 262234 13379 [ 374.909772 ] ------- -------------------- ---------------- ---------------- ---------------- 10 th run, with fit_nowait [ 225.211058] Kernel Profile Points [ 225.214743] status name total nr avg.ns [ 225.224440] ------- -------------------- ---------------- ---------------- ---------------- [ 225.234137] off flush_tlb_others 0.000131029 53 2473 [ 225.243833] off __do_kmalloc_node 1.211421872 1331984 910 [ 225.253529] off pcache_miss 6.583096125 786574 8370 [ 225.263226] off pcache_flush 3.464430818 262227 13212 [ 225.272922] ------- -------------------- ---------------- ---------------- ---------------- Sum: with fit_nowait: [ 225.253529] off pcache_miss 6.583096125 786574 8370 [ 225.263226] off pcache_flush 3.464430818 262227 13212 [ 374.890381] off pcache_miss 6.615317677 786582 8411 [ 374.900076] off pcache_flush 3.508328900 262234 13379 [ 674.425062] off pcache_miss 6.467938547 786571 8223 [ 674.434758] off pcache_flush 3.342783614 262225 12748 Without fit_nowait: [ 428.589205] off pcache_miss 6.807601189 786575 8655 [ 428.598899] off pcache_flush 3.699044847 262227 14107 [ 901.265560] off pcache_miss 6.814540477 786572 8664 [ 901.275257] off pcache_flush 3.699187003 262224 14107 [ 321.557031] off pcache_miss 6.806060415 786573 8653 [ 321.566726] off pcache_flush 3.725455841 262231 14207 04/07 Sat \u00b6 Well, now we finished all the profiling stuff. Continue on other work. Now I like listening Jazz while coding. Amazing Jazz, really good. Once again, ib_mad_completion_handler bug will happen. During application run, or even after application exit. [ 465.835447 ] nr_mremap_pset_diff : 0 [ 477.086886 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21 [ 477.095620 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 477.104345 ] IP : [ < ffffffff81058277 > ] ib_mad_completion_handler + 0xc7 / 0x810 ib_mad_completion_handler + 0xc7 / 0x808 : ib_mad_recv_done_handler at drivers / infiniband / core / mad . c : 1899 ( inlined by ) ib_mad_completion_handler at drivers / infiniband / core / mad . c : 2345 After remove net from pcache miss: [ 465.572131 ] Kernel Profile Points [ 465.575815 ] status name total nr avg . ns [ 465.585510 ] ------- -------------------- ---------------- ---------------- ---------------- [ 465.595206 ] off flush_tlb_others 0.000000000 0 0 [ 465.604901 ] off __do_kmalloc_node 0.656371295 1762220 373 [ 465.614597 ] off pcache_miss 7.172572671 786596 9119 [ 465.624291 ] off pcache_flush 3.698294960 262251 14103 [ 465.633987 ] ------- -------------------- ---------------- ---------------- ---------------- After remove net from pcache flush: [ 684.984000 ] Kernel Profile Points [ 684.987683 ] status name total nr avg . ns [ 684.997379 ] ------- -------------------- ---------------- ---------------- ---------------- [ 685.007074 ] off flush_tlb_others 0.000000000 0 0 [ 685.016770 ] off __do_kmalloc_node 0.627372836 1500543 419 [ 685.026464 ] off pcache_miss 7.128702028 786596 9063 [ 685.036159 ] off pcache_flush 3.660772506 262251 13960 [ 685.045855 ] ------- -------------------- ---------------- ---------------- ---------------- malloc, miss, flush are too slow. Especially the flush, how can it take 13.9us? It must be our handlers! lego_copy_to_user stuff. 04/06 Fri \u00b6 Well. Now we have in-kernel strace, in-kernel readprofile. Yummy. 04/05 Thur \u00b6 Discussion with Yilun. 1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure. 2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch. 3. I probably want to use per-cpu counter. Anyway, continue strace work first. Finished. 04/04 Wed \u00b6 STRACE Performance \u00b6 TF has very bad performance. It is either due to the syscall or pcache. Now I\u2019m adding facilities to track syscall activities, including average latency, total time. Basic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping. But this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity. Oh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime. So the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it. Yeh! Let us do this tomorrow. We will have a robust kernel version strace. SM Heartbeat \u00b6 Continue run some experiments on yesterday\u2019s case. One we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval: [ 1224.034898 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15 [ 1224.130616 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 15 [ 1224.222189 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 16 [ 1224.417181 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 16 [ 1393.159845 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 17 [ 1393.255546 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 17 [ 1393.347132 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 18 [ 1393.538972 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 18 [ 1449.437542 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 19 [ 1449.533248 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 19 [ 1449.624833 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 20 [ 1449.722512 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 20 [ 4322.423624 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21 [ 4322.519328 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 21 [ 4322.610914 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 22 [ 4322.708594 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 22 [ 4350.750574 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 23 [ 4350.846278 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 23 [ 4350.937863 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 24 [ 4351.035543 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 24 [ 4519.690559 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 25 [ 4519.786262 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 25 [ 4519.877848 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 26 [ 4519.975527 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 26 [ 4576.396279 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 27 [ 4576.491979 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 27 [ 4576.583565 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 28 [ 4576.681245 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 28 [ 4942.886820 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 29 [ 4942.982523 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 29 [ 4943.074108 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 30 [ 4943.171789 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 30 04/03 Tue \u00b6 BUG BUG BUG \u00b6 Finished basic replication mechanism last night. Today merged several patches. And both Yilun and I think there is something wrong with ib_mad_completion_handler . It seems it will break things behind our back. This is one bug catched today: ib_mad_completion_handler \u00b6 At very early stage : [ 1174.406177 ] newpid : 20 home : 1 replica : 1 [ 1174.452983 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 1177.462795 ] ib_mad_completion_handler 2324 got successful recv cq op 128 mad_got_one 22 [ 1177.556502 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 1177.650101 ] IP : [ < ffffffff81059104 > ] ib_mad_completion_handler + 0xb4 / 0x8a0 . / scripts / faddr2line vmImage ib_mad_completion_handler + 0xb4 ib_mad_completion_handler + 0xb4 / 0x899 : ib_mad_recv_done_handler at drivers / infiniband / core / mad . c : 1899 ( inlined by ) ib_mad_completion_handler at drivers / infiniband / core / mad . c : 2325 ib_mad_recv_done_handler () : 1899 : qp_info = mad_list -> mad_queue -> qp_info ; A more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4: [ 863.887705 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 868.478424 ] p2m_fork ( cpu10 ) : O succeed cur : 20 - exe . o new : 21 [ 868.541991 ] BUG : unable to handle kernel NULL pointer dereference at 000000000000000 8 [ 868.635569 ] IP : [ < ffffffff810656d4 > ] __schedule + 0x94 / 0x1e0 [ 868.701090 ] PGD 0 [ 868.725010 ] general protection fault : 0000 [ # 1 ] SMP PROCESSOR [ 868.793651 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 737 Source : clear_tsk_need_resched ( prev ); Even this one for Phoenix: [ 763.442043 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000010 [ 763.535636 ] IP : [ < ffffffff81018d6f > ] task_curr + 0xf / 0x30 [ 763.598035 ] PGD 103e956067 PUD 103e964067 PMD 0 [ 763.653154 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 763.700992 ] CPU : 12 PID : 21 Comm : word_count - pthr 4.0.0 - lego - ys + # 740 [ 763.777950 ] RIP : 0010 : [ < ffffffff81018d6f > ] [ < ffffffff81018d6f > ] task_curr + 0xf / 0x30 This NEVER happen before. And this part of code should be correct. We\u2019ve ran a lot things.. I doubt if recent IB merge corrupt things. fit_poll_cq \u00b6 Another one: [ 690.401626 ] stat : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt [ 690.507742 ] SYSC_close () CPU12 PID : 21 [ fd : 4 ] -> [ / sys / devices / system / cpu / online ] [ 713.899884 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 21 [ 713.995606 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 21 [ 714.087185 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 22 [ 714.184871 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 22 [ 742.078102 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 23 [ 742.173810 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 23 [ 742.265399 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 24 [ 742.363085 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 24 [ 847.063372 ] mlx4_ib_handle_error_cqe syndrome 21 [ 847.116511 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.170590 ] send request failed at connection 7 as 12 [ 847.230909 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.284988 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.339067 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.393146 ] fit_poll_cq : failed status ( 5 ) for wr_id 1832 [ 847.457624 ] fit_poll_cq : failed status ( 5 ) for wr_id 1833 [ 847.522103 ] fit_poll_cq : connection 7 Recv weird event as - 1 [ 847.589701 ] fit_poll_cq : failed status ( 5 ) for wr_id 1834 [ 847.654179 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 847.725938 ] fit_poll_cq : failed status ( 5 ) for wr_id 1835 [ 847.790416 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 847.862174 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.916252 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.970331 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.024410 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.078490 ] fit_poll_cq : failed status ( 5 ) for wr_id 1836 [ 848.142967 ] fit_poll_cq : failed status ( 5 ) for wr_id 1837 [ 848.207446 ] fit_poll_cq : connection 7 Recv weird event as - 1 [ 848.275044 ] fit_poll_cq : failed status ( 5 ) for wr_id 1838 [ 848.339523 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 848.411281 ] fit_poll_cq : failed status ( 5 ) for wr_id 1839 [ 848.475760 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 848.547517 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.601596 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.655675 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.709753 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.763832 ] fit_poll_cq : failed status ( 5 ) for wr_id 1840 [ 848.828313 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 848.921908 ] IP : [ < ffffffff8106346d > ] fit_poll_cq + 0x4ad / 0x510 [ 848.989507 ] PGD 0 [ 849.013426 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 849.061265 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 744 [ 849.131983 ] RIP : 0010 : [ < ffffffff8106346d > ] [ < ffffffff8106346d > ] fit_poll_cq + 0x4ad / 0x510 [ 849.228700 ] RSP : 0000 : ffff88103e813d88 EFLAGS : 00010246 [ 849.292139 ] RAX : 000000000000100 8 RBX : ffff88103effbad0 RCX : 0000000000000000 [ 849.377418 ] RDX : 0000000000000000 RSI : ffffffff811d46e0 RDI : ffffffff811dbc08 [ 849.462695 ] RBP : ffff88103e813ea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 849.547973 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 849.633251 ] R13 : ffff88103e801008 R14 : 0000000000000004 R15 : ffff88103e813da0 [ 849.718529 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc40000 ( 0000 ) knlGS : 0000000000000000 [ 849.815246 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 849.883884 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 849.969163 ] Stack : [ 849.993082 ] ffffffff81003299 000001 b03e813da0 0000000000000004 0000000000000730 [ 850.080440 ] 000000 8100000005 00001008000000f 9 ffff88103eff8c50 002 c222040000000 [ 850.167798 ] 0010004000000002 ffff88107fc20000 0000000000000731 ffffffff00000005 [ 850.255156 ] ffff8810000000f9 ffff88103eff8c50 0000000000000000 ffff88103e813e38 [ 850.342513 ] ffffffff81019854 0000000000000732 ffff881000000005 ffff8810000000f9 [ 850.429871 ] Call Trace : [ 850.458992 ] < TSK > [ 850.481870 ] [ < ffffffff81003299 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 850.560909 ] [ < ffffffff81019854 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 850.628506 ] [ < ffffffff81065708 > ] ? __schedule + 0xf8 / 0x1e0 [ 850.691945 ] [ < ffffffff810634d0 > ] ? fit_poll_cq + 0x510 / 0x510 [ 850.757464 ] [ < ffffffff810634e4 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 850.824021 ] [ < ffffffff81020636 > ] kthread + 0xf6 / 0x120 [ 850.882260 ] [ < ffffffff81020540 > ] ? __kthread_parkme + 0x70 / 0x70 [ 850.950898 ] [ < ffffffff8100e572 > ] ret_from_fork + 0x22 / 0x30 /* handle normal reply */ ... memcpy (( void * ) ctx -> reply_ready_indicators [ reply_indicator_index ], & length , sizeof ( int )); ... ( This is a bad memcpy : reply_indicator_index , ctx , etc should be checked .) IB Spec: QP, CQE, WQE, SEND \u00b6 The channel adapter detects the WQE posting and accesses the WQE. The channel adapter interprets the command, validates the WQE\u2019s virtual 12 addresses, translates it to physical addresses, and accesses the data. The outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source & destination GIDs). The channel adapter then adds the local route header and calculates both the variant and invariant checksums. For a Send operation, the QP retrieves the address of the receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not the last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing the receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator. When the originator receives an acknowledgment, it creates a CQE on the 5 CQ and retires the WQE from the send queue. A QP can have multiple outstanding messages at any one time but the 8 target always acknowledges in the order sent, thus WQEs are retired in the order that they are posted. 04/02 Mon \u00b6 Patching storage replica handler, able to finish today. 04/01 Sun \u00b6 Anyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover. I\u2019m tired. :-( A month to go. Record a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o: P [ 30801.296160 ] ibapi_send_reply () CPU : 8 PID : 19 timeout ( 30010 ms ), caller : clflush_one + 0x1c9 / 0x370 [ 30938.564843 ] mlx4_ib_handle_error_cqe syndrome 21 [ 30938.617988 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.672068 ] send request failed at connection 6 as 12 [ 30938.732389 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.786470 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.840551 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.894632 ] fit_poll_cq : failed status ( 5 ) for wr_id 1584 [ 30938.959112 ] fit_poll_cq : failed status ( 5 ) for wr_id 1585 [ 30939.023593 ] fit_poll_cq : connection 6 Recv weird event as - 1 [ 30939.091194 ] fit_poll_cq : failed status ( 5 ) for wr_id 1586 [ 30939.155676 ] fit_poll_cq : connection 6 Recv weird event as - 30704 [ 30939.227436 ] fit_poll_cq : failed status ( 5 ) for wr_id 1587 [ 30939.291917 ] fit_poll_cq : connection 6 Recv weird event as - 30704 [ 30939.363678 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.417759 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.471839 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.525921 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.580002 ] fit_poll_cq : failed status ( 5 ) for wr_id 1588 [ 30939.644483 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 30939.738083 ] IP : [ < ffffffff81062fcd > ] fit_poll_cq + 0x4ad / 0x510 [ 30939.805684 ] PGD 0 [ 30939.829604 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 30939.877445 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 715 [ 30939.948166 ] RIP : 0010 : [ < ffffffff81062fcd > ] [ < ffffffff81062fcd > ] fit_poll_cq + 0x4ad / 0x510 fit_poll_cq at net / lego / fit_internal . c : 1734 memcpy (( void * ) ctx -> reply_ready_indicators [ reply_indicator_index ], & length , sizeof ( int )); M [ 30913.642698 ] mlx4_ib_handle_error_cqe syndrome 21 [ 30913.695839 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.749919 ] send request failed at connection 1 as 12 [ 30913.810236 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.864315 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.918395 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.972474 ] fit_poll_cq : failed status ( 5 ) for wr_id 305 [ 30914.035912 ] fit_poll_cq : failed status ( 5 ) for wr_id 306","title":"April 2018"},{"location":"lego/log/log-04-2018/#april-2018","text":"","title":"April 2018"},{"location":"lego/log/log-04-2018/#0504-fri","text":"We made it. We\u2019ve done our part, now, it depends on reviewers. Please, be mercy, our hardworking deserves something good.","title":"05/04 Fri"},{"location":"lego/log/log-04-2018/#0429-sun","text":"Rolling.","title":"04/29 Sun"},{"location":"lego/log/log-04-2018/#0426-thus","text":"Fix the victim pte_same issue in SMP race cases. SMP is really pain in the ass, how many times? But\u2026 another victim ref count bug show up in SMP. First log in 0426-w15-\u00bd 0426 - w15 - 1 / 3 [ 206.381646 ] CPU12 PID28 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x2e )( allocated | usable | hasdata | waitflush ) pcm : ( null ) pset : ffff88207ff72000 [ 206.416658 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffd0000000 [ 206.433431 ] CPU12 PID28 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.468429 ] CPU12 PID28 rmap to pset : ffff88207ff72000 set_idx : 0 nr_lru : 63 [ 206.484425 ] CPU12 PID28 victim dumped because : PCACHE_BUG_ON_VICTIM ( ! VictimAllocated ( v ) || ! VictimUsable ( v ) || ! VictimFlushed ( v ) || VictimWriteback ( v ) || VictimLocked ( v )) [ 206.543952 ] CPU : 12 PID : 28 Comm : python 4.0.0 - lego + # 274 [ 206.521849 ] WARNING : CPU : 12 PID : 28 at managers / processor / pcache / victim . c : 196 __put_victim_nolist + 0xa5 / 0xd0 [ 206.722631 ] [ < ffffffff8103b555 > ] __put_victim_nolist + 0xa5 / 0xd0 [ 206.729127 ] [ < ffffffff8103c419 > ] victim_try_fill_pcache + 0x2d9 / 0x460 [ 206.736107 ] [ < ffffffff8103b740 > ] ? victim_insert_hit_entry + 0x170 / 0x170 [ 206.743378 ] [ < ffffffff810371ea > ] pcache_handle_fault + 0x18a / 0x750 [ 206.399206 ] CPU8 PID19 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.425092 ] CPU8 PID19 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffd0000000 [ 206.450977 ] CPU8 PID19 victim : ffff88207ff69120 index : 4 refcount : 0 nr_fill : 0 locked : 0 flags :( 0x4e )( allocated | usable | hasdata | flushed ) pcm : ( null ) pset : ffff88207ff72000 [ 206.476475 ] CPU8 PID19 rmap to pset : ffff88207ff72000 set_idx : 0 nr_lru : 63 [ 206.501779 ] CPU8 PID19 victim dumped because : PCACHE_BUG_ON_VICTIM ( victim_ref_count ( v ) == 0 ) [ 206.549963 ] CPU : 8 PID : 19 Comm : kvictim_flushd 4.0.0 - lego + # 274 [ 206.532803 ] WARNING : CPU : 8 PID : 19 at . / include / processor / pcache_victim . h : 119 __victim_flush_func + 0x1e4 / 0x1f0","title":"04/26 Thus"},{"location":"lego/log/log-04-2018/#0425","text":"Stay humble. Be real.","title":"04/25"},{"location":"lego/log/log-04-2018/#0422-sun","text":"Testing. Hardworking!","title":"04/22 Sun"},{"location":"lego/log/log-04-2018/#0421-sat","text":"Another major bug report in 0421-w15-19. Rmapped corrupted. lock issue? Fixed. It is handle_m2m_fork bug. pcache_miss_error + 0x20 Keep it going. I can not remember how many times I have seen this bug issue. And I have no idea. [ 714.144354 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.150171 ] PGD 115 c067 PUD 115e067 PMD 0 [ 714.154729 ] Oops : 0010 [ # 1 ] SMP PROCESSOR [ 714.159189 ] CPU : 0 PID : 15 Comm : ib_mad_completi 4.0.0 - lego + # 245 [ 714.165976 ] BUG : unable to handle kernel paging request at ffffffffffff8100 [ 714.173732 ] IP : [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.179549 ] PGD 115 c067 PUD 115e067 PMD 0 [ 714.184106 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.192638 ] RSP : 0000 : ffff88103e88fc80 EFLAGS : 00010046 [ 714.198552 ] RAX : 6e82000000000098 RBX : 7 b0bffffffffffff RCX : 0000000000000001 [ 714.206503 ] RDX : ffff88103e88fd28 RSI : 0000000000000000 RDI : 44 c0ffffffff8116 [ 714.214453 ] RBP : ffff88103e88fcd0 R08 : 000000000000001f R09 : ffff88103e8643c0 [ 714.222403 ] R10 : ffff88103e88fe68 R11 : 0000000000000001 R12 : a9670000018d71ba [ 714.230354 ] R13 : 0000000000000000 R14 : ffff88103e85d0f8 R15 : ffff88103dd58000 [ 714.238304 ] Oops : 0010 [ # 2 ] SMP PROCESSOR [ 714.242763 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 714.251781 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 714.258180 ] CR2 : ffffffffffff8100 CR3 : 000000000115 9000 CR4 : 00000000000406 b0 [ 714.266130 ] CPU : 10 PID : 20 Comm : python 4.0.0 - lego + # 245 [ 714.272141 ] RIP : 0010 : [ < ffffffffffff8100 > ] [ < ffffffffffff8100 > ] 0xffffffffffff8100 [ 714.280673 ] RSP : 001 8 : ffff88103dd8fe10 EFLAGS : 00010202 [ 714.286588 ] RAX : ffff88101fa54270 RBX : 00000000000 c92a6 RCX : 0000000000000002 [ 714.294538 ] RDX : 00000000ff ffffff RSI : 0000000000000000 RDI : 44 c0ffffffff8116 [ 714.302488 ] RBP : ffff88103dd8fe20 R08 : ffff88101fa6f000 R09 : ffff88101fa54400 [ 714.310439 ] R10 : ffff880000000000 R11 : 00000000407e9 c00 R12 : ffff88101fa54000 [ 714.318389 ] R13 : ffff88103dd68000 R14 : ffff88101fa60000 R15 : ffff88101fa54000 [ 714.326339 ] Stack : [ 714.328569 ] FS : 00007ff ff7fdf740 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 714.337585 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 714.343984 ] CR2 : ffffffffffff8100 CR3 : 000000103 dd9a000 CR4 : 00000000000406 a0 [ 714.351936 ] Stack : [ 714.354165 ] ffffffff810157f9 00000000003 d0f00 ffff88103dd8fec0 ffffffff8101dde5 [ 714.362309 ] ffff88103dd8fe68 ffffffff81036788 000000000000003 8 000000000000003 8 [ 714.370453 ] 00007ff fd89a79c0 ffff88101fa541c0 ffff88101fa54188 0000000000000000 [ 714.378598 ] 000000101f a60000 00007ff fd89a79d0 00007ff fd89a7700 0000000000000000 [ 714.386742 ] 00007ff fd89a6fb0 ffff88103dd8ff58 000000000000003 8 00000000003 d0f00 [ 714.394886 ] Call Trace : [ 714.397600 ] ffffffff81014f37 00000000000000 86 ffff88107fc05d80 ffff88103e864000 [ 714.405745 ] 0000000000000000 ffff88107fc04980 0000000000000000 0000000000000000 [ 714.413889 ] ffff88103e85d0f8 ffff88103dd58000 ffff88103e88fce8 ffffffff81016bb7 [ 714.422034 ] 000000007f c05d80 ffff88103e88fd10 ffffffff81006754 ffffffffffff0000 [ 714.430177 ] ffff88107fc05d80 ffff88103e864000 ffff88103e88fe00 ffffffff8100e4ea [ 714.438321 ] Call Trace : [ 714.441037 ] < TSK > [ 714.443169 ] [ < ffffffff810157f9 > ] ? ktime_get + 0x19 / 0x60 [ 714.448890 ] [ < ffffffff8101dde5 > ] copy_process + 0x2c5 / 0x1170 [ 714.454998 ] [ < ffffffff81036788 > ] ? strace_printflags + 0x88 / 0xc0 [ 714.461495 ] < TSK > [ 714.463627 ] [ < ffffffff81014f37 > ] ? update_wall_time + 0x47 / 0x6b0 [ 714.470123 ] [ < ffffffff81016bb7 > ] tick_handle_periodic + 0x67 / 0x70 [ 714.476716 ] [ < ffffffff81006754 > ] apic_timer_interrupt + 0x55 / 0x90 [ 714.483309 ] [ < ffffffff8101ecb6 > ] do_fork + 0x26 / 0x160 [ 714.488738 ] [ < ffffffff8101eea9 > ] sys_clone + 0x29 / 0x30 [ 714.494265 ] [ < ffffffff8100e8ad > ] do_syscall_64 + 0x3d / 0xd0 [ 714.500180 ] [ < ffffffff8100d7ac > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 714.507257 ] [ < ffffffff8100e4ea > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 714.514335 ] < EOT >","title":"04/21 Sat"},{"location":"lego/log/log-04-2018/#0420-fri","text":"Glad TF finally working now! Keep seeing this message from kernel. It have been many many times. Very deterministic. BUG : unable to handle kernel paging request at ffffffffffff8100","title":"04/20 Fri"},{"location":"lego/log/log-04-2018/#0419-thur","text":"Patched clflush to use tgid, n_nid directly without task_struct.","title":"04/19 Thur"},{"location":"lego/log/log-04-2018/#in-a-256m-excache-today-0419-w15-4-a-timeout-happen-first-which-will-be-handled-as-segfault-to-kill-all-threads-in-an-eviction-victim_prepare_hits-the-get_memory_nodes-encounter-the-null-again-looks-like-the-thread_group-mm-got-cleared-before","text":"","title":"In a 256M excache today (0419-w15-4), a timeout happen first, which will be handled as segfault to kill all threads. In an eviction-&gt;victim_prepare_hits, the get_memory_nodes() encounter the NULL again. Looks like the thread_group-&gt;mm got cleared before."},{"location":"lego/log/log-04-2018/#0418-wed","text":"Try best to fix the pipe bug. (I found it by using my old way of debugging. By writing a function that test if PTE is corrupted or not. I put that function around the sycall enter/exit. So it help to find which syscall corrupt memory. I have used this stupid technique to find so many hard-to-find memory corruption bugs.....) do_close_on_exec dup2 Re-read Yutong\u2019s patch again. It touches a lot handler code. This has to be verified before using any nowait reply. pipe\u2019s wakeup may have issue? 0418-w15-41. 39sec","title":"04/18 Wed"},{"location":"lego/log/log-04-2018/#0417-tue","text":"Checking list: -pcache: ibapi use va or pa, does it matter?- No, I change it to use the VA. Then we don\u2019t have the need to use PA reply any more. =ib_mad, does it really corrupt Memory= Still not sure. Should be something come from the ib_poll_cq . M side per PTE lock, check if the lock is really the same lock! -Mail I20. Check CPT.- Dist-VMA First make sure, TF+no-dist-vma work on my own setting. Though sometimes random bug happen (I doubt it is IB). Then turn on dist-vma w/wo zerofill w/wo kfree w/wo all-zero Debug. w/wo M side per PTE lock Change most handlers to use TX buffer. Reduce the random mismatched reply case. P side watchdog patch: what to print -It looks like it is more easier to have bug when I turn on those debug counter printing. I probably should check those buffer mgmt. All next test have zerofill:- w print F 0417-w15-2(rmap_walk list_for_each_entrry #GP) F 0417-w15-3(pcache_copy_page_range corrupted PTE) F 0417-w15-4(fit_poll_cq+0x39 ib_poll_cq() \u2026) F 0417-w15-5(pcache_copy_page_range corrupted PTE) wo strace exit: S 0417-w15-6(each 100 step take ~39s/ Linux is ~34s) S 0417-w15-7(filling shuffle data, that works) F 0417-w15-8(pcache_copy_page_range+0x5d1) F 0417-w15-9(rmap_walk+0x47 #GP) disable strace: F 0417-w15-10(pcache_copy_page_range+0x5d1) Conclusion it has nothing to do with the strace thing. most of them fail around nr_reqs=19103 Why the pcache_copy_page_range always happen, after some fork, execve. w strace (fork, vfork, clone, execve) F 0417-w15-11 (pcache_cp_pg_range). Understand its flow. Back to make sure P side per PTE lock is correct. If it is pcache_cp fault, it always fail at nr_reqs=19103 . And it is: 1) python fork, 2) execve sh. S 0417-w15-12. With global PTE lock. Passed the failed stage above. F 0417-W15-13. With global PTE lock. Failed at pcache_cp. Same place. (Since global PTE lock also fail, so it is not the lock issue. Still someone write to wrong memory.) F 0417-w15-14. With global PTE lock. Same place. Found that I printed a misleading debug info. Modified a little bit to print the actual pte content. Hope can get some valid info next round. F 0417-w15-15. Same place. copy: addr: 0x7fffdca07000, ptecont: 0x8800000000000 . zap: ptent: 0x340 address: 0x7fffdca08000 . F 0417-w15-16. Well. BUG in ib_mad_send handler. I add the same checking in ib_mad_receive. This is really just used to catch it. Not fixing it. F 0417-w15-17. Again, addr: 0x7fffdc207000, ptecont: 0x8800000000000 F 0417-w15-18. addr: 0x7fffdca07000, ptecont: 0x8800000000000 Conclusion Only these two addresses addr: 0x7fffdca07000, ptecont: 0x8800000000000 pte:ffff88103ea87038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000) addr: 0x7fffdc207000, ptecont: 0x8800000000000 pte:ffff88103ea97038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000) Bug found. In pipe_read/write. It somehow corrupted memory. Damn. -Another first thing, check this weird log.. : Hmm, this log should be fine. mad_post is after recv_done_handler. So even if we detect corrupted memory in handler, it has nothing to do with mad_post. The root cause should come from ib_poll_cq, that is where we pass wc to, and where the wc.wr_id was filled in.- [ 3850.911144 ] ib_mad_recv_done_handler () : c1 : 2060 c2 : 12 wc -> wr_id : 0xffff88103eea1398 [ 3850.921881 ] ib_mad_post_receive_mads () : c1 : 2060 c2 : 13 recv_wr . wr_id : 0xffff88103eea1008 recv_queue : ffff88103ee42520 [ 3850.933620 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 13 [ 3850.942346 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 14 [ 3850.951266 ] ib_mad_recv_done_handler () : c1 : 2061 c2 : 13 wc -> wr_id : 0xffff88103eea1560 [ 3850.961999 ] ib_mad_post_receive_mads () : c1 : 2061 c2 : 14 recv_wr . wr_id : 0xffff88103eea11d0 recv_queue : ffff88103ee42520 [ 3850.973737 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 14 [ 3851.257563 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 15 [ 3851.266295 ] ib_mad_recv_done_handler () : c1 : 2062 c2 : 14 wc -> wr_id : 0xffff88103eea1728 [ 3851.277029 ] ib_mad_post_receive_mads () : c1 : 2062 c2 : 15 recv_wr . wr_id : 0xffff88103eea1398 recv_queue : ffff88103ee42520 [ 3851.288767 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 15 [ 3851.297493 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 16 [ 3851.306413 ] ib_mad_recv_done_handler () : c1 : 2063 c2 : 15 wc -> wr_id : 0xffff88103eea18f0 [ 3851.317147 ] ib_mad_post_receive_mads () : c1 : 2063 c2 : 16 recv_wr . wr_id : 0xffff88103eea1560 recv_queue : ffff88103ee42520 [ 3851.328886 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 16 [ 3851.903180 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 17 [ 3851.911913 ] ib_mad_recv_done_handler () : c1 : 2064 c2 : 16 wc -> wr_id : 0xffff88103eea1ab8 [ 3851.922646 ] ib_mad_post_receive_mads () : c1 : 2064 c2 : 17 recv_wr . wr_id : 0xffff88103eea1728 recv_queue : ffff88103ee42520 [ 3851.934384 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 17 [ 3851.943110 ] ib_mad_completion_handler 2383 got successful recv cq op 128 mad_got_one 18 [ 3851.952030 ] ib_mad_recv_done_handler () : c1 : 2065 c2 : 17 wc -> wr_id : 0xffff88103eea1c80 [ 3851.962764 ] ib_mad_post_receive_mads () : c1 : 2065 c2 : 18 recv_wr . wr_id : 0xffff88103eea18f0 recv_queue : ffff88103ee42520 [ 3851.974502 ] ib_mad_completion_handler 2377 got successful send cq op 0 mad_got_one 18 [ 3864.723128 ] *** FIT layer ready to go ! [ 3864.727206 ] *** [ 3867.339488 ] Processor LLC Configurations : [ 3867.343760 ] PhysStart : 0x100000000 [ 3867.348705 ] VirtStart : 0xffff880100000000 [ 3867.354329 ] Registered Size : 0x400000000 [ 3867.359274 ] Actual Used Size : 0x208000000 [ 3867.364219 ] NR cachelines : 2097152 [ 3867.368776 ] Associativity : 8 [ 3867.372751 ] NR Sets : 262144 [ 3867.377210 ] Cacheline size : 4096 B [ 3867.381672 ] Metadata size : 64 B [ 3867.385937 ] NR cacheline bits : 12 [ 0 - 11 ] 0x0000000000000fff [ 3867.392821 ] NR set - index bits : 18 [ 12 - 29 ] 0x000000003ffff000 [ 3867.399705 ] NR tag bits : 34 [ 30 - 63 ] 0xffffffffc0000000 [ 3867.406588 ] NR pages for data : 2097152 [ 3867.411147 ] NR pages for meta : 32768 [ 3867.415509 ] Cacheline ( pa ) range : [ 0x100000000 - 0x2ffffffff ] [ 3867.423848 ] Metadata ( pa ) range : [ 0x300000000 - 0x307ffffff ] [ 3867.432186 ] Cacheline ( va ) range : [ 0xffff880100000000 - 0xffff8802ffffffff ] [ 3867.440524 ] Metadata ( va ) range : [ ffff880300000000 - 0xffff880307ffffff ] [ 3867.448862 ] pcache_set_map ( 064 B ) : [ ffff88207ec00000 - 0xffff88207fbfffff ] [ 3867.457201 ] Way cache stride : 0x40000000 [ 3867.462048 ] Memmap $ semantic : memblock reserved [ 3867.468156 ] NR victim $ entries : 8 [ 3867.472725 ] newpid : 1 home : 1 replica : 1 [ 3867.476980 ] p2m_fork ( cpu0 ) : I cur : 1 - kernel_init new : 20 [ 3867.482718 ] p2m_fork ( cpu0 ) : O succeed cur : 1 - kernel_init new : 20 [ 3867.489197 ] Processor : Processor manager is running . [ 3867.494724 ] Online CPU : 0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 3867.500444 ] Active CPU : 0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 3867.505777 ] [ 0 ] Thread [ kvictim_flushd : 19 ] pinned at CPU 8 [ 3867.511982 ] [ 1 ] Thread [ recvpollcq : 17 ] pinned at CPU 4 [ 3867.539217 ] do_close_on_exec () : TODO , not implemented . [ 3867.549209 ] STDOUT : --- [ Before execv ^ V ] --- [ 3867.553870 ] STDOUT : --- [ e --- [ 3867.557880 ] newpid : 20 home : 1 replica : 1 [ 3867.562248 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 3867.567560 ] p2m_fork ( cpu10 ) : O succeed cur : 20 - exe . o new : 21 [ 3867.573670 ] CPU12 PID21 sys_execve [ 3867.578681 ] do_close_on_exec () : TODO , not implemented . [ 3867.584215 ] CPU12 PID21 sys_execve = 0 , 0x0 [ 3867.599867 ] BUG : unable to handle kernel paging request at 000000040 8446080 [ 3867.607436 ] IP : [ < ffffffff8101bbbf > ] task_tick_rt + 0x1f / 0xd0","title":"04/17 Tue"},{"location":"lego/log/log-04-2018/#0416-mon","text":"Make dist-vma work with TF first. Tough work. 0416-w14-7 : 1) do_wp_page triggered, 2) dealock on per pte lock. This really should not happen. It is single worker. Basically means the page->lock is not intialized. Probabaly our per PTE lock implementation is wrong. [ 5220.250552 ] hb : worker [ 0 ] CPU 4 stucked [ 5220.254819 ] hb : common_header [ op = 0x20000000 src_nid : 0 ] [ 5220.260734 ] hb : msg [ pid = 21 , tgid = 21 , flags = 0x51 , vaddr = 0x7fff7b7fdfb8 ] [ 5220.267911 ] CPU : 4 PID : 31 Comm : thpool - worker0 4.0.0 - lego - ys + # 237 [ 5220.274890 ] RIP : 0010 : [ < ffffffff81031aa3 > ] [ < ffffffff81031aa3 > ] handle_lego_mm_fault + 0x373 / 0x4f0 handle_lego_mm_fault + 0x373 / 0x4ee : arch_spin_lock at arch / x86 / include / asm / spinlock . h : 21 ( inlined by ) spin_lock at include / lego / spinlock . h : 72 ( inlined by ) do_anonymous_page at managers / memory / vm / fault . c : 115 ( inlined by ) handle_pte_fault at managers / memory / vm / fault . c : 142 ( inlined by ) handle_lego_mm_fault at managers / memory / vm / fault . c : 225 A IB bug during normal run (P M S TF), this is REALLY weird: [ 395.259560 ] CPU12 PID21 sys_execve [ 395.263345 ] BUG : unable to handle kernel NULL pointer dereference at 00000000000001 a0 [ 395.272068 ] IP : [ < ffffffff81064c09 > ] fit_poll_cq + 0x39 / 0x530 fit_poll_cq + 0x39 / 0x523 : git :( test_vma )] $ . / scripts / faddr2line vmImage fit_poll_cq + 0x39 ib_poll_cq at include / rdma / ib_verbs . h : 1614 ( inlined by ) fit_poll_cq at net / lego / fit_internal . c : 1671 Catch the ib_mad bug once.. and mlx4_error follows. I added more checking to where the mad_queue was assigned. [ 787.471385 ] ib_mad_completion_handler 2365 got successful recv cq op 128 mad_got_one 15 [ 787.480124 ] BUG ! mad_list : ffff88103eea1728 mad_queue : ( null ) [ 787.487491 ] ------------ [ cut here ] ------------ [ 787.492630 ] WARNING : CPU : 0 PID : 15 at drivers / infiniband / core / mad . c : 1909 ib_mad_completion_handler + 0xa56 / 0xab0","title":"04/16 Mon"},{"location":"lego/log/log-04-2018/#0415-sun","text":"Trying TF myself. Had a bug report on 0415-w15-5, on fork, execve etc. [ 317.436811 ] newpid : 22 home : 1 replica : 1 [ 317.477701 ] pte : ffff88103e94a038 pfn : 0x0 flags :() [ 317.482752 ] pte dumped because : corrupted [ 317.487213 ] ------------ [ cut here ] ------------ [ 317.492352 ] WARNING : CPU : 14 PID : 22 at managers / processor / pgtable . c : 365 pcache_copy_page_range + 0x5d1 / 0x6c0 [ 317.503213 ] CPU : 14 PID : 22 Comm : python 4.0.0 - lego + # 93 [ 317.552082 ] Call Trace : [ 317.554799 ] < TSK > [ 317.556930 ] [ < ffffffff810123a1 > ] __warn . constprop .0 + 0x91 / 0xd0 [ 317.563330 ] [ < ffffffff8101246f > ] warn_slowpath_null + 0xf / 0x20 [ 317.569634 ] [ < ffffffff8102d401 > ] pcache_copy_page_range + 0x5d1 / 0x6c0 [ 317.576615 ] [ < ffffffff81037ed7 > ] fork_dup_pcache + 0x27 / 0x30 [ 317.582723 ] [ < ffffffff8101e514 > ] copy_process + 0xcf4 / 0x1140 [ 317.588833 ] [ < ffffffff8101e986 > ] do_fork + 0x26 / 0x160 [ 317.594264 ] [ < ffffffff8101eb89 > ] sys_clone + 0x29 / 0x30 [ 317.599789 ] [ < ffffffff8100e66d > ] do_syscall_64 + 0x3d / 0xd0 [ 317.605705 ] [ < ffffffff8100d56c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 317.612782 ] < EOT > [ 317.614917 ] --- [ end trace 0000000000000000 ] --- [ 317.625561 ] p2m_fork ( cpu14 ) : I cur : 22 - python new : 36 [ 330.209312 ] p2m_fork ( cpu14 ) : O succeed cur : 22 - python new : 36 [ 330.310909 ] ------------ [ cut here ] ------------ [ 330.315864 ] BUG : failure at managers / processor / pcache / rmap . c : 804 / pcache_zap_pte () ! [ 330.324302 ] Kernel Panic - not syncing : BUG ! [ 330.329050 ] CPU : 0 PID : 36 Comm : python 4.0.0 - lego + # 93 [ 330.377824 ] Call Trace : [ 330.380540 ] < TSK > [ 330.382672 ] [ < ffffffff81026493 > ] panic + 0xc2 / 0x105 [ 330.387908 ] [ < ffffffff8101bbcc > ] ? task_tick_rt + 0x2c / 0xd0 [ 330.393920 ] [ < ffffffff81019245 > ] ? scheduler_tick + 0x55 / 0x60 [ 330.400126 ] [ < ffffffff810168f5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 330.406913 ] [ < ffffffff81006684 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 330.413700 ] [ < ffffffff8100e2aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 330.420973 ] [ < ffffffff810125ad > ] ? printk + 0x11d / 0x1b0 [ 330.426597 ] [ < ffffffff810375bc > ] pcache_zap_pte + 0x14c / 0x190 [ 330.432802 ] [ < ffffffff81035db0 > ] ? __pcache_remove_rmap_one + 0x70 / 0x70 [ 330.439978 ] [ < ffffffff8102cd25 > ] unmap_page_range + 0x325 / 0x3f0 [ 330.446379 ] [ < ffffffff8102ce0e > ] release_pgtable + 0x1e / 0x40 [ 330.452487 ] [ < ffffffff81037ef8 > ] pcache_process_exit + 0x18 / 0x20 [ 330.458984 ] [ < ffffffff8101d3c4 > ] mmput + 0x34 / 0xb0 [ 330.464123 ] [ < ffffffff8102c38d > ] do_execve + 0x42d / 0x760 [ 330.469845 ] [ < ffffffff8102c6c9 > ] sys_execve + 0x9 / 0x10 [ 330.475371 ] [ < ffffffff8100e66d > ] do_syscall_64 + 0x3d / 0xd0 [ 330.481286 ] [ < ffffffff8100d56c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 [ 330.488364 ] < EOT > [ 330.490501 ] --- [ end Kernel panic - not syncing : BUG ! one more [ 369.223161] newpid: 22 home:1 replica: 1 [ 369.264307] pte:ffff88103ea41038 (0x0) pfn:0x0 flags:() [ 369.269938] pte dumped because: corrupted [ 369.274399] ------------[ cut here ]------------ [ 369.279538] WARNING: CPU: 14 PID: 22 at managers/processor/pgtable.c:365 pcache_copy_page_range+0x5d1/0x6c0 [ 369.290398] CPU: 14 PID: 22 Comm: python 4.0.0-lego+ #94 [ 369.296310] Stack: [ 369.341976] <TSK> [ 369.344107] [<ffffffff810123a1>] __warn.constprop.0+0x91/0xd0 [ 369.350508] [<ffffffff8101246f>] warn_slowpath_null+0xf/0x20 [ 369.356809] [<ffffffff8102d401>] pcache_copy_page_range+0x5d1/0x6c0 [ 369.363790] [<ffffffff81037f07>] fork_dup_pcache+0x27/0x30 [ 369.369897] [<ffffffff8101e514>] copy_process+0xcf4/0x1140 [ 369.376006] [<ffffffff8101e986>] do_fork+0x26/0x160 [ 369.381435] [<ffffffff8101eb89>] sys_clone+0x29/0x30 [ 369.386960] [<ffffffff8100e66d>] do_syscall_64+0x3d/0xd0 [ 369.392875] [<ffffffff8100d56c>] entry_SYSCALL64_slow_path+0x25/0x25 [ 369.399952] <EOT> [ 369.402086] ---[ end trace 0000000000000000 ]--- [ 369.412750] p2m_fork(cpu14): I cur:22-python new:36 [ 369.418215] p2m_fork(cpu14): O succeed cur:22-python new:36 [ 369.500829] ptent: 0x340 address: 0x7fffe2408000 [ 369.505783] pte:ffff88103dbe5040 (0x340) pfn:0x0 flags:(dirty|global|softw1) [ 369.513637] pte dumped because: corrupted [ 369.518095] ------------[ cut here ]------------ [ 369.523236] BUG: failure at managers/processor/pcache/rmap.c:808/pcache_zap_pte()! [ 369.531672] Kernel Panic - not syncing: BUG!","title":"04/15 Sun"},{"location":"lego/log/log-04-2018/#0414-sat","text":"Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm. Will they do harm Implement replication Add IB counter","title":"04/14 Sat"},{"location":"lego/log/log-04-2018/#0413-fri","text":"Patched M side pgtable to use per PTE/PMD lock. So thpool in M will not be bottlnecked by the page_table_lock.","title":"04/13 Fri"},{"location":"lego/log/log-04-2018/#ib_mad_recv_done_handler-may-corrupt-memory-again","text":"Somehow, during testing of this patch. Running with MT-Phoenix 1GB, the P side has reported bad pgd entries. I\u2019m using fork+execve way. The child(phoenix) already exit. This msg is printed when parent exit_mm. The pgd table should either be 0, or valid pud va. Memory corruption happened\u2026 [ 2551.687806 ] Kernel strace [ 2551.690715 ] Task : 21 : 21 nr_accumulated_threads : 1 [ 2551.696327 ] % time seconds usecs / call calls errors syscall [ 2551.703704 ] ------ -------------- ----------- --------- --------- ---------------- [ 2551.712141 ] 98.63 66.942660568 66942661 1 0 sys_wait4 [ 2551.719898 ] 0.45 0.457060789 457061 1 0 sys_clone [ 2551.727654 ] 0.20 0.204320071 51081 4 0 sys_brk [ 2551.735216 ] 0.40 0.040378189 40379 1 0 sys_mmap [ 2551.742876 ] 0.13 0.013682424 4561 3 0 sys_write [ 2551.750633 ] 0.10 0.000001039 2 1 0 sys_newfstat [ 2551.758681 ] 0.88 0.000000888 1 2 0 sys_rt_sigaction [ 2551.767114 ] 0.79 0.000000792 1 2 0 sys_futex [ 2551.774871 ] 0.77 0.000000770 1 1 0 sys_rt_sigprocmask [ 2551.783501 ] 0.54 0.000000548 1 1 0 sys_arch_prctl [ 2551.791742 ] 0.49 0.000000499 1 1 0 sys_newuname [ 2551.799789 ] 0.46 0.000000469 1 1 0 sys_getrlimit [ 2551.807933 ] 0.19 0.000000195 1 1 0 sys_set_tid_address [ 2551.816659 ] 0.19 0.000000190 1 1 0 sys_set_robust_list [ 2551.825386 ] 0.18 0.000000181 1 1 0 sys_ioctl [ 2551.833143 ] ------ -------------- ----------- --------- --------- ---------------- [ 2551.841577 ] 100.00 67.658107612 22 0 total [ 2551.848945 ] [ 2551.850591 ] [ 2551.852240 ] Kernel Profile Points [ 2551.855924 ] status name total nr avg . ns [ 2551.865621 ] ------- -------------------- ---------------- ---------------- ---------------- [ 2551.875317 ] off flush_tlb_others 0.000204992 58 3535 [ 2551.885014 ] off __do_kmalloc_node 0.300783843 281501 1069 [ 2551.894709 ] off __pcache_zerofill 0.009844770 16558 595 [ 2551.904404 ] off pcache_miss 54.414457906 257869 211016 [ 2551.914100 ] off pcache_flush 0.000000000 0 0 [ 2551.923795 ] ------- -------------------- ---------------- ---------------- ---------------- [ 2551.933490 ] [ 2552.074985 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956028 ( ffffffff81146ca0 ) [ 2552.084206 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956030 ( ffff88103e956030 ) [ 2552.093611 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956038 ( ffff88103e956030 ) [ 2552.103016 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956048 ( ffff88103cc48740 ) [ 2552.112421 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956050 ( 00000000000001 c0 ) [ 2552.121825 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956058 ( ffff88103eea2008 ) [ 2552.131230 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956060 ( ffff88103eea17d8 ) [ 2552.140635 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956068 ( ffff88103ee42520 ) [ 2552.150040 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9560e8 ( 000000103e9560 f0 ) [ 2552.159444 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956118 ( 010200 8081018101 ) [ 2552.168849 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956120 ( 3 c010b0012000000 ) [ 2552.178254 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956128 ( 0000000000001100 ) [ 2552.187659 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956138 ( 00000000ff ffffff ) [ 2552.197064 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956158 ( 0307 8 a2402010101 ) [ 2552.206467 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956160 ( 0307 8 a2453946600 ) [ 2552.215872 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956168 ( 0307 8 a2450946600 ) [ 2552.225277 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956170 ( 0310 800051946600 ) [ 2552.234682 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956178 ( c902000100000000 ) [ 2552.244088 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956198 ( bfd0cc054a122000 ) [ 2552.253492 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561a0 ( 0000000000 98 b9c8 ) [ 2552.262897 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561a8 ( bfe0fe0610914e01 ) [ 2552.272302 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561b0 ( 000000000050f 2 c7 ) [ 2552.281706 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561b8 ( bfd9a30000ec5100 ) [ 2552.291111 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561c0 ( bffc91d40f20f2c7 ) [ 2552.300516 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561c8 ( 0f 20 cd054a20f2c7 ) [ 2552.309920 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561d0 ( 1094 edcf0f60edcf ) [ 2552.319325 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e9561d8 ( 0000000000000100 ) [ 2552.328730 ] . / arch / x86 / include / asm / pgtable . h : 579 : bad pgd ffff88103e956218 ( 0000000000005 aa5 ) [ 2552.338151 ] nr_pgfault : 26 Second run, saw this invalid pointer deference again! Combined with the above log, I think ib_mad is definitely corrupting memory! I have to take a look. qp_info = mad_list->mad_queue->qp_info;","title":"ib_mad_recv_done_handler may corrupt memory, again."},{"location":"lego/log/log-04-2018/#patching-the-handlers-to-use-tx-buffer","text":"Patched. Once race condition: pcache_handle_miss use the page itself as the reply buffer. Assume later on, it changes to use nowait reply. When the reply is buffered in the queue and has not been sent. Another munmap comes in and invalidate this area, then the page will be freed. The data is invalidate. But this case seems abnormal. The application will not do so I guess.","title":"Patching the handlers to use tx buffer."},{"location":"lego/log/log-04-2018/#check-if-page-table-pages-page-themselves-are-freed-in-munmap-at-both-p-and-m-need-to-confirm","text":"","title":"Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm."},{"location":"lego/log/log-04-2018/#tonight-task-think-about-how-to-do-the-vma-replication-how-to-combine-with-the-line-replicaiton","text":"","title":"Tonight task. Think about how to do the VMA replication, how to combine with the $ line replicaiton."},{"location":"lego/log/log-04-2018/#0412-thur","text":"Patched zerofill. All done. Testing new driver fix with Phoenix - 1 st run, the mismatch reply is still there. mmap() replied address is different from the one printed. So segfault follows. (0412-w15-4) - 2st run, 3st run, succeed. 0412-w15-9 0412-w14-9 First time testing phoenix with zerofill (no net). Somehow, P has pcache timeout, but M\u2019s watchdog show there is no pending requests. This happen once before I remember\u2026 0412-w15-10. Have not seen this ib mad thing for a long time. Indeed somewhere is wrong. [ 297.794969] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15 [ 297.803706] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020 [ 297.812431] IP: [<ffffffff81058937>] ib_mad_completion_handler+0xc7/0x810 2","title":"04/12 Thur"},{"location":"lego/log/log-04-2018/#0411-wed","text":"Adding anon first touch opt. 0411-p/m-9 : this log indicate M does not have any unhandled requests, but P side has 1 __pcache_fill timeout. It seems the message is lost somewhere. 0411-p/m-11 : catch one with the debug msg Yiying added. She says the M side send queue has 2 reqs. But poll does not return any error. Weird. Help debugging IB issue.","title":"04/11 Wed"},{"location":"lego/log/log-04-2018/#0410-tue","text":"Found. IB stuck. Damn. [ 2240.294960] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2242.694733] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2245.094524] RIP: 0010:[<ffffffff8104a6e3>] [<ffffffff8104a6e3>] mlx4_ib_poll_cq+0x383/0x6a0 [ 2247.494306] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2249.894088] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2252.293870] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2254.693651] RIP: 0010:[<ffffffff8104a6d8>] [<ffffffff8104a6d8>] mlx4_ib_poll_cq+0x378/0x6a0 [ 2257.093431] RIP: 0010:[<ffffffff8104a6e3>] [<ffffffff8104a6e3>] mlx4_ib_poll_cq+0x383/0x6a0","title":"04/10 Tue"},{"location":"lego/log/log-04-2018/#0409-mon","text":"thpool testing. 4 workers. MT-phoenix: [ root @ wuklab05 ys ] # cat 040 9 - p | grep __munmap [ 227.054974 ] CPU14 PID22 strace__munmap ([ 0x7fffb0ba9000 - 0x7fffb4000000 ], 54882304 ) = 0 , 0x0 [ 227.093466 ] CPU16 PID23 strace__munmap ([ 0x7fffab7ff000 - 0x7fffac000000 ], 8392704 ) = 0 , 0x0 [ 227.102773 ] CPU14 PID22 strace__munmap ([ 0x7fffb8000000 - 0x7fffb8ba9000 ], 12226560 ) = 0 , 0x0 [ 227.141265 ] CPU18 PID24 strace__munmap ([ 0x7fffa8000000 - 0x7fffac000000 ], 67108864 ) = 0 , 0x0 [ 227.150669 ] CPU16 PID23 strace__munmap ([ 0x7fffb0000000 - 0x7fffb37ff000 ], 58716160 ) = 0 , 0x0 [ 227.218248 ] CPU22 PID26 strace__munmap ([ 0x7fffa0000000 - 0x7fffa4000000 ], 67108864 ) = 0 , 0x0 [ 227.285826 ] CPU2 PID28 strace__munmap ([ 0x7fff98000000 - 0x7fff9c000000 ], 67108864 ) = 0 , 0x0 [ 227.440567 ] CPU14 PID31 strace__munmap ([ 0x7fff8a7fd000 - 0x7fff8c000000 ], 25178112 ) = 0 , 0x0 [ 227.449972 ] CPU12 PID30 strace__munmap ([ 0x7fff88000000 - 0x7fff8c000000 ], 67108864 ) = 0 , 0x0 [ 227.459376 ] CPU14 PID31 strace__munmap ([ 0x7fff90000000 - 0x7fff927fd000 ], 41930752 ) = 0 , 0x0 [ 227.490109 ] CPU18 PID33 strace__munmap ([ 0x7fff80000000 - 0x7fff84000000 ], 67108864 ) = 0 , 0x0 [ 227.723140 ] word_count - pthr [ 29 ] : segfault at 0x4e842010 ip 0000000000420354 sp 00007ff fb17f9bc0 error 6 0x4e842010 Print mmap on M, if segfault. Printed, the 0x4e842010 is never a valid address. thpool makes Memory side SMP. Probably bring some issues. Found: P CPU22 PID26 strace__mmap(addr=0x0, len=0xfb000, prot(0x3)=PROT_READ|PROT_WRITE, flags(0x22)=MAP_PRIVATE|MAP_ANONYMOUS, fd=18446744073709551615( ), off=0x0) = 1317351432, 0x4e853008 word_count-pthr[26]: segfault at 0x4e853010 ip 0000000000420354 sp 00007fff972a8bc0 error 6 M [ 583.120615] 00400000-004d9000 r-xp 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 583.131578] 006d9000-006dc000 rw-p 000d9000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread [ 583.142729] 006dc000-00755000 rw-p 00000000 [heap] [ 583.148254] 7fff529c9000-7fffb93aa000 rw-p 00000000 [ 583.153974] 7fffb93aa000-7ffff7fff000 rw-p 00000000 /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_1GB.txt [ 583.167355] 7ffffffde000-7ffffffff000 rw-p 00000000 [stack] [ 583.173753] ------------[ cut here ]------------ [ 583.178892] WARNING: CPU: 4 PID: 31 at managers/memory/handle_pcache/fault.c:55 handle_p2m_pcache_miss+0x18e/0x1d0 [ 583.190430] src_nid:0,pid:21,vaddr:0x4e853010 [ 583.195279] CPU: 4 PID: 31 Comm: thpool-worker0 4.0.0-lego-ys+ #90 Confirmed. I printed added a number to mmap requests. And the compare the results of both P and M. The data is wrong. Btw, I\u2019m only running 1 worker thread at M, which makes it single thread handling. So, I\u2019m going to, 1) first use kmalloc to get the reply buffer, and 2) revert back the IB MAX_OUT config, remove the #ifdef COMP_MEMORY. See if it is this patch\u2019s issue. P : CPU18 PID24 strace__mmap ( 30 ..) = - 1325940736 , 0x7fffb0f7c000 CPU22 PID26 strace__mmap ( 31 ..) = 2144269992 , 0x7fcef6a8 M : ... handle_p2m_mmap () : 30 7ff fb0f7c000 handle_p2m_mmap () : 31 7ff fb0efe000 ... Anyway, this is temporary fixed by using kmalloced reply buffer. Spent whole afternoon and whole night. Finally figure out why timeout happen in P. It is because somewhere in the middle, M has 1 or more requests stucked/unhandled. Deadlock happen in the middle. Like this one. 5 requests queued waiting, 1 is being handled. And that 1 handler stuck. And it is handle_pcache_miss. Now, I need to find out where it stuck! thpool-worker0 nr_queued: 5 1 Oh, I really hope we can have some soft/hw lockdep, watchdog stuff . This should make out life much much much much much easier!","title":"04/09 Mon"},{"location":"lego/log/log-04-2018/#0408-sun","text":"Trying the fit_nowait patch. First try fit_nowait patch, without any chanegs to other code. See if this patch can work. Second, modify pcache to use reply_message_nowait. See if this can work. and improve performance. Third, if 2) can improve, perf. Move on to modify thpool patch. 1 st , P fail at ib_mad, during boot: [ 349.239220 ] Online CPU : 0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 349.244940 ] Active CPU : 0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22 [ 349.250272 ] [ 0 ] Thread [ kvictim_flushd : 19 ] pinned at CPU 8 [ 349.256478 ] [ 1 ] Thread [ recvpollcq : 17 ] pinned at CPU 4 [ 356.188819 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 13 [ 356.197545 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 356.206270 ] IP : [ < ffffffff81058287 > ] ib_mad_completion_handler + 0xc7 / 0x810 2st run, P side, config MAX_OUT to 1. Then single-thread pheonix with 1GB data finished. But forgot to turn on the profile point. Run one more time. 3st run. Same with 2st run setting. But with profile on. Bug shows. Ugh. I still think it is because of ib_mad_handler. It must write to wrong memory locations, and corrupt things randomly. [ 456.237913 ] do_close_on_exec () : TODO , not implemented . ... [ 456.263274 ] BUG : unable to handle kernel paging request at 00000002f 4 bfbf58 [ 456.270843 ] IP : [ < ffffffff8101bbff > ] task_tick_rt + 0x1f / 0xd0 [ 456.277048 ] PGD 0 [ 456.279279 ] Thread overran stack , or stack corrupted [ 456.284804 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 456.289265 ] CPU : 10 PID : 20 Comm : kevict_sweepd 4.0.0 - lego + # 40 [ 456.295858 ] RIP : 0010 : [ < ffffffff8101bbff > ] [ < ffffffff8101bbff > ] task_tick_rt + 0x1f / 0xd0 4st run, succeed. But it looks like the perf is very bad. Oh. but 99% of the pcache miss are file-backed, which will go to storage. So the number is actually doubled. With fit_nowait patch : [ 308.660051 ] Kernel Profile Points [ 308.663734 ] status name total nr avg . ns [ 308.673431 ] ------- -------------------- ---------------- ---------------- ---------------- [ 308.683128 ] off flush_tlb_others 0.000130715 53 2467 [ 308.692824 ] off __do_kmalloc_node 0.097344056 265647 367 [ 308.702521 ] off pcache_miss 4.504660891 258211 17446 [ 308.712218 ] off pcache_flush 0.000000000 0 0 [ 308.721914 ] ------- -------------------- ---------------- ---------------- ---------------- 5st run. Just run large malloc test. Looks better than yesterday\u2019s result. But I\u2019m using 15 as P today, instead of 13. So, let me try one more time to see if it is the machine. With fit_nowait patch : [ 674.382592 ] Kernel Profile Points [ 674.386277 ] status name total nr avg . ns [ 674.395974 ] ------- -------------------- ---------------- ---------------- ---------------- [ 674.405670 ] off flush_tlb_others 0.000130838 53 2469 [ 674.415366 ] off __do_kmalloc_node 1.604700641 1584917 1013 [ 674.425062 ] off pcache_miss 6.467938547 786571 8223 [ 674.434758 ] off pcache_flush 3.342783614 262225 12748 [ 674.444455 ] ------- -------------------- ---------------- ---------------- ---------------- [ 674.554497 ] nr_pgfault : 786513 [ 674.557706 ] nr_clflush : 262225 [ 674.561099 ] nr_pgfault_wp : 0 [ 674.564299 ] nr_pgfault_wp_cow : 0 [ 674.567887 ] nr_pgfault_wp_reuse : 0 [ 674.571668 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 674.577195 ] nr_pcache_fill_from_memory : 786511 [ 674.582139 ] nr_pcache_fill_from_victim : 2 6st run. Looks like the above fit_nowait can have 400ns improvement. But how come? I did not even change the pcache handling to use ibapi_nowait!!! Maybe random variation. Let me run more. Without fit_nowait patches [ 428.546738 ] Kernel Profile Points [ 428.550424 ] status name total nr avg . ns [ 428.560119 ] ------- -------------------- ---------------- ---------------- ---------------- [ 428.569815 ] off flush_tlb_others 0.000131140 53 2475 [ 428.579510 ] off __do_kmalloc_node 1.758704197 1331927 1321 [ 428.589205 ] off pcache_miss 6.807601189 786575 8655 [ 428.598899 ] off pcache_flush 3.699044847 262227 14107 [ 428.608594 ] ------- -------------------- ---------------- ---------------- ---------------- [ 428.618289 ] [ 428.718670 ] nr_pgfault : 786515 [ 428.721878 ] nr_clflush : 262227 [ 428.725272 ] nr_pgfault_wp : 0 [ 428.728470 ] nr_pgfault_wp_cow : 0 [ 428.732058 ] nr_pgfault_wp_reuse : 0 [ 428.735840 ] nr_pgfault_due_to_concurrent_eviction : 0 [ 428.741365 ] nr_pcache_fill_from_memory : 786515 [ 428.746310 ] nr_pcache_fill_from_victim : 0 7 th run. without fit_nowait. without fit_nowait . [ 901.223090 ] Kernel Profile Points [ 901.226775 ] status name total nr avg . ns [ 901.236472 ] ------- -------------------- ---------------- ---------------- ---------------- [ 901.246168 ] off flush_tlb_others 0.000130802 53 2468 [ 901.255865 ] off __do_kmalloc_node 1.862575608 1331923 1399 [ 901.265560 ] off pcache_miss 6.814540477 786572 8664 [ 901.275257 ] off pcache_flush 3.699187003 262224 14107 [ 901.284953 ] ------- -------------------- ---------------- ---------------- ---------------- 8 th run. without fit_nowait. [ 321.514564 ] Kernel Profile Points [ 321.518250 ] status name total nr avg . ns [ 321.527945 ] ------- -------------------- ---------------- ---------------- ---------------- [ 321.537639 ] off flush_tlb_others 0.000130934 53 2471 [ 321.547335 ] off __do_kmalloc_node 2.216772665 1331939 1665 [ 321.557031 ] off pcache_miss 6.806060415 786573 8653 [ 321.566726 ] off pcache_flush 3.725455841 262231 14207 [ 321.576421 ] ------- -------------------- ---------------- ---------------- ---------------- 9 th run. with fit_nowait [ 374.847912 ] Kernel Profile Points [ 374.851597 ] status name total nr avg . ns [ 374.861293 ] ------- -------------------- ---------------- ---------------- ---------------- [ 374.870989 ] off flush_tlb_others 0.000130858 53 2470 [ 374.880684 ] off __do_kmalloc_node 1.485304454 1331934 1116 [ 374.890381 ] off pcache_miss 6.615317677 786582 8411 [ 374.900076 ] off pcache_flush 3.508328900 262234 13379 [ 374.909772 ] ------- -------------------- ---------------- ---------------- ---------------- 10 th run, with fit_nowait [ 225.211058] Kernel Profile Points [ 225.214743] status name total nr avg.ns [ 225.224440] ------- -------------------- ---------------- ---------------- ---------------- [ 225.234137] off flush_tlb_others 0.000131029 53 2473 [ 225.243833] off __do_kmalloc_node 1.211421872 1331984 910 [ 225.253529] off pcache_miss 6.583096125 786574 8370 [ 225.263226] off pcache_flush 3.464430818 262227 13212 [ 225.272922] ------- -------------------- ---------------- ---------------- ---------------- Sum: with fit_nowait: [ 225.253529] off pcache_miss 6.583096125 786574 8370 [ 225.263226] off pcache_flush 3.464430818 262227 13212 [ 374.890381] off pcache_miss 6.615317677 786582 8411 [ 374.900076] off pcache_flush 3.508328900 262234 13379 [ 674.425062] off pcache_miss 6.467938547 786571 8223 [ 674.434758] off pcache_flush 3.342783614 262225 12748 Without fit_nowait: [ 428.589205] off pcache_miss 6.807601189 786575 8655 [ 428.598899] off pcache_flush 3.699044847 262227 14107 [ 901.265560] off pcache_miss 6.814540477 786572 8664 [ 901.275257] off pcache_flush 3.699187003 262224 14107 [ 321.557031] off pcache_miss 6.806060415 786573 8653 [ 321.566726] off pcache_flush 3.725455841 262231 14207","title":"04/08 Sun"},{"location":"lego/log/log-04-2018/#0407-sat","text":"Well, now we finished all the profiling stuff. Continue on other work. Now I like listening Jazz while coding. Amazing Jazz, really good. Once again, ib_mad_completion_handler bug will happen. During application run, or even after application exit. [ 465.835447 ] nr_mremap_pset_diff : 0 [ 477.086886 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21 [ 477.095620 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 477.104345 ] IP : [ < ffffffff81058277 > ] ib_mad_completion_handler + 0xc7 / 0x810 ib_mad_completion_handler + 0xc7 / 0x808 : ib_mad_recv_done_handler at drivers / infiniband / core / mad . c : 1899 ( inlined by ) ib_mad_completion_handler at drivers / infiniband / core / mad . c : 2345 After remove net from pcache miss: [ 465.572131 ] Kernel Profile Points [ 465.575815 ] status name total nr avg . ns [ 465.585510 ] ------- -------------------- ---------------- ---------------- ---------------- [ 465.595206 ] off flush_tlb_others 0.000000000 0 0 [ 465.604901 ] off __do_kmalloc_node 0.656371295 1762220 373 [ 465.614597 ] off pcache_miss 7.172572671 786596 9119 [ 465.624291 ] off pcache_flush 3.698294960 262251 14103 [ 465.633987 ] ------- -------------------- ---------------- ---------------- ---------------- After remove net from pcache flush: [ 684.984000 ] Kernel Profile Points [ 684.987683 ] status name total nr avg . ns [ 684.997379 ] ------- -------------------- ---------------- ---------------- ---------------- [ 685.007074 ] off flush_tlb_others 0.000000000 0 0 [ 685.016770 ] off __do_kmalloc_node 0.627372836 1500543 419 [ 685.026464 ] off pcache_miss 7.128702028 786596 9063 [ 685.036159 ] off pcache_flush 3.660772506 262251 13960 [ 685.045855 ] ------- -------------------- ---------------- ---------------- ---------------- malloc, miss, flush are too slow. Especially the flush, how can it take 13.9us? It must be our handlers! lego_copy_to_user stuff.","title":"04/07 Sat"},{"location":"lego/log/log-04-2018/#0406-fri","text":"Well. Now we have in-kernel strace, in-kernel readprofile. Yummy.","title":"04/06 Fri"},{"location":"lego/log/log-04-2018/#0405-thur","text":"Discussion with Yilun. 1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure. 2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch. 3. I probably want to use per-cpu counter. Anyway, continue strace work first. Finished.","title":"04/05 Thur"},{"location":"lego/log/log-04-2018/#0404-wed","text":"","title":"04/04 Wed"},{"location":"lego/log/log-04-2018/#strace-performance","text":"TF has very bad performance. It is either due to the syscall or pcache. Now I\u2019m adding facilities to track syscall activities, including average latency, total time. Basic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping. But this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity. Oh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime. So the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it. Yeh! Let us do this tomorrow. We will have a robust kernel version strace.","title":"STRACE Performance"},{"location":"lego/log/log-04-2018/#sm-heartbeat","text":"Continue run some experiments on yesterday\u2019s case. One we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval: [ 1224.034898 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15 [ 1224.130616 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 15 [ 1224.222189 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 16 [ 1224.417181 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 16 [ 1393.159845 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 17 [ 1393.255546 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 17 [ 1393.347132 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 18 [ 1393.538972 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 18 [ 1449.437542 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 19 [ 1449.533248 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 19 [ 1449.624833 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 20 [ 1449.722512 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 20 [ 4322.423624 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21 [ 4322.519328 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 21 [ 4322.610914 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 22 [ 4322.708594 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 22 [ 4350.750574 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 23 [ 4350.846278 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 23 [ 4350.937863 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 24 [ 4351.035543 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 24 [ 4519.690559 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 25 [ 4519.786262 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 25 [ 4519.877848 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 26 [ 4519.975527 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 26 [ 4576.396279 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 27 [ 4576.491979 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 27 [ 4576.583565 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 28 [ 4576.681245 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 28 [ 4942.886820 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 29 [ 4942.982523 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 29 [ 4943.074108 ] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 30 [ 4943.171789 ] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 30","title":"SM Heartbeat"},{"location":"lego/log/log-04-2018/#0403-tue","text":"","title":"04/03 Tue"},{"location":"lego/log/log-04-2018/#bug-bug-bug","text":"Finished basic replication mechanism last night. Today merged several patches. And both Yilun and I think there is something wrong with ib_mad_completion_handler . It seems it will break things behind our back. This is one bug catched today:","title":"BUG BUG BUG"},{"location":"lego/log/log-04-2018/#ib_mad_completion_handler","text":"At very early stage : [ 1174.406177 ] newpid : 20 home : 1 replica : 1 [ 1174.452983 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 1177.462795 ] ib_mad_completion_handler 2324 got successful recv cq op 128 mad_got_one 22 [ 1177.556502 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000020 [ 1177.650101 ] IP : [ < ffffffff81059104 > ] ib_mad_completion_handler + 0xb4 / 0x8a0 . / scripts / faddr2line vmImage ib_mad_completion_handler + 0xb4 ib_mad_completion_handler + 0xb4 / 0x899 : ib_mad_recv_done_handler at drivers / infiniband / core / mad . c : 1899 ( inlined by ) ib_mad_completion_handler at drivers / infiniband / core / mad . c : 2325 ib_mad_recv_done_handler () : 1899 : qp_info = mad_list -> mad_queue -> qp_info ; A more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4: [ 863.887705 ] p2m_fork ( cpu10 ) : I cur : 20 - exe . o new : 21 [ 868.478424 ] p2m_fork ( cpu10 ) : O succeed cur : 20 - exe . o new : 21 [ 868.541991 ] BUG : unable to handle kernel NULL pointer dereference at 000000000000000 8 [ 868.635569 ] IP : [ < ffffffff810656d4 > ] __schedule + 0x94 / 0x1e0 [ 868.701090 ] PGD 0 [ 868.725010 ] general protection fault : 0000 [ # 1 ] SMP PROCESSOR [ 868.793651 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 737 Source : clear_tsk_need_resched ( prev ); Even this one for Phoenix: [ 763.442043 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000010 [ 763.535636 ] IP : [ < ffffffff81018d6f > ] task_curr + 0xf / 0x30 [ 763.598035 ] PGD 103e956067 PUD 103e964067 PMD 0 [ 763.653154 ] Oops : 0000 [ # 1 ] SMP PROCESSOR [ 763.700992 ] CPU : 12 PID : 21 Comm : word_count - pthr 4.0.0 - lego - ys + # 740 [ 763.777950 ] RIP : 0010 : [ < ffffffff81018d6f > ] [ < ffffffff81018d6f > ] task_curr + 0xf / 0x30 This NEVER happen before. And this part of code should be correct. We\u2019ve ran a lot things.. I doubt if recent IB merge corrupt things.","title":"ib_mad_completion_handler"},{"location":"lego/log/log-04-2018/#fit_poll_cq","text":"Another one: [ 690.401626 ] stat : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt [ 690.507742 ] SYSC_close () CPU12 PID : 21 [ fd : 4 ] -> [ / sys / devices / system / cpu / online ] [ 713.899884 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 21 [ 713.995606 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 21 [ 714.087185 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 22 [ 714.184871 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 22 [ 742.078102 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 23 [ 742.173810 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 23 [ 742.265399 ] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 24 [ 742.363085 ] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 24 [ 847.063372 ] mlx4_ib_handle_error_cqe syndrome 21 [ 847.116511 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.170590 ] send request failed at connection 7 as 12 [ 847.230909 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.284988 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.339067 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.393146 ] fit_poll_cq : failed status ( 5 ) for wr_id 1832 [ 847.457624 ] fit_poll_cq : failed status ( 5 ) for wr_id 1833 [ 847.522103 ] fit_poll_cq : connection 7 Recv weird event as - 1 [ 847.589701 ] fit_poll_cq : failed status ( 5 ) for wr_id 1834 [ 847.654179 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 847.725938 ] fit_poll_cq : failed status ( 5 ) for wr_id 1835 [ 847.790416 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 847.862174 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.916252 ] mlx4_ib_handle_error_cqe syndrome 5 [ 847.970331 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.024410 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.078490 ] fit_poll_cq : failed status ( 5 ) for wr_id 1836 [ 848.142967 ] fit_poll_cq : failed status ( 5 ) for wr_id 1837 [ 848.207446 ] fit_poll_cq : connection 7 Recv weird event as - 1 [ 848.275044 ] fit_poll_cq : failed status ( 5 ) for wr_id 1838 [ 848.339523 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 848.411281 ] fit_poll_cq : failed status ( 5 ) for wr_id 1839 [ 848.475760 ] fit_poll_cq : connection 7 Recv weird event as - 30704 [ 848.547517 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.601596 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.655675 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.709753 ] mlx4_ib_handle_error_cqe syndrome 5 [ 848.763832 ] fit_poll_cq : failed status ( 5 ) for wr_id 1840 [ 848.828313 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 848.921908 ] IP : [ < ffffffff8106346d > ] fit_poll_cq + 0x4ad / 0x510 [ 848.989507 ] PGD 0 [ 849.013426 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 849.061265 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 744 [ 849.131983 ] RIP : 0010 : [ < ffffffff8106346d > ] [ < ffffffff8106346d > ] fit_poll_cq + 0x4ad / 0x510 [ 849.228700 ] RSP : 0000 : ffff88103e813d88 EFLAGS : 00010246 [ 849.292139 ] RAX : 000000000000100 8 RBX : ffff88103effbad0 RCX : 0000000000000000 [ 849.377418 ] RDX : 0000000000000000 RSI : ffffffff811d46e0 RDI : ffffffff811dbc08 [ 849.462695 ] RBP : ffff88103e813ea8 R08 : 0000000000000000 R09 : 0000000000000000 [ 849.547973 ] R10 : 0000000000000002 R11 : 0000000000000004 R12 : 0000000000000000 [ 849.633251 ] R13 : ffff88103e801008 R14 : 0000000000000004 R15 : ffff88103e813da0 [ 849.718529 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc40000 ( 0000 ) knlGS : 0000000000000000 [ 849.815246 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 849.883884 ] CR2 : 0000000000000000 CR3 : 000000000113 d000 CR4 : 00000000000406 a0 [ 849.969163 ] Stack : [ 849.993082 ] ffffffff81003299 000001 b03e813da0 0000000000000004 0000000000000730 [ 850.080440 ] 000000 8100000005 00001008000000f 9 ffff88103eff8c50 002 c222040000000 [ 850.167798 ] 0010004000000002 ffff88107fc20000 0000000000000731 ffffffff00000005 [ 850.255156 ] ffff8810000000f9 ffff88103eff8c50 0000000000000000 ffff88103e813e38 [ 850.342513 ] ffffffff81019854 0000000000000732 ffff881000000005 ffff8810000000f9 [ 850.429871 ] Call Trace : [ 850.458992 ] < TSK > [ 850.481870 ] [ < ffffffff81003299 > ] ? native_smp_send_reschedule + 0x39 / 0x50 [ 850.560909 ] [ < ffffffff81019854 > ] ? try_to_wake_up + 0xe4 / 0x1f0 [ 850.628506 ] [ < ffffffff81065708 > ] ? __schedule + 0xf8 / 0x1e0 [ 850.691945 ] [ < ffffffff810634d0 > ] ? fit_poll_cq + 0x510 / 0x510 [ 850.757464 ] [ < ffffffff810634e4 > ] fit_poll_cq_pass + 0x14 / 0x30 [ 850.824021 ] [ < ffffffff81020636 > ] kthread + 0xf6 / 0x120 [ 850.882260 ] [ < ffffffff81020540 > ] ? __kthread_parkme + 0x70 / 0x70 [ 850.950898 ] [ < ffffffff8100e572 > ] ret_from_fork + 0x22 / 0x30 /* handle normal reply */ ... memcpy (( void * ) ctx -> reply_ready_indicators [ reply_indicator_index ], & length , sizeof ( int )); ... ( This is a bad memcpy : reply_indicator_index , ctx , etc should be checked .)","title":"fit_poll_cq"},{"location":"lego/log/log-04-2018/#ib-spec-qp-cqe-wqe-send","text":"The channel adapter detects the WQE posting and accesses the WQE. The channel adapter interprets the command, validates the WQE\u2019s virtual 12 addresses, translates it to physical addresses, and accesses the data. The outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source & destination GIDs). The channel adapter then adds the local route header and calculates both the variant and invariant checksums. For a Send operation, the QP retrieves the address of the receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not the last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing the receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator. When the originator receives an acknowledgment, it creates a CQE on the 5 CQ and retires the WQE from the send queue. A QP can have multiple outstanding messages at any one time but the 8 target always acknowledges in the order sent, thus WQEs are retired in the order that they are posted.","title":"IB Spec: QP, CQE, WQE, SEND"},{"location":"lego/log/log-04-2018/#0402-mon","text":"Patching storage replica handler, able to finish today.","title":"04/02 Mon"},{"location":"lego/log/log-04-2018/#0401-sun","text":"Anyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover. I\u2019m tired. :-( A month to go. Record a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o: P [ 30801.296160 ] ibapi_send_reply () CPU : 8 PID : 19 timeout ( 30010 ms ), caller : clflush_one + 0x1c9 / 0x370 [ 30938.564843 ] mlx4_ib_handle_error_cqe syndrome 21 [ 30938.617988 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.672068 ] send request failed at connection 6 as 12 [ 30938.732389 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.786470 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.840551 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30938.894632 ] fit_poll_cq : failed status ( 5 ) for wr_id 1584 [ 30938.959112 ] fit_poll_cq : failed status ( 5 ) for wr_id 1585 [ 30939.023593 ] fit_poll_cq : connection 6 Recv weird event as - 1 [ 30939.091194 ] fit_poll_cq : failed status ( 5 ) for wr_id 1586 [ 30939.155676 ] fit_poll_cq : connection 6 Recv weird event as - 30704 [ 30939.227436 ] fit_poll_cq : failed status ( 5 ) for wr_id 1587 [ 30939.291917 ] fit_poll_cq : connection 6 Recv weird event as - 30704 [ 30939.363678 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.417759 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.471839 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.525921 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30939.580002 ] fit_poll_cq : failed status ( 5 ) for wr_id 1588 [ 30939.644483 ] BUG : unable to handle kernel NULL pointer dereference at ( null ) [ 30939.738083 ] IP : [ < ffffffff81062fcd > ] fit_poll_cq + 0x4ad / 0x510 [ 30939.805684 ] PGD 0 [ 30939.829604 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 30939.877445 ] CPU : 4 PID : 17 Comm : recvpollcq 4.0.0 - lego - ys + # 715 [ 30939.948166 ] RIP : 0010 : [ < ffffffff81062fcd > ] [ < ffffffff81062fcd > ] fit_poll_cq + 0x4ad / 0x510 fit_poll_cq at net / lego / fit_internal . c : 1734 memcpy (( void * ) ctx -> reply_ready_indicators [ reply_indicator_index ], & length , sizeof ( int )); M [ 30913.642698 ] mlx4_ib_handle_error_cqe syndrome 21 [ 30913.695839 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.749919 ] send request failed at connection 1 as 12 [ 30913.810236 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.864315 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.918395 ] mlx4_ib_handle_error_cqe syndrome 5 [ 30913.972474 ] fit_poll_cq : failed status ( 5 ) for wr_id 305 [ 30914.035912 ] fit_poll_cq : failed status ( 5 ) for wr_id 306","title":"04/01 Sun"},{"location":"lego/log/log-08-2018/","text":"Aug 2018 \u00b6 Aug 31 \u00b6 One major TODO \u00b6 Check do_handle_p2m_pcache_miss() . We MUST remove that mempcy, maybe by using another flag in thpool. This is just no acceptable. Ugh \u00b6 Fuck. Without debug_mm, there is still memory corruption. Try max_send_wr and number of QPs \u00b6 without lock_ib, with debug_mm. Change max_send_wr at all P M S. QP=4, max_send_wr = 1: always fail QP=4, max_send_wr = 256: always fail QP=24, max_send_wr = 1: succeed (0831-w14-18 0831-w14-20) QP=24, max_send_wr = 256: succeed (0831-w14-16 0831-w14-17) Pay attention to the 0831-w14-15 \uff1a something wrong with our timekeeping code? QP=24, max_send_wr = 1 case. After Victim bug fix \u00b6 MNIST 4 threads With lock_ib, debug_mm etc: 3 successful runs Only with debug_mm: Well fit failed. Lost CQE. Now the debug scope is limited. Let me try the micro test suite, to stress ibapi_send_reply itself. Potential: read/write buffer. Aug 30 \u00b6 Be humble. Identified victim bug. \u00b6 Finally. I thought it through, and with the help of this 0830-w14-12 . The bug is in victim_try_fill_pcache() , when there are multiple hits to the same victim. Since we released the usable_victim_lock after a hit. There might a be race case where: 1) CPU0 reached dec_and_test_filling , and passed to free the line. 2) CPU1 just got to the victim_check_hit , and increment the fill counter to 1 again. When CPU1 finished filling, and do dec_and_test_filling , it will do the free again!!! What a double free. Tomorrow, let me do the fix. Thought: adding more sync in victim_check_hit part. Basically we want to ensure only one CPU can do the final free. After adding pi_lock \u00b6 Okay. the pi_lock is added. Although it is mostly used by futex-pi and rt-mutex, we lego does not have these two guys. Therefore, it is only used by sched/core.c, exit.c, and kthread.c. 99% is in core.c The purpose of having this back is to have the spin_lock_irqsave(&p->pi_lock) back. Most scheduling code is not recursive, we have to disable interrupt. Of course we can use spin_lock_irqsave(&rq->lock) instead of spin_lock(&rq->lock) . But this is too dangerous at this stage. Porting based on Linux now is the fastest and safest way. The importance of disabling interrupt in some kernel path!! Good. Now I\u2019m seeing now debuggable victim issue. Classical deadlock catched. Now, only two victims. [ 2819.068997 ] CPU14 PID29 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 532 nr_lru : 63 fault_uva : 0x7fff98614000 [ 2819.094409 ] CPU14 PID29 -- Start Dump Victim Cache [ 0 ] total : 2 [ 2819.114188 ] CPU14 PID29 victim [ 0 ] : ffffffff810c2880 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.133289 ] CPU14 PID29 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.141723 ] CPU14 PID29 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.149770 ] CPU14 PID29 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.168870 ] CPU14 PID29 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.177306 ] CPU14 PID29 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.185352 ] CPU14 PID29 -- End Dump Victim Cache [ 0 ] [ 2819.081708 ] CPU16 PID30 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 0 nr_lru : 63 fault_uva : 0x7fffcc000024 [ 2819.209008 ] CPU16 PID30 -- Start Dump Victim Cache [ 1 ] total : 2 [ 2819.223358 ] CPU16 PID30 victim [ 0 ] : ffffffff810c2880 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.252443 ] CPU16 PID30 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.260879 ] CPU16 PID30 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.268926 ] CPU16 PID30 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.288026 ] CPU16 PID30 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.296461 ] CPU16 PID30 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.304508 ] CPU16 PID30 -- End Dump Victim Cache [ 1 ] [ 2819.101391 ] CPU18 PID31 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 15 nr_lru : 63 fault_uva : 0x7fff98c0f000 [ 2819.328165 ] CPU18 PID31 -- Start Dump Victim Cache [ 2 ] total : 2 [ 2819.335146 ] CPU18 PID31 victim [ 0 ] : ffffffff810c2880 refcount : 1 nr_fill : 0 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.354246 ] CPU18 PID31 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.362680 ] CPU18 PID31 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.370728 ] CPU18 PID31 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.389828 ] CPU18 PID31 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.398262 ] CPU18 PID31 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.406310 ] CPU18 PID31 -- End Dump Victim Cache [ 2 ] # # This guy grabbed the fill counter right before the first timout # That's why the above three timeout happen. And this one is 20s later # which equals to the timeout second. # [ 2839.327457 ] CPU12 PID28 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 0 nr_lru : 63 fault_uva : 0x7fffb0000f00 [ 2839.339964 ] CPU12 PID28 -- Start Dump Victim Cache [ 3 ] total : 2 [ 2839.346945 ] CPU12 PID28 victim [ 0 ] : ffffffff810c2880 refcount : 1 nr_fill : 0 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2839.366046 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2839.374480 ] CPU12 PID28 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2839.382527 ] CPU12 PID28 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2839.401628 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2839.410062 ] CPU12 PID28 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2839.418109 ] CPU12 PID28 -- End Dump Victim Cache [ 3 ] rq->lock deadlock \u00b6 Alright. We had rq->lock deadlock issue. Basically, we missed the part of disabling interrupt. A timer interrupt will try to acquire the lock again. Then, bang we have a deadlock. Digging into the code, you will be able to find the cause easily. The root cause we removed all the pi_lock stuff, which actually have a lot irqsave usages\u2026 Oh man, maybe it\u2019s time to add it back. [ 3367.835389] ------------------- cut here ------------------- [ 3367.841504] Possible deadlock happend locker_cpu: 0 [ 3367.846934] Current call stack: [ 3367.850425] CPU: 0 PID: 1 Comm: kernel_init 4.0.0-lego+ #437 [ 3367.856726] Stack: [ 3367.858957] ffff88107ff0fa58 ffffffff8101f4b6 ffff88107fc05e00 00000004a817c800 [ 3367.867101] ffff88107ff0fa80 ffffffff8101f52e ffff88107fc05e00 ffff88107ffb4000 [ 3367.875246] 0000000000000000 ffff88107ff0faa0 ffffffff8101b1ae ffff88107fc04980 [ 3367.883390] 0000000000000000 ffff88107ff0fab8 ffffffff810174f5 0000000000000286 [ 3367.891535] ffff88107ff0fae0 ffffffff81006774 ffff88107ffb9000 ffff88107fc05e00 [ 3367.899680] Call Trace: [ 3367.902396] <TSK> [ 3367.904528] [<ffffffff8101f4c2>] report_deadlock+0x62/0x80 [ 3367.910637] [<ffffffff8101f52e>] debug_spin_lock+0x4e/0x60 [ 3367.916745] [<ffffffff8101b1ae>] scheduler_tick+0x2e/0x60 [ 3367.922756] [<ffffffff810174f5>] tick_handle_periodic+0x45/0x70 [ 3367.929350] [<ffffffff81006774>] apic_timer_interrupt+0x54/0x90 [ 3367.935943] [<ffffffff8100e8aa>] smp__apic_timer_interrupt+0x6a/0x70 [ 3367.943021] [<ffffffff8101db99>] ? enqueue_task_rt+0x149/0x250 [ 3367.949518] [<ffffffff8105908a>] ? __mlx4_write_mtt+0xea/0x140 [ 3367.956014] [<ffffffff8101ad34>] activate_task+0x44/0x50 [ 3367.961929] [<ffffffff8101b667>] ttwu_do_activate+0x27/0x50 [ 3367.968134] [<ffffffff8101b89c>] try_to_wake_up+0xdc/0x1f0 [ 3367.974243] [<ffffffff8106cc20>] ? ib_mad_send_done_handler.isra.22+0x4d0/0x4d0 [ 3367.982388] [<ffffffff8101ba80>] wake_up_process+0x10/0x20 [ 3367.988497] [<ffffffff81023116>] __kthread_create_on_node+0x146/0x230 [ 3367.995671] [<ffffffff8102329f>] kthread_create_on_node+0x2f/0x40 [ 3368.002459] [<ffffffff81066873>] ? ib_create_cq+0x23/0x60 [ 3368.008470] [<ffffffff810695e1>] ib_mad_init_device+0x1f1/0x7b0 [ 3368.015064] [<ffffffff81067246>] ib_register_device+0x5d6/0x690 [ 3368.021657] [<ffffffff8105e9d3>] mlx4_ib_add+0x653/0x780 [ 3368.027571] [<ffffffff8105147d>] mlx4_add_device+0x8d/0x130 [ 3368.033777] [<ffffffff8105158c>] mlx4_register_interface+0x6c/0xa0 [ 3368.040661] [<ffffffff811dc660>] mlx4_ib_init+0x10/0x20 [ 3368.046478] [<ffffffff811dc619>] mlx4_init+0x19/0x50 [ 3368.052005] [<ffffffff811dc68d>] ib_core_init+0x1d/0x30 [ 3368.057823] [<ffffffff811db7f9>] device_init+0x9/0x10 [ 3368.063447] [<ffffffff8100030b>] kernel_init+0x4b/0xc0 [ 3368.069168] [<ffffffff8101b0ea>] ? schedule_tail+0xa/0x40 [ 3368.075178] [<ffffffff810002c0>] ? 0xffffffff810002c0 [ 3368.080803] [<ffffffff8100eb32>] ret_from_fork+0x22/0x30 [ 3368.086718] <EOT> 0830-w14-1: I really don\u2019t know how this happen. The refcounter and fill counter should be enough to serialize.. [37722.177024] CPU20 PID31 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x12e)(allocated|usable|hasdata|waitflush|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.196623] CPU20 PID31 hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000 [37722.204572] CPU20 PID31 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.224154] CPU20 PID31 rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63 [37722.232299] CPU20 PID31 victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimAllocated(v) || !VictimUsable(v) || !VictimFlushed(v) || VictimWriteback(v) || VictimLocked(v)) [37722.254790] WARNING: CPU: 20 PID: 31 at managers/processor/pcache/victim.c:196 __put_victim_nolist+0xb8/0x140 ffffffff8103e170[37722.453632] [<ffffffff8103c9c8>] __put_victim_nolist+0xb8/0x140 0000000000000000[37722.461873] [<ffffffff8103db18>] victim_try_fill_pcache+0x2f8/0x440 [37722.265842] CPU10 PID20 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.291438] CPU10 PID20 hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000 [37722.301616] CPU10 PID20 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.324206] CPU10 PID20 rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63 [37722.332349] CPU10 PID20 victim dumped because: PCACHE_BUG_ON_VICTIM(victim_ref_count(v) == 0) [37722.350673] WARNING: CPU: 10 PID: 20 at ./include/processor/pcache_victim.h:127 __victim_flush_func+0x232/0x250 [37722.363568] CPU: 10 PID: 20 Comm: kvictim_flushd 4.0.0-lego+ #435 [37722.534003] [<ffffffff8103e152>] __victim_flush_func+0x232/0x250 [37722.547577] [<ffffffff8103e1d9>] victim_flush_async+0x69/0xb0 [37722.553975] [<ffffffff81022ec1>] kthread+0x111/0x130 [37722.565900] [<ffffffff8100eb32>] ret_from_fork+0x22/0x30 Aug 29 \u00b6 The only thing left about core_IB is: ib_sa_query, which will be invoked when there is a mlx4 interrupts. Not sure if this is important. Anyway. Testing TF 4 threads MNIST again. When I enable SEQ_IBAPI\uff1a 0829-w14-11 (0829-w09-11) succeed 0829-w14-12: P side seems have deadlock. Let me enable DEBUG_SPINLOCK. 0829-w14-13: SEQ_IBAPI, DEBUG_SPINLOCK, this is a very useful log: [ 531.495545 ] STDOUT : --- [ INFO : tensorflow : loss = 0.5256375 , step = 101 ( 25.166 sec ) ] --- [ 531.624474 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000064 [ 531.633016 ] IP : [ < ffffffff8103b60e > ] __put_victim_nolist + 0xe / 0xa0 [ 531.639803 ] PGD 0 [ 531.642032 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 531.646493 ] CPU : 10 PID : 20 Comm : kvictim_flushd 4.0.0 - lego + # 426 [ 531.653279 ] RIP : 0010 : [ < ffffffff8103b60e > ] [ < ffffffff8103b60e > ] __put_victim_nolist + 0xe / 0xa0 [ 531.662781 ] RSP : 0000 : ffff880fe392fde0 EFLAGS : 00010006 [ 531.668696 ] RAX : 0000000000000000 RBX : ffffffff810c2b00 RCX : ffffffff810c2b70 [ 531.676646 ] RDX : ffffffff810c2b70 RSI : 0000007 aea3f42fa RDI : ffffffff810c2b00 [ 531.684597 ] RBP : ffff880fe392fdf0 R08 : 000000000000001f R09 : 0000000000000002 [ 531.692548 ] R10 : 00000000 80000000 R11 : 00000000000664 c3 R12 : ffff88207ff57000 [ 531.700498 ] R13 : ffffffff810c2b60 R14 : ffff880a72555000 R15 : ffffffff810c2b48 [ 531.708449 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 531.717466 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 531.723865 ] CR2 : 0000000000000064 CR3 : 00000000011 b9000 CR4 : 00000000000406 a0 [ 531.731816 ] Stack : [ 531.734046 ] ffffffff810c2b00 ffff88207ff57000 ffff880fe392fe08 ffffffff8103bbea [ 531.742190 ] ffffffff810c2b00 ffff880fe392fe48 ffffffff8103c729 00000000 8103 d7c2 [ 531.750335 ] ffff880a72555060 ffff88107ff0fdc8 0000000000000000 ffffffff8103c780 [ 531.758479 ] 0000000000000000 ffff880fe392fe60 ffffffff8103c7e6 ffff880fe391c000 [ 531.766623 ] ffff880fe392ff48 ffffffff81022e81 0000000000000000 0000000000000000 [ 531.774768 ] Call Trace : [ 531.777483 ] < TSK > [ 531.779617 ] [ < ffffffff8103bbea > ] __put_victim + 0x4a / 0x50 [ 531.785433 ] [ < ffffffff8103c729 > ] __victim_flush_func + 0xb9 / 0x110 [ 531.792027 ] [ < ffffffff8103c780 > ] ? __victim_flush_func + 0x110 / 0x110 [ 531.798911 ] [ < ffffffff8103c7e6 > ] victim_flush_async + 0x66 / 0x90 [ 531.805310 ] [ < ffffffff81022e81 > ] kthread + 0x111 / 0x130 [ 531.810836 ] [ < ffffffff81022d70 > ] ? __kthread_parkme + 0x70 / 0x70 [ 531.817236 ] [ < ffffffff8100eb32 > ] ret_from_fork + 0x22 / 0x30 [ 531.823151 ] < EOT > 0829-w14-14: this looks like a double free, or concurrent eviction. But if you look into the evict code, we will check the Flushed flag. It means another eviction routine should have skipped this line, and will not pick this line to do eviction. Some other possibilities? check until 0829-w14-18 [ 1671.661424 ] ------------ [ cut here ] ------------ [ 1671.666378 ] BUG : failure at managers / processor / pcache / victim . c : 610 / victim_finish_insert () ! [ 1671.675591 ] Kernel Panic - not syncing : BUG ! [ 1671.680339 ] CPU : 20 PID : 31 Comm : python 4.0.0 - lego + # 426 [ 1671.686351 ] Stack : [ 1671.688581 ] ffff880fbe76fda0 ffffffff810289b7 ffffffff00000008 ffff880fbe76fdb0 [ 1671.696725 ] ffff880fbe76fd68 ffffff0021475542 ffff88107fd45e00 ffff880fbe753000 [ 1671.704870 ] 0000000000000000 0000000000000001 ffff880fbe76f9b0 ffffffff8101b1b7 [ 1671.713015 ] ffff88107fd44980 ffff880fbe76f9d8 ffffffff8101405f 0000000000000000 [ 1671.721160 ] 0000000000000001 ffff880ff992a000 0000000000000001 ffff880fbe76f9f0 [ 1671.729304 ] Call Trace : [ 1671.732019 ] < TSK > [ 1671.734153 ] [ < ffffffff810289c3 > ] panic + 0xc2 / 0x10a [ 1671.739388 ] [ < ffffffff8101b1b7 > ] ? scheduler_tick + 0x57 / 0x60 [ 1671.745593 ] [ < ffffffff8101405f > ] ? generic_smp_call_function_single_interrupt + 0x8f / 0x160 [ 1671.754611 ] [ < ffffffff8100339e > ] ? call_function_interrupt + 0x2e / 0x40 [ 1671.761688 ] [ < ffffffff8100e9fa > ] ? smp__call_function_interrupt + 0x6a / 0x70 [ 1671.769251 ] [ < ffffffff8101f4bb > ] ? debug_spin_lock + 0x1b / 0x50 [ 1671.775555 ] [ < ffffffff81075efc > ] ? fit_internal_poll_sendcq + 0x6c / 0x140 [ 1671.782826 ] [ < ffffffff81042039 > ] ? find_next_bit + 0x19 / 0x20 [ 1671.788934 ] [ < ffffffff8101f4bb > ] ? debug_spin_lock + 0x1b / 0x50 [ 1671.795236 ] [ < ffffffff8101dcac > ] ? task_tick_rt + 0x2c / 0xd0 [ 1671.801248 ] [ < ffffffff8101b1b7 > ] ? scheduler_tick + 0x57 / 0x60 [ 1671.807453 ] [ < ffffffff810174d5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1671.814240 ] [ < ffffffff81006774 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1671.821029 ] [ < ffffffff8100e8aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1671.828300 ] [ < ffffffff81012bc8 > ] ? printk + 0x118 / 0x1b0 [ 1671.833924 ] [ < ffffffff8103c161 > ] victim_finish_insert + 0x171 / 0x180 [ 1671.840711 ] [ < ffffffff8103b2a2 > ] pcache_evict_line + 0xf2 / 0x2e0 [ 1671.847110 ] [ < ffffffff81038d7c > ] pcache_alloc + 0x1ac / 0x380 [ 1671.853122 ] [ < ffffffff8103a10c > ] ? pcache_add_rmap + 0x7c / 0x260 [ 1671.859521 ] [ < ffffffff810382bb > ] common_do_fill_page + 0x2b / 0x1e0 [ 1671.866114 ] [ < ffffffff81038631 > ] pcache_handle_fault + 0x1c1 / 0x620 [ 1671.872804 ] [ < ffffffff81037fc0 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1671.879398 ] [ < ffffffff8101006f > ] do_page_fault + 0xaf / 0x1c0 [ 1671.885410 ] [ < ffffffff8100dedf > ] page_fault + 0x1f / 0x30 0829-w14-16: we got this by having debug_spinlock, and seq_ibapi. This is interesting and serious. I think our general C code is fine.. Should I go check the assembly part? This is the rq->lock? come on\u2026 [ 683.748135 ] ------------------- cut here ------------------- [ 683.754252 ] Possible deadlock happend [ 683.758323 ] Current call stack : [ 683.761815 ] CPU : 4 PID : 39 Comm : python 4.0.0 - lego + # 428 [ 683.767728 ] Stack : [ 683.769959 ] ffff880fc1c1fc38 ffffffff8101f48c ffff88107fc45e00 ffff880fc1c1fc60 [ 683.778103 ] ffffffff8101f4e4 ffff88107fc45e00 ffff880fc23fb000 0000000000000000 [ 683.786247 ] ffff880fc1c1fc80 ffffffff8101b18e ffff88107fc44980 0000000000000004 [ 683.794391 ] ffff880fc1c1fc98 ffffffff810174d5 ffffffff8101dddb ffff880fc1c1fcc0 [ 683.802537 ] ffffffff81006774 ffff88107fc45e00 00000004 a817c800 000000 9 a8a78c5e7 [ 683.810680 ] Call Trace : [ 683.813396 ] < TSK > [ 683.815528 ] [ < ffffffff8101f498 > ] report_deadlock + 0x58 / 0x60 [ 683.821637 ] [ < ffffffff8101f4e4 > ] debug_spin_lock + 0x44 / 0x50 [ 683.827745 ] [ < ffffffff8101b18e > ] scheduler_tick + 0x2e / 0x60 [ 683.833758 ] [ < ffffffff810174d5 > ] tick_handle_periodic + 0x45 / 0x70 [ 683.840351 ] [ < ffffffff8101dddb > ] ? dequeue_task_rt + 0x1b / 0x180 [ 683.846750 ] [ < ffffffff81006774 > ] apic_timer_interrupt + 0x54 / 0x90 [ 683.853343 ] [ < ffffffff8100e8aa > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 683.860421 ] [ < ffffffff8101f4d1 > ] ? debug_spin_lock + 0x31 / 0x50 [ 683.866723 ] [ < ffffffff8101b86e > ] try_to_wake_up + 0xce / 0x1f0 [ 683.872832 ] [ < ffffffff8101b9e4 > ] wake_up_q + 0x54 / 0xc0 [ 683.878358 ] [ < ffffffff81028487 > ] do_futex + 0x407 / 0x620 [ 683.883982 ] [ < ffffffff8103a941 > ] ? pcache_add_rmap + 0xb1 / 0x600 [ 683.890381 ] [ < ffffffff8102870c > ] sys_futex + 0x6c / 0x130 [ 683.896005 ] [ < ffffffff8100ec66 > ] do_syscall_64 + 0x36 / 0xc0 [ 683.901919 ] [ < ffffffff8100db6c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25 Aug 27 \u00b6 There a lot lost CQE cases. This one is about P->M->S. And M lost the CQE for the WQE sent to S. 0827 - w9 - 5 [ 963.304865 ] watchdog : worker [ 0 ] CPU10 stucked [ 963.309712 ] watchdog : common_header [ op = 0x20000000 src_nid : 0 ] [ 963.316210 ] CPU : 10 PID : 20 Comm : thpool - worker0 4.0.0 - lego + # 43 [ 963.322899 ] RIP : 0010 : [ < ffffffff8106ad51 > ] [ < ffffffff8106ad51 > ] fit_send_reply_with_rdma_write_with_imm + 0x2a1 / 0x3a0 [ 963.334632 ] RSP : 0000 : ffff88103ef3fc20 EFLAGS : 000002 87 [ 963.340547 ] RAX : 00000000ff ffb6d4 RBX : 000000000000000 b RCX : 0000000000001770 [ 963.348498 ] RDX : 00000000ff ffa70d RSI : fffffffffffff039 RDI : 0000000000000000 [ 963.356450 ] RBP : ffff88103ef3fcc0 R08 : 000000000000001f R09 : 0000000000000002 [ 963.364400 ] R10 : 00000000 80000000 R11 : 000077ff 80000000 R12 : 0000000000000000 [ 963.372352 ] R13 : ffff88103ef26738 R14 : 00000000000 b3d54 R15 : ffff88103ef25008 [ 963.380303 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 963.389320 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 963.395720 ] CR2 : 0000000000000000 CR3 : 000000000116 a000 CR4 : 00000000000406 a0 [ 963.403671 ] Stack : [ 963.405901 ] 00007ff f000b3d54 ffffffff800b3d54 ffff881000000004 ffff88103ef3fc78 [ 963.414045 ] 0000000 900000000 ffff881000000000 0000100 800000001 ffff88103d216000 [ 963.422191 ] ffff88103eebae48 800 b3d540000011c ffffff9b00000246 ffffea0000000001 [ 963.430337 ] 000000103 d216000 0000000000010 c00 000000000000011 c 000000000000100 8 [ 963.438481 ] 000000000000011 c 000000000000100 8 ffff88103eebae48 ffff88103ef3fd70 [ 963.446626 ] Call Trace : [ 963.449342 ] < TSK > [ 963.451475 ] [ < ffffffff81067c80 > ] ibapi_send_reply_imm + 0x50 / 0xd0 [ 963.458068 ] [ < ffffffff8102e953 > ] ? __storage_read + 0xc3 / 0x120 [ 963.464371 ] [ < ffffffff8102e953 > ] __storage_read + 0xc3 / 0x120 [ 963.470480 ] [ < ffffffff8102e9bf > ] storage_read + 0xf / 0x50 [ 963.476201 ] [ < ffffffff8102eab7 > ] storage_vma_fault + 0xb7 / 0x130 [ 963.482600 ] [ < ffffffff8103262f > ] handle_lego_mm_fault + 0x13f / 0x4a0 [ 963.489389 ] [ < ffffffff8102ecf4 > ] common_handle_p2m_miss . isra .1 + 0x54 / 0xc0 [ 963.496855 ] [ < ffffffff8102edc7 > ] handle_p2m_pcache_miss + 0x67 / 0x2d0 [ 963.503739 ] [ < ffffffff8102bf96 > ] thpool_worker_func + 0x296 / 0x3a0 [ 963.510332 ] [ < ffffffff8102bd00 > ] ? handle_bad_request + 0x40 / 0x40 [ 963.516926 ] [ < ffffffff81020ca6 > ] kthread + 0xf6 / 0x120 [ 963.522357 ] [ < ffffffff81020bb0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 963.528756 ] [ < ffffffff8100e632 > ] ret_from_fork + 0x22 / 0x30 hmm, another on lost CQE happen at P. Today is weird, why we happen to have so many lost CQE today? Think about why CQE is not generated? 0827 - w14 - 6 [ 1185.835707 ] ***** ***** Fail to to get the CQE from send_cq after 20 seconds ! ***** This means the packet was lost and something went wrong ***** with your NIC ... ***** connection_id : 7 dest node : 1 ***** [ 1185.856465 ] IB Stats : [ 1185.858985 ] nr_ib_send_reply : 3452 [ 1185.864221 ] nr_bytes_tx : 506507 [ 1185.869456 ] nr_bytes_rx : 8981004 [ 1185.874692 ] ------------ [ cut here ] ------------ [ 1185.879829 ] WARNING : CPU : 14 PID : 22 at net / lego / fit_internal . c : 1108 fit_internal_poll_sendcq + 0xe5 / 0x140 [ 1185.890399 ] CPU : 14 PID : 22 Comm : python 4.0.0 - lego + # 356 [ 1185.896410 ] Stack : [ 1185.898640 ] ffff88103c49fb30 ffffffff810126f5 ffff88103cb22000 00000004 a817c800 [ 1185.906784 ] 0000010f7139214f 0000000000000007 ffff88103c49fb40 ffffffff810127cf [ 1185.914927 ] ffff88103c49fbf0 ffffffff810724b5 000000023 cb2c280 ffff88103cb2c1f8 [ 1185.923072 ] 00000000000002 86 ffff88103c49fc18 ffff88103cb06000 ffff88103cb2c150 [ 1185.931217 ] 000000000000024 b ffff88108101c7dc ffff88107fce5d80 ffff88103c46f000 [ 1185.939360 ] Call Trace : [ 1185.942075 ] < TSK > [ 1185.944209 ] [ < ffffffff81012701 > ] __warn . constprop .1 + 0x91 / 0xd0 [ 1185.950607 ] [ < ffffffff810127cf > ] warn_slowpath_null + 0xf / 0x20 [ 1185.956909 ] [ < ffffffff810724b5 > ] fit_internal_poll_sendcq + 0xe5 / 0x140 [ 1185.963987 ] [ < ffffffff81019dd5 > ] ? scheduler_tick + 0x55 / 0x60 [ 1185.970192 ] [ < ffffffff81072662 > ] fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 1185.979791 ] [ < ffffffff810741ff > ] fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0 [ 1185.988420 ] [ < ffffffff810368c2 > ] ? __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1185.995401 ] [ < ffffffff810701e9 > ] ibapi_send_reply_timeout + 0x79 / 0x120 [ 1186.002479 ] [ < ffffffff810368c2 > ] ? __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1186.009459 ] [ < ffffffff810368c2 > ] __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1186.016245 ] [ < ffffffff81036ac4 > ] common_do_fill_page + 0xf4 / 0x1f0 [ 1186.022839 ] [ < ffffffff81036d80 > ] pcache_handle_fault + 0x1c0 / 0x610 [ 1186.029528 ] [ < ffffffff81036800 > ] ? __pcache_do_zerofill_page + 0x100 / 0x100 [ 1186.036995 ] [ < ffffffff8100fdff > ] do_page_fault + 0xaf / 0x1c0 [ 1186.043005 ] [ < ffffffff8100dc1f > ] page_fault + 0x1f / 0x30 Aug 26 \u00b6 Oh well. I saw the same damn lost packet issue again. The issue can be desribed as: P use lite rpc to send a request to M. M processed the handled, and called rpc reply to sent back to P. M need to poll send_cq to poll completion. But M fail to get the CQE for the should-be-sent-out WQE. This is tested with M\u2019s CONFIG_FIT_NOWAIT optimization, which is basically an optimization that M will not poll cq every time a reply was sent out, instead, do batch polling. The following stack dump was reported by M side watchdog. It is not necessary mlx4_poll_cq\u2019s issue, since there is a while (1) loop at fit code. Oh well. Log name : 0826 - w9 - 1 [ 187736.669027 ] watchdog : worker [ 0 ] CPU10 stucked [ 187736.673972 ] watchdog : common_header [ op = 0x30000000 src_nid : 0 ] [ 187736.680566 ] CPU : 10 PID : 20 Comm : thpool - worker0 4.0.0 - lego + # 26 [ 187736.687351 ] RIP : 0010 : [ < ffffffff810522c3 > ] [ < ffffffff810522c3 > ] mlx4_ib_poll_cq + 0x1d3 / 0x850 [ 187736.696854 ] RSP : 0000 : ffff88103ef3f750 EFLAGS : 000002 86 [ 187736.702865 ] RAX : 00000000ff fffff5 RBX : 0000000000000000 RCX : ffff88103ed6b050 [ 187736.710913 ] RDX : 00000000 80630000 RSI : 0000000000000001 RDI : ffff88103edb0bf0 [ 187736.718961 ] RBP : ffff88103ef3f7b8 R08 : 0000000000000020 R09 : 0000000000000002 [ 187736.727007 ] R10 : 0000000ff c53fddc R11 : 0000000040 bf1040 R12 : ffff88103ef3f7c8 [ 187736.735055 ] R13 : 0000000000000000 R14 : 0000000000000000 R15 : ffff88103edb0bf0 [ 187736.743104 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 187736.752218 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 187736.758714 ] CR2 : 0000000000000000 CR3 : 000000000116 a000 CR4 : 00000000000406 a0 [ 187736.766762 ] Stack : [ 187736.769089 ] 0000000ff c53fddc 0000000000000002 0000000000000020 ffff88103edb0c98 [ 187736.777331 ] 00000000000002 86 00000000 80630000 ffff88103ef3f7d0 000063 8000000018 [ 187736.785572 ] ffff88103edb0bf0 0000000000000001 ffff88103ef25008 0000000000000003 [ 187736.793813 ] 000000000000000 c ffff88103ef3fd30 ffffffff8106920c ffff88103ef3fd54 [ 187736.802054 ] 0000000100000000 0000000100000000 ffff88103edb07b0 ffff88103e81b008 [ 187736.810296 ] Call Trace : [ 187736.813108 ] < TSK > [ 187736.815338 ] [ < ffffffff8106920c > ] fit_internal_poll_sendcq + 0x6c / 0xe0 [ 187736.822416 ] [ < ffffffff8106ab2f > ] ? fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0 [ 187736.831336 ] [ < ffffffff81033ff0 > ] ? _lego_copy_to_user + 0x110 / 0x250 [ 187736.838220 ] [ < ffffffff81028d65 > ] ? __free_pages + 0x25 / 0x30 [ 187736.844329 ] [ < ffffffff8102e981 > ] ? __storage_read + 0xf1 / 0x120 [ 187736.850728 ] [ < ffffffff81019865 > ] ? scheduler_tick + 0x55 / 0x60 [ 187736.857031 ] [ < ffffffff810693d2 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.866920 ] [ < ffffffff810693d2 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.876810 ] [ < ffffffff8103043f > ] ? __vma_adjust + 0x38f / 0x550 [ 187736.883113 ] [ < ffffffff81030944 > ] ? vma_merge + 0x1a4 / 0x280 [ 187736.889123 ] [ < ffffffff81030f20 > ] ? arch_get_unmapped_area_topdown + 0xe0 / 0x220 [ 187736.897075 ] [ < ffffffff810693d2 > ] fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.906771 ] [ < ffffffff81069ab5 > ] fit_ack_reply_callback + 0x185 / 0x1e0 [ 187736.913848 ] [ < ffffffff8102f129 > ] ? handle_p2m_flush_one + 0x69 / 0x160 [ 187736.920830 ] [ < ffffffff8102bde0 > ] thpool_worker_func + 0xe0 / 0x3a0 [ 187736.927424 ] [ < ffffffff8102bd00 > ] ? handle_bad_request + 0x40 / 0x40 [ 187736.934113 ] [ < ffffffff81020ca6 > ] kthread + 0xf6 / 0x120 [ 187736.939639 ] [ < ffffffff81020bb0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 187736.946137 ] [ < ffffffff8100e632 > ] ret_from_fork + 0x22 / 0x30 Aug 22 \u00b6 Damn it!!! After so much effort verifying we had a solid IB stack, we still has memory corruption and deadlock issues. Fuck! One thing at a time, simple stuff first. Okay, tomorrow first add DEBUG_SPINLOCK to detect possible deadlocks. This, could help to identify some buggy code. After this, I will spend some time looking into the LITE, it\u2019s fucking HEAVY. I do found a lot issues during summer. Personally, I\u2019m not feeling good this days. I treat someone with love and respect, but there is not too much in return. Yeahyeahyeah, I know how this works. It\u2019s just sad that sometimes you just have a BAD timing. I\u2019ve went through too much things in 2018, good and bad. I care sooo much about the people I love, family and others. I feel this is good, of course. Anyway, it is supposed to be a Lego dump, that no one probably interested in.","title":"Aug 2018"},{"location":"lego/log/log-08-2018/#aug-2018","text":"","title":"Aug 2018"},{"location":"lego/log/log-08-2018/#aug-31","text":"","title":"Aug 31"},{"location":"lego/log/log-08-2018/#one-major-todo","text":"Check do_handle_p2m_pcache_miss() . We MUST remove that mempcy, maybe by using another flag in thpool. This is just no acceptable.","title":"One major TODO"},{"location":"lego/log/log-08-2018/#ugh","text":"Fuck. Without debug_mm, there is still memory corruption.","title":"Ugh"},{"location":"lego/log/log-08-2018/#try-max_send_wr-and-number-of-qps","text":"without lock_ib, with debug_mm. Change max_send_wr at all P M S. QP=4, max_send_wr = 1: always fail QP=4, max_send_wr = 256: always fail QP=24, max_send_wr = 1: succeed (0831-w14-18 0831-w14-20) QP=24, max_send_wr = 256: succeed (0831-w14-16 0831-w14-17) Pay attention to the 0831-w14-15 \uff1a something wrong with our timekeeping code? QP=24, max_send_wr = 1 case.","title":"Try max_send_wr and number of QPs"},{"location":"lego/log/log-08-2018/#after-victim-bug-fix","text":"MNIST 4 threads With lock_ib, debug_mm etc: 3 successful runs Only with debug_mm: Well fit failed. Lost CQE. Now the debug scope is limited. Let me try the micro test suite, to stress ibapi_send_reply itself. Potential: read/write buffer.","title":"After Victim bug fix"},{"location":"lego/log/log-08-2018/#aug-30","text":"Be humble.","title":"Aug 30"},{"location":"lego/log/log-08-2018/#identified-victim-bug","text":"Finally. I thought it through, and with the help of this 0830-w14-12 . The bug is in victim_try_fill_pcache() , when there are multiple hits to the same victim. Since we released the usable_victim_lock after a hit. There might a be race case where: 1) CPU0 reached dec_and_test_filling , and passed to free the line. 2) CPU1 just got to the victim_check_hit , and increment the fill counter to 1 again. When CPU1 finished filling, and do dec_and_test_filling , it will do the free again!!! What a double free. Tomorrow, let me do the fix. Thought: adding more sync in victim_check_hit part. Basically we want to ensure only one CPU can do the final free.","title":"Identified victim bug."},{"location":"lego/log/log-08-2018/#after-adding-pi_lock","text":"Okay. the pi_lock is added. Although it is mostly used by futex-pi and rt-mutex, we lego does not have these two guys. Therefore, it is only used by sched/core.c, exit.c, and kthread.c. 99% is in core.c The purpose of having this back is to have the spin_lock_irqsave(&p->pi_lock) back. Most scheduling code is not recursive, we have to disable interrupt. Of course we can use spin_lock_irqsave(&rq->lock) instead of spin_lock(&rq->lock) . But this is too dangerous at this stage. Porting based on Linux now is the fastest and safest way. The importance of disabling interrupt in some kernel path!! Good. Now I\u2019m seeing now debuggable victim issue. Classical deadlock catched. Now, only two victims. [ 2819.068997 ] CPU14 PID29 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 532 nr_lru : 63 fault_uva : 0x7fff98614000 [ 2819.094409 ] CPU14 PID29 -- Start Dump Victim Cache [ 0 ] total : 2 [ 2819.114188 ] CPU14 PID29 victim [ 0 ] : ffffffff810c2880 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.133289 ] CPU14 PID29 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.141723 ] CPU14 PID29 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.149770 ] CPU14 PID29 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.168870 ] CPU14 PID29 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.177306 ] CPU14 PID29 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.185352 ] CPU14 PID29 -- End Dump Victim Cache [ 0 ] [ 2819.081708 ] CPU16 PID30 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 0 nr_lru : 63 fault_uva : 0x7fffcc000024 [ 2819.209008 ] CPU16 PID30 -- Start Dump Victim Cache [ 1 ] total : 2 [ 2819.223358 ] CPU16 PID30 victim [ 0 ] : ffffffff810c2880 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.252443 ] CPU16 PID30 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.260879 ] CPU16 PID30 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.268926 ] CPU16 PID30 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.288026 ] CPU16 PID30 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.296461 ] CPU16 PID30 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.304508 ] CPU16 PID30 -- End Dump Victim Cache [ 1 ] [ 2819.101391 ] CPU18 PID31 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 15 nr_lru : 63 fault_uva : 0x7fff98c0f000 [ 2819.328165 ] CPU18 PID31 -- Start Dump Victim Cache [ 2 ] total : 2 [ 2819.335146 ] CPU18 PID31 victim [ 0 ] : ffffffff810c2880 refcount : 1 nr_fill : 0 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.354246 ] CPU18 PID31 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2819.362680 ] CPU18 PID31 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.370728 ] CPU18 PID31 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2819.389828 ] CPU18 PID31 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2819.398262 ] CPU18 PID31 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2819.406310 ] CPU18 PID31 -- End Dump Victim Cache [ 2 ] # # This guy grabbed the fill counter right before the first timout # That's why the above three timeout happen. And this one is 20s later # which equals to the timeout second. # [ 2839.327457 ] CPU12 PID28 Abort victim alloc ( 20010 ms ) nr_usable_victims : 2. From pset_idx : 0 nr_lru : 63 fault_uva : 0x7fffb0000f00 [ 2839.339964 ] CPU12 PID28 -- Start Dump Victim Cache [ 3 ] total : 2 [ 2839.346945 ] CPU12 PID28 victim [ 0 ] : ffffffff810c2880 refcount : 1 nr_fill : 0 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2839.366046 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffcc000000 [ 2839.374480 ] CPU12 PID28 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2839.382527 ] CPU12 PID28 victim [ 1 ] : ffffffff810c2900 refcount : 2 nr_fill : 1 max_fill : 4 locked : 0 flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree ) pcm : ( null ) pset : ffff88207ff5a000 [ 2839.401628 ] CPU12 PID28 hit [ 0 ] owner : 21 m_nid : 1 rep_nid : 1 addr : 0x7fffb0000000 [ 2839.410062 ] CPU12 PID28 rmap to pset : ffff88207ff5a000 set_idx : 0 nr_lru : 63 [ 2839.418109 ] CPU12 PID28 -- End Dump Victim Cache [ 3 ]","title":"After adding pi_lock"},{"location":"lego/log/log-08-2018/#rq-lock-deadlock","text":"Alright. We had rq->lock deadlock issue. Basically, we missed the part of disabling interrupt. A timer interrupt will try to acquire the lock again. Then, bang we have a deadlock. Digging into the code, you will be able to find the cause easily. The root cause we removed all the pi_lock stuff, which actually have a lot irqsave usages\u2026 Oh man, maybe it\u2019s time to add it back. [ 3367.835389] ------------------- cut here ------------------- [ 3367.841504] Possible deadlock happend locker_cpu: 0 [ 3367.846934] Current call stack: [ 3367.850425] CPU: 0 PID: 1 Comm: kernel_init 4.0.0-lego+ #437 [ 3367.856726] Stack: [ 3367.858957] ffff88107ff0fa58 ffffffff8101f4b6 ffff88107fc05e00 00000004a817c800 [ 3367.867101] ffff88107ff0fa80 ffffffff8101f52e ffff88107fc05e00 ffff88107ffb4000 [ 3367.875246] 0000000000000000 ffff88107ff0faa0 ffffffff8101b1ae ffff88107fc04980 [ 3367.883390] 0000000000000000 ffff88107ff0fab8 ffffffff810174f5 0000000000000286 [ 3367.891535] ffff88107ff0fae0 ffffffff81006774 ffff88107ffb9000 ffff88107fc05e00 [ 3367.899680] Call Trace: [ 3367.902396] <TSK> [ 3367.904528] [<ffffffff8101f4c2>] report_deadlock+0x62/0x80 [ 3367.910637] [<ffffffff8101f52e>] debug_spin_lock+0x4e/0x60 [ 3367.916745] [<ffffffff8101b1ae>] scheduler_tick+0x2e/0x60 [ 3367.922756] [<ffffffff810174f5>] tick_handle_periodic+0x45/0x70 [ 3367.929350] [<ffffffff81006774>] apic_timer_interrupt+0x54/0x90 [ 3367.935943] [<ffffffff8100e8aa>] smp__apic_timer_interrupt+0x6a/0x70 [ 3367.943021] [<ffffffff8101db99>] ? enqueue_task_rt+0x149/0x250 [ 3367.949518] [<ffffffff8105908a>] ? __mlx4_write_mtt+0xea/0x140 [ 3367.956014] [<ffffffff8101ad34>] activate_task+0x44/0x50 [ 3367.961929] [<ffffffff8101b667>] ttwu_do_activate+0x27/0x50 [ 3367.968134] [<ffffffff8101b89c>] try_to_wake_up+0xdc/0x1f0 [ 3367.974243] [<ffffffff8106cc20>] ? ib_mad_send_done_handler.isra.22+0x4d0/0x4d0 [ 3367.982388] [<ffffffff8101ba80>] wake_up_process+0x10/0x20 [ 3367.988497] [<ffffffff81023116>] __kthread_create_on_node+0x146/0x230 [ 3367.995671] [<ffffffff8102329f>] kthread_create_on_node+0x2f/0x40 [ 3368.002459] [<ffffffff81066873>] ? ib_create_cq+0x23/0x60 [ 3368.008470] [<ffffffff810695e1>] ib_mad_init_device+0x1f1/0x7b0 [ 3368.015064] [<ffffffff81067246>] ib_register_device+0x5d6/0x690 [ 3368.021657] [<ffffffff8105e9d3>] mlx4_ib_add+0x653/0x780 [ 3368.027571] [<ffffffff8105147d>] mlx4_add_device+0x8d/0x130 [ 3368.033777] [<ffffffff8105158c>] mlx4_register_interface+0x6c/0xa0 [ 3368.040661] [<ffffffff811dc660>] mlx4_ib_init+0x10/0x20 [ 3368.046478] [<ffffffff811dc619>] mlx4_init+0x19/0x50 [ 3368.052005] [<ffffffff811dc68d>] ib_core_init+0x1d/0x30 [ 3368.057823] [<ffffffff811db7f9>] device_init+0x9/0x10 [ 3368.063447] [<ffffffff8100030b>] kernel_init+0x4b/0xc0 [ 3368.069168] [<ffffffff8101b0ea>] ? schedule_tail+0xa/0x40 [ 3368.075178] [<ffffffff810002c0>] ? 0xffffffff810002c0 [ 3368.080803] [<ffffffff8100eb32>] ret_from_fork+0x22/0x30 [ 3368.086718] <EOT> 0830-w14-1: I really don\u2019t know how this happen. The refcounter and fill counter should be enough to serialize.. [37722.177024] CPU20 PID31 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x12e)(allocated|usable|hasdata|waitflush|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.196623] CPU20 PID31 hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000 [37722.204572] CPU20 PID31 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.224154] CPU20 PID31 rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63 [37722.232299] CPU20 PID31 victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimAllocated(v) || !VictimUsable(v) || !VictimFlushed(v) || VictimWriteback(v) || VictimLocked(v)) [37722.254790] WARNING: CPU: 20 PID: 31 at managers/processor/pcache/victim.c:196 __put_victim_nolist+0xb8/0x140 ffffffff8103e170[37722.453632] [<ffffffff8103c9c8>] __put_victim_nolist+0xb8/0x140 0000000000000000[37722.461873] [<ffffffff8103db18>] victim_try_fill_pcache+0x2f8/0x440 [37722.265842] CPU10 PID20 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.291438] CPU10 PID20 hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000 [37722.301616] CPU10 PID20 victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm: (null) pset:ffff88207ff5b980 [37722.324206] CPU10 PID20 rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63 [37722.332349] CPU10 PID20 victim dumped because: PCACHE_BUG_ON_VICTIM(victim_ref_count(v) == 0) [37722.350673] WARNING: CPU: 10 PID: 20 at ./include/processor/pcache_victim.h:127 __victim_flush_func+0x232/0x250 [37722.363568] CPU: 10 PID: 20 Comm: kvictim_flushd 4.0.0-lego+ #435 [37722.534003] [<ffffffff8103e152>] __victim_flush_func+0x232/0x250 [37722.547577] [<ffffffff8103e1d9>] victim_flush_async+0x69/0xb0 [37722.553975] [<ffffffff81022ec1>] kthread+0x111/0x130 [37722.565900] [<ffffffff8100eb32>] ret_from_fork+0x22/0x30","title":"rq-&gt;lock deadlock"},{"location":"lego/log/log-08-2018/#aug-29","text":"The only thing left about core_IB is: ib_sa_query, which will be invoked when there is a mlx4 interrupts. Not sure if this is important. Anyway. Testing TF 4 threads MNIST again. When I enable SEQ_IBAPI\uff1a 0829-w14-11 (0829-w09-11) succeed 0829-w14-12: P side seems have deadlock. Let me enable DEBUG_SPINLOCK. 0829-w14-13: SEQ_IBAPI, DEBUG_SPINLOCK, this is a very useful log: [ 531.495545 ] STDOUT : --- [ INFO : tensorflow : loss = 0.5256375 , step = 101 ( 25.166 sec ) ] --- [ 531.624474 ] BUG : unable to handle kernel NULL pointer dereference at 0000000000000064 [ 531.633016 ] IP : [ < ffffffff8103b60e > ] __put_victim_nolist + 0xe / 0xa0 [ 531.639803 ] PGD 0 [ 531.642032 ] Oops : 0002 [ # 1 ] SMP PROCESSOR [ 531.646493 ] CPU : 10 PID : 20 Comm : kvictim_flushd 4.0.0 - lego + # 426 [ 531.653279 ] RIP : 0010 : [ < ffffffff8103b60e > ] [ < ffffffff8103b60e > ] __put_victim_nolist + 0xe / 0xa0 [ 531.662781 ] RSP : 0000 : ffff880fe392fde0 EFLAGS : 00010006 [ 531.668696 ] RAX : 0000000000000000 RBX : ffffffff810c2b00 RCX : ffffffff810c2b70 [ 531.676646 ] RDX : ffffffff810c2b70 RSI : 0000007 aea3f42fa RDI : ffffffff810c2b00 [ 531.684597 ] RBP : ffff880fe392fdf0 R08 : 000000000000001f R09 : 0000000000000002 [ 531.692548 ] R10 : 00000000 80000000 R11 : 00000000000664 c3 R12 : ffff88207ff57000 [ 531.700498 ] R13 : ffffffff810c2b60 R14 : ffff880a72555000 R15 : ffffffff810c2b48 [ 531.708449 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 531.717466 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 531.723865 ] CR2 : 0000000000000064 CR3 : 00000000011 b9000 CR4 : 00000000000406 a0 [ 531.731816 ] Stack : [ 531.734046 ] ffffffff810c2b00 ffff88207ff57000 ffff880fe392fe08 ffffffff8103bbea [ 531.742190 ] ffffffff810c2b00 ffff880fe392fe48 ffffffff8103c729 00000000 8103 d7c2 [ 531.750335 ] ffff880a72555060 ffff88107ff0fdc8 0000000000000000 ffffffff8103c780 [ 531.758479 ] 0000000000000000 ffff880fe392fe60 ffffffff8103c7e6 ffff880fe391c000 [ 531.766623 ] ffff880fe392ff48 ffffffff81022e81 0000000000000000 0000000000000000 [ 531.774768 ] Call Trace : [ 531.777483 ] < TSK > [ 531.779617 ] [ < ffffffff8103bbea > ] __put_victim + 0x4a / 0x50 [ 531.785433 ] [ < ffffffff8103c729 > ] __victim_flush_func + 0xb9 / 0x110 [ 531.792027 ] [ < ffffffff8103c780 > ] ? __victim_flush_func + 0x110 / 0x110 [ 531.798911 ] [ < ffffffff8103c7e6 > ] victim_flush_async + 0x66 / 0x90 [ 531.805310 ] [ < ffffffff81022e81 > ] kthread + 0x111 / 0x130 [ 531.810836 ] [ < ffffffff81022d70 > ] ? __kthread_parkme + 0x70 / 0x70 [ 531.817236 ] [ < ffffffff8100eb32 > ] ret_from_fork + 0x22 / 0x30 [ 531.823151 ] < EOT > 0829-w14-14: this looks like a double free, or concurrent eviction. But if you look into the evict code, we will check the Flushed flag. It means another eviction routine should have skipped this line, and will not pick this line to do eviction. Some other possibilities? check until 0829-w14-18 [ 1671.661424 ] ------------ [ cut here ] ------------ [ 1671.666378 ] BUG : failure at managers / processor / pcache / victim . c : 610 / victim_finish_insert () ! [ 1671.675591 ] Kernel Panic - not syncing : BUG ! [ 1671.680339 ] CPU : 20 PID : 31 Comm : python 4.0.0 - lego + # 426 [ 1671.686351 ] Stack : [ 1671.688581 ] ffff880fbe76fda0 ffffffff810289b7 ffffffff00000008 ffff880fbe76fdb0 [ 1671.696725 ] ffff880fbe76fd68 ffffff0021475542 ffff88107fd45e00 ffff880fbe753000 [ 1671.704870 ] 0000000000000000 0000000000000001 ffff880fbe76f9b0 ffffffff8101b1b7 [ 1671.713015 ] ffff88107fd44980 ffff880fbe76f9d8 ffffffff8101405f 0000000000000000 [ 1671.721160 ] 0000000000000001 ffff880ff992a000 0000000000000001 ffff880fbe76f9f0 [ 1671.729304 ] Call Trace : [ 1671.732019 ] < TSK > [ 1671.734153 ] [ < ffffffff810289c3 > ] panic + 0xc2 / 0x10a [ 1671.739388 ] [ < ffffffff8101b1b7 > ] ? scheduler_tick + 0x57 / 0x60 [ 1671.745593 ] [ < ffffffff8101405f > ] ? generic_smp_call_function_single_interrupt + 0x8f / 0x160 [ 1671.754611 ] [ < ffffffff8100339e > ] ? call_function_interrupt + 0x2e / 0x40 [ 1671.761688 ] [ < ffffffff8100e9fa > ] ? smp__call_function_interrupt + 0x6a / 0x70 [ 1671.769251 ] [ < ffffffff8101f4bb > ] ? debug_spin_lock + 0x1b / 0x50 [ 1671.775555 ] [ < ffffffff81075efc > ] ? fit_internal_poll_sendcq + 0x6c / 0x140 [ 1671.782826 ] [ < ffffffff81042039 > ] ? find_next_bit + 0x19 / 0x20 [ 1671.788934 ] [ < ffffffff8101f4bb > ] ? debug_spin_lock + 0x1b / 0x50 [ 1671.795236 ] [ < ffffffff8101dcac > ] ? task_tick_rt + 0x2c / 0xd0 [ 1671.801248 ] [ < ffffffff8101b1b7 > ] ? scheduler_tick + 0x57 / 0x60 [ 1671.807453 ] [ < ffffffff810174d5 > ] ? tick_handle_periodic + 0x45 / 0x70 [ 1671.814240 ] [ < ffffffff81006774 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1671.821029 ] [ < ffffffff8100e8aa > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1671.828300 ] [ < ffffffff81012bc8 > ] ? printk + 0x118 / 0x1b0 [ 1671.833924 ] [ < ffffffff8103c161 > ] victim_finish_insert + 0x171 / 0x180 [ 1671.840711 ] [ < ffffffff8103b2a2 > ] pcache_evict_line + 0xf2 / 0x2e0 [ 1671.847110 ] [ < ffffffff81038d7c > ] pcache_alloc + 0x1ac / 0x380 [ 1671.853122 ] [ < ffffffff8103a10c > ] ? pcache_add_rmap + 0x7c / 0x260 [ 1671.859521 ] [ < ffffffff810382bb > ] common_do_fill_page + 0x2b / 0x1e0 [ 1671.866114 ] [ < ffffffff81038631 > ] pcache_handle_fault + 0x1c1 / 0x620 [ 1671.872804 ] [ < ffffffff81037fc0 > ] ? pcache_meta_to_kva + 0x30 / 0x30 [ 1671.879398 ] [ < ffffffff8101006f > ] do_page_fault + 0xaf / 0x1c0 [ 1671.885410 ] [ < ffffffff8100dedf > ] page_fault + 0x1f / 0x30 0829-w14-16: we got this by having debug_spinlock, and seq_ibapi. This is interesting and serious. I think our general C code is fine.. Should I go check the assembly part? This is the rq->lock? come on\u2026 [ 683.748135 ] ------------------- cut here ------------------- [ 683.754252 ] Possible deadlock happend [ 683.758323 ] Current call stack : [ 683.761815 ] CPU : 4 PID : 39 Comm : python 4.0.0 - lego + # 428 [ 683.767728 ] Stack : [ 683.769959 ] ffff880fc1c1fc38 ffffffff8101f48c ffff88107fc45e00 ffff880fc1c1fc60 [ 683.778103 ] ffffffff8101f4e4 ffff88107fc45e00 ffff880fc23fb000 0000000000000000 [ 683.786247 ] ffff880fc1c1fc80 ffffffff8101b18e ffff88107fc44980 0000000000000004 [ 683.794391 ] ffff880fc1c1fc98 ffffffff810174d5 ffffffff8101dddb ffff880fc1c1fcc0 [ 683.802537 ] ffffffff81006774 ffff88107fc45e00 00000004 a817c800 000000 9 a8a78c5e7 [ 683.810680 ] Call Trace : [ 683.813396 ] < TSK > [ 683.815528 ] [ < ffffffff8101f498 > ] report_deadlock + 0x58 / 0x60 [ 683.821637 ] [ < ffffffff8101f4e4 > ] debug_spin_lock + 0x44 / 0x50 [ 683.827745 ] [ < ffffffff8101b18e > ] scheduler_tick + 0x2e / 0x60 [ 683.833758 ] [ < ffffffff810174d5 > ] tick_handle_periodic + 0x45 / 0x70 [ 683.840351 ] [ < ffffffff8101dddb > ] ? dequeue_task_rt + 0x1b / 0x180 [ 683.846750 ] [ < ffffffff81006774 > ] apic_timer_interrupt + 0x54 / 0x90 [ 683.853343 ] [ < ffffffff8100e8aa > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 683.860421 ] [ < ffffffff8101f4d1 > ] ? debug_spin_lock + 0x31 / 0x50 [ 683.866723 ] [ < ffffffff8101b86e > ] try_to_wake_up + 0xce / 0x1f0 [ 683.872832 ] [ < ffffffff8101b9e4 > ] wake_up_q + 0x54 / 0xc0 [ 683.878358 ] [ < ffffffff81028487 > ] do_futex + 0x407 / 0x620 [ 683.883982 ] [ < ffffffff8103a941 > ] ? pcache_add_rmap + 0xb1 / 0x600 [ 683.890381 ] [ < ffffffff8102870c > ] sys_futex + 0x6c / 0x130 [ 683.896005 ] [ < ffffffff8100ec66 > ] do_syscall_64 + 0x36 / 0xc0 [ 683.901919 ] [ < ffffffff8100db6c > ] entry_SYSCALL64_slow_path + 0x25 / 0x25","title":"Aug 29"},{"location":"lego/log/log-08-2018/#aug-27","text":"There a lot lost CQE cases. This one is about P->M->S. And M lost the CQE for the WQE sent to S. 0827 - w9 - 5 [ 963.304865 ] watchdog : worker [ 0 ] CPU10 stucked [ 963.309712 ] watchdog : common_header [ op = 0x20000000 src_nid : 0 ] [ 963.316210 ] CPU : 10 PID : 20 Comm : thpool - worker0 4.0.0 - lego + # 43 [ 963.322899 ] RIP : 0010 : [ < ffffffff8106ad51 > ] [ < ffffffff8106ad51 > ] fit_send_reply_with_rdma_write_with_imm + 0x2a1 / 0x3a0 [ 963.334632 ] RSP : 0000 : ffff88103ef3fc20 EFLAGS : 000002 87 [ 963.340547 ] RAX : 00000000ff ffb6d4 RBX : 000000000000000 b RCX : 0000000000001770 [ 963.348498 ] RDX : 00000000ff ffa70d RSI : fffffffffffff039 RDI : 0000000000000000 [ 963.356450 ] RBP : ffff88103ef3fcc0 R08 : 000000000000001f R09 : 0000000000000002 [ 963.364400 ] R10 : 00000000 80000000 R11 : 000077ff 80000000 R12 : 0000000000000000 [ 963.372352 ] R13 : ffff88103ef26738 R14 : 00000000000 b3d54 R15 : ffff88103ef25008 [ 963.380303 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 963.389320 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 963.395720 ] CR2 : 0000000000000000 CR3 : 000000000116 a000 CR4 : 00000000000406 a0 [ 963.403671 ] Stack : [ 963.405901 ] 00007ff f000b3d54 ffffffff800b3d54 ffff881000000004 ffff88103ef3fc78 [ 963.414045 ] 0000000 900000000 ffff881000000000 0000100 800000001 ffff88103d216000 [ 963.422191 ] ffff88103eebae48 800 b3d540000011c ffffff9b00000246 ffffea0000000001 [ 963.430337 ] 000000103 d216000 0000000000010 c00 000000000000011 c 000000000000100 8 [ 963.438481 ] 000000000000011 c 000000000000100 8 ffff88103eebae48 ffff88103ef3fd70 [ 963.446626 ] Call Trace : [ 963.449342 ] < TSK > [ 963.451475 ] [ < ffffffff81067c80 > ] ibapi_send_reply_imm + 0x50 / 0xd0 [ 963.458068 ] [ < ffffffff8102e953 > ] ? __storage_read + 0xc3 / 0x120 [ 963.464371 ] [ < ffffffff8102e953 > ] __storage_read + 0xc3 / 0x120 [ 963.470480 ] [ < ffffffff8102e9bf > ] storage_read + 0xf / 0x50 [ 963.476201 ] [ < ffffffff8102eab7 > ] storage_vma_fault + 0xb7 / 0x130 [ 963.482600 ] [ < ffffffff8103262f > ] handle_lego_mm_fault + 0x13f / 0x4a0 [ 963.489389 ] [ < ffffffff8102ecf4 > ] common_handle_p2m_miss . isra .1 + 0x54 / 0xc0 [ 963.496855 ] [ < ffffffff8102edc7 > ] handle_p2m_pcache_miss + 0x67 / 0x2d0 [ 963.503739 ] [ < ffffffff8102bf96 > ] thpool_worker_func + 0x296 / 0x3a0 [ 963.510332 ] [ < ffffffff8102bd00 > ] ? handle_bad_request + 0x40 / 0x40 [ 963.516926 ] [ < ffffffff81020ca6 > ] kthread + 0xf6 / 0x120 [ 963.522357 ] [ < ffffffff81020bb0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 963.528756 ] [ < ffffffff8100e632 > ] ret_from_fork + 0x22 / 0x30 hmm, another on lost CQE happen at P. Today is weird, why we happen to have so many lost CQE today? Think about why CQE is not generated? 0827 - w14 - 6 [ 1185.835707 ] ***** ***** Fail to to get the CQE from send_cq after 20 seconds ! ***** This means the packet was lost and something went wrong ***** with your NIC ... ***** connection_id : 7 dest node : 1 ***** [ 1185.856465 ] IB Stats : [ 1185.858985 ] nr_ib_send_reply : 3452 [ 1185.864221 ] nr_bytes_tx : 506507 [ 1185.869456 ] nr_bytes_rx : 8981004 [ 1185.874692 ] ------------ [ cut here ] ------------ [ 1185.879829 ] WARNING : CPU : 14 PID : 22 at net / lego / fit_internal . c : 1108 fit_internal_poll_sendcq + 0xe5 / 0x140 [ 1185.890399 ] CPU : 14 PID : 22 Comm : python 4.0.0 - lego + # 356 [ 1185.896410 ] Stack : [ 1185.898640 ] ffff88103c49fb30 ffffffff810126f5 ffff88103cb22000 00000004 a817c800 [ 1185.906784 ] 0000010f7139214f 0000000000000007 ffff88103c49fb40 ffffffff810127cf [ 1185.914927 ] ffff88103c49fbf0 ffffffff810724b5 000000023 cb2c280 ffff88103cb2c1f8 [ 1185.923072 ] 00000000000002 86 ffff88103c49fc18 ffff88103cb06000 ffff88103cb2c150 [ 1185.931217 ] 000000000000024 b ffff88108101c7dc ffff88107fce5d80 ffff88103c46f000 [ 1185.939360 ] Call Trace : [ 1185.942075 ] < TSK > [ 1185.944209 ] [ < ffffffff81012701 > ] __warn . constprop .1 + 0x91 / 0xd0 [ 1185.950607 ] [ < ffffffff810127cf > ] warn_slowpath_null + 0xf / 0x20 [ 1185.956909 ] [ < ffffffff810724b5 > ] fit_internal_poll_sendcq + 0xe5 / 0x140 [ 1185.963987 ] [ < ffffffff81019dd5 > ] ? scheduler_tick + 0x55 / 0x60 [ 1185.970192 ] [ < ffffffff81072662 > ] fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 1185.979791 ] [ < ffffffff810741ff > ] fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0 [ 1185.988420 ] [ < ffffffff810368c2 > ] ? __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1185.995401 ] [ < ffffffff810701e9 > ] ibapi_send_reply_timeout + 0x79 / 0x120 [ 1186.002479 ] [ < ffffffff810368c2 > ] ? __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1186.009459 ] [ < ffffffff810368c2 > ] __pcache_do_fill_page + 0xc2 / 0x1d0 [ 1186.016245 ] [ < ffffffff81036ac4 > ] common_do_fill_page + 0xf4 / 0x1f0 [ 1186.022839 ] [ < ffffffff81036d80 > ] pcache_handle_fault + 0x1c0 / 0x610 [ 1186.029528 ] [ < ffffffff81036800 > ] ? __pcache_do_zerofill_page + 0x100 / 0x100 [ 1186.036995 ] [ < ffffffff8100fdff > ] do_page_fault + 0xaf / 0x1c0 [ 1186.043005 ] [ < ffffffff8100dc1f > ] page_fault + 0x1f / 0x30","title":"Aug 27"},{"location":"lego/log/log-08-2018/#aug-26","text":"Oh well. I saw the same damn lost packet issue again. The issue can be desribed as: P use lite rpc to send a request to M. M processed the handled, and called rpc reply to sent back to P. M need to poll send_cq to poll completion. But M fail to get the CQE for the should-be-sent-out WQE. This is tested with M\u2019s CONFIG_FIT_NOWAIT optimization, which is basically an optimization that M will not poll cq every time a reply was sent out, instead, do batch polling. The following stack dump was reported by M side watchdog. It is not necessary mlx4_poll_cq\u2019s issue, since there is a while (1) loop at fit code. Oh well. Log name : 0826 - w9 - 1 [ 187736.669027 ] watchdog : worker [ 0 ] CPU10 stucked [ 187736.673972 ] watchdog : common_header [ op = 0x30000000 src_nid : 0 ] [ 187736.680566 ] CPU : 10 PID : 20 Comm : thpool - worker0 4.0.0 - lego + # 26 [ 187736.687351 ] RIP : 0010 : [ < ffffffff810522c3 > ] [ < ffffffff810522c3 > ] mlx4_ib_poll_cq + 0x1d3 / 0x850 [ 187736.696854 ] RSP : 0000 : ffff88103ef3f750 EFLAGS : 000002 86 [ 187736.702865 ] RAX : 00000000ff fffff5 RBX : 0000000000000000 RCX : ffff88103ed6b050 [ 187736.710913 ] RDX : 00000000 80630000 RSI : 0000000000000001 RDI : ffff88103edb0bf0 [ 187736.718961 ] RBP : ffff88103ef3f7b8 R08 : 0000000000000020 R09 : 0000000000000002 [ 187736.727007 ] R10 : 0000000ff c53fddc R11 : 0000000040 bf1040 R12 : ffff88103ef3f7c8 [ 187736.735055 ] R13 : 0000000000000000 R14 : 0000000000000000 R15 : ffff88103edb0bf0 [ 187736.743104 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fca0000 ( 0000 ) knlGS : 0000000000000000 [ 187736.752218 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 187736.758714 ] CR2 : 0000000000000000 CR3 : 000000000116 a000 CR4 : 00000000000406 a0 [ 187736.766762 ] Stack : [ 187736.769089 ] 0000000ff c53fddc 0000000000000002 0000000000000020 ffff88103edb0c98 [ 187736.777331 ] 00000000000002 86 00000000 80630000 ffff88103ef3f7d0 000063 8000000018 [ 187736.785572 ] ffff88103edb0bf0 0000000000000001 ffff88103ef25008 0000000000000003 [ 187736.793813 ] 000000000000000 c ffff88103ef3fd30 ffffffff8106920c ffff88103ef3fd54 [ 187736.802054 ] 0000000100000000 0000000100000000 ffff88103edb07b0 ffff88103e81b008 [ 187736.810296 ] Call Trace : [ 187736.813108 ] < TSK > [ 187736.815338 ] [ < ffffffff8106920c > ] fit_internal_poll_sendcq + 0x6c / 0xe0 [ 187736.822416 ] [ < ffffffff8106ab2f > ] ? fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0 [ 187736.831336 ] [ < ffffffff81033ff0 > ] ? _lego_copy_to_user + 0x110 / 0x250 [ 187736.838220 ] [ < ffffffff81028d65 > ] ? __free_pages + 0x25 / 0x30 [ 187736.844329 ] [ < ffffffff8102e981 > ] ? __storage_read + 0xf1 / 0x120 [ 187736.850728 ] [ < ffffffff81019865 > ] ? scheduler_tick + 0x55 / 0x60 [ 187736.857031 ] [ < ffffffff810693d2 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.866920 ] [ < ffffffff810693d2 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.876810 ] [ < ffffffff8103043f > ] ? __vma_adjust + 0x38f / 0x550 [ 187736.883113 ] [ < ffffffff81030944 > ] ? vma_merge + 0x1a4 / 0x280 [ 187736.889123 ] [ < ffffffff81030f20 > ] ? arch_get_unmapped_area_topdown + 0xe0 / 0x220 [ 187736.897075 ] [ < ffffffff810693d2 > ] fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350 [ 187736.906771 ] [ < ffffffff81069ab5 > ] fit_ack_reply_callback + 0x185 / 0x1e0 [ 187736.913848 ] [ < ffffffff8102f129 > ] ? handle_p2m_flush_one + 0x69 / 0x160 [ 187736.920830 ] [ < ffffffff8102bde0 > ] thpool_worker_func + 0xe0 / 0x3a0 [ 187736.927424 ] [ < ffffffff8102bd00 > ] ? handle_bad_request + 0x40 / 0x40 [ 187736.934113 ] [ < ffffffff81020ca6 > ] kthread + 0xf6 / 0x120 [ 187736.939639 ] [ < ffffffff81020bb0 > ] ? __kthread_parkme + 0x70 / 0x70 [ 187736.946137 ] [ < ffffffff8100e632 > ] ret_from_fork + 0x22 / 0x30","title":"Aug 26"},{"location":"lego/log/log-08-2018/#aug-22","text":"Damn it!!! After so much effort verifying we had a solid IB stack, we still has memory corruption and deadlock issues. Fuck! One thing at a time, simple stuff first. Okay, tomorrow first add DEBUG_SPINLOCK to detect possible deadlocks. This, could help to identify some buggy code. After this, I will spend some time looking into the LITE, it\u2019s fucking HEAVY. I do found a lot issues during summer. Personally, I\u2019m not feeling good this days. I treat someone with love and respect, but there is not too much in return. Yeahyeahyeah, I know how this works. It\u2019s just sad that sometimes you just have a BAD timing. I\u2019ve went through too much things in 2018, good and bad. I care sooo much about the people I love, family and others. I feel this is good, of course. Anyway, it is supposed to be a Lego dump, that no one probably interested in.","title":"Aug 22"},{"location":"lego/log/log-09-2018/","text":"Sep 2018 \u00b6 Sep 20 \u00b6 [ 54.602054 ] nr_pcache_pee_free : 0 [ 54.602537 ] nr_pcache_pee_free_kmalloc : 0 [ 1468.765410 ] mlx4_msi_x_interrupt () : IRQ : 27 CPU : 1 [ 1468.766956 ] event PORT_MNG_CHG arrived [ 1468.768193 ] < mlx4_ib > handle_port_mgmt_change_event : rereg [ 1468.813660 ] ib_cache : ib_cache_update () : Updated port 1 of dev 0000 : 00 : 08.0 [ 1468.815097 ] ib_sa_event () : TODO [ 1479.178651 ] mlx4_msi_x_interrupt () : IRQ : 27 CPU : 1 [ 1479.180201 ] event PORT_MNG_CHG arrived [ 1479.181430 ] < mlx4_ib > handle_port_mgmt_change_event : rereg [ 1479.190813 ] bad : scheduling from the idle thread ! [ 1479.192158 ] CPU : 1 PID : 0 Comm : swapper / 1 4.0.0 - lego + # 146 [ 1479.193622 ] Stack : [ 1479.194408 ] ffff88083fddf980 ffffffff8101eefc ffff88083fc45d80 ffff88083fc45d80 [ 1479.196826 ] ffff88083fddf9a8 ffffffff8101ace4 00000001 810067 d4 ffff88083fe43000 [ 1479.199226 ] ffffffffffff0000 ffff88083fddf9e0 ffffffff81078bf6 ffffffff8100e8ea [ 1479.203615 ] ffffffffffff0000 0000000000000000 ffff88083fe43000 ffff88083fe43000 [ 1479.206532 ] ffff88083fddf9f8 ffffffff81078ca3 7ff fffffffffffff ffff88083fddfa68 [ 1479.208791 ] Call Trace : [ 1479.209606 ] < TSK > [ 1479.210322 ] [ < ffffffff8101ef08 > ] dequeue_task_idle + 0x48 / 0x60 [ 1479.211726 ] [ < ffffffff8101ace4 > ] deactivate_task + 0x44 / 0x50 [ 1479.213092 ] [ < ffffffff81078bf6 > ] __schedule + 0x146 / 0x1e0 [ 1479.214410 ] [ < ffffffff8100e8ea > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1479.215960 ] [ < ffffffff81078ca3 > ] schedule + 0x13 / 0x30 [ 1479.217211 ] [ < ffffffff810789da > ] schedule_timeout + 0x12a / 0x1a0 [ 1479.218625 ] [ < ffffffff81079e54 > ] __down_common + 0xaa / 0x103 [ 1479.219904 ] [ < ffffffff81079ec5 > ] __down + 0x18 / 0x1a [ 1479.221046 ] [ < ffffffff8101f24c > ] down + 0x3c / 0x40 [ 1479.222163 ] [ < ffffffff8104dba7 > ] __mlx4_cmd + 0x1d7 / 0x3c0 [ 1479.223397 ] [ < ffffffff810619de > ] mlx4_MAD_IFC + 0x22e / 0x490 [ 1479.224666 ] [ < ffffffff8105d321 > ] __mlx4_ib_query_pkey + 0x181 / 0x240 [ 1479.226045 ] [ < ffffffff8105d3f3 > ] mlx4_ib_query_pkey + 0x13 / 0x20 [ 1479.227365 ] [ < ffffffff81064cb4 > ] ib_query_pkey + 0x14 / 0x20 [ 1479.228617 ] [ < ffffffff810651a7 > ] ib_cache_update + 0x237 / 0x480 [ 1479.229862 ] [ < ffffffff810657f8 > ] ib_cache_event + 0x28 / 0x30 [ 1479.231026 ] [ < ffffffff81064bf0 > ] ib_dispatch_event + 0x40 / 0x70 [ 1479.232222 ] [ < ffffffff810627c8 > ] handle_port_mgmt_change_event + 0x158 / 0x1c0 [ 1479.233602 ] [ < ffffffff8105b5ac > ] mlx4_ib_event + 0x7c / 0xa0 [ 1479.234744 ] [ < ffffffff8104ee55 > ] mlx4_dispatch_event + 0x65 / 0x90 [ 1479.235968 ] [ < ffffffff8104f2c3 > ] mlx4_eq_int + 0x273 / 0x4f0 [ 1479.237113 ] [ < ffffffff8104f616 > ] mlx4_msi_x_interrupt + 0x36 / 0x40 [ 1479.238352 ] [ < ffffffff81017894 > ] handle_irq_event_percpu + 0x24 / 0xa0 [ 1479.239584 ] [ < ffffffff81017938 > ] handle_irq_event + 0x28 / 0x50 [ 1479.240696 ] [ < ffffffff810180fe > ] handle_edge_irq + 0x5e / 0xc0 [ 1479.241794 ] [ < ffffffff810054c3 > ] do_IRQ + 0x43 / 0xd0 [ 1479.242779 ] [ < ffffffff810067d4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1479.243971 ] [ < ffffffff8100e0aa > ] common_interrupt + 0x6a / 0x6a [ 1479.245084 ] [ < ffffffff8101c6b0 > ] ? cpu_idle + 0x10 / 0x30 [ 1479.246123 ] [ < ffffffff81003425 > ] start_secondary_cpu + 0x55 / 0x60 [ 1479.247278 ] < EOT > Sep 17 \u00b6 Can not believe I\u2019m wasting time on this crap X again. Sep 16 \u00b6 Tests done today: Setting Log nr_workers Tracing (strace/counter/profiling) Runtime (s) pcache_flush_net (us) TF-MNIST, Linux 13.2s TF4-MNIST, 128MB 0916-w14-1 1 ON avg 48.5s 9891 TF4-MNIST, 128MB 0916-w14-2 1 OFF (46.1+44.6+45.5+45.7+44)/5 = 45.2s N/A TF4-MNIST, 128MB 0916-w14-4 4 ON (43.4+44+43.9+42.6+42.1)/5=43.2 8351 TF4-MNIST, 128MB 0916-w14-3 4 OFF (40.1+42.1+42.0+41.7+42.1)/5 = 41.6 N/A TF4-Cifar, Linux 235.5s TF4-Cifar, 128MB 0916-w14-5 4 OFF (636.2+635.0+636.8+637.2+634.1)/5=635.8 N/A TF4-Cifar, 128MB 0916-w14-6 1 OFF (660.2+662.2+662.8+663.8+661+5)/5=663s N/A TF4-Cifar, 256MB 0916-w14-7 1 OFF 486s N/A Sep 15 \u00b6 DAMN. Let us summarize today. Okay. Fixed the double-post-cqe issue. Hehe. The post part is the only fucking left code that I did not look into at fit_poll_recv_cq. And, ironically, there is no error checking for ib_post_recv(), which won\u2019t generate any error/warning. error checking error checking\u2026 Anyway fuck it. Today I created a new tag v0.0.9, hope we have a stable net. The RPC profile code is very stressing, and fit survived. The following wanring is fixed by post rx_depth/2. [ 1812.017204 ] fit : To align first QPN , we skipped : # 72 # 72 # 73 # 74 # 75 # 76 # 77 # 78 # 79 [ 1812.157570 ] fit : fit_post_receives_message () - 628 CPU 2 Fail to post recv conn_id : 12 [ 1812.166013 ] ------------ [ cut here ] ------------ [ 1812.171152 ] WARNING : CPU : 2 PID : 16 at net / lego / fit_internal . c : 629 fit_post_receives_message . isra .7 + 0xce / 0x100 [ 1812.182302 ] CPU : 2 PID : 16 Comm : ib - initd 4.0.0 - lego + # 95 [ 1812.188314 ] Stack : [ 1812.190544 ] ffff880ff98bfd50 ffffffff8101299b 0000000000000 cff 0000000000000060 [ 1812.198689 ] 0000000000000 d00 0000000000000100 ffff880ff98dc030 ffff880ff98bfd60 [ 1812.206834 ] ffffffff81012a8f ffff880ff98bfdc8 ffffffff810743de fffffff4fffffff4 [ 1812.214978 ] ffff880ff98bfd80 0000000000000000 0000000000000 cff 0000000000000000 [ 1812.223124 ] 0000000000000000 ffff880ff98dc000 0000000000000000 000000000000000 c [ 1812.231269 ] Call Trace : [ 1812.233984 ] < TSK > [ 1812.236116 ] [ < ffffffff810129a7 > ] __warn . constprop .0 + 0xa7 / 0x100 [ 1812.242613 ] [ < ffffffff81012a8f > ] warn_slowpath_null + 0xf / 0x20 [ 1812.248915 ] [ < ffffffff810743de > ] fit_post_receives_message . isra .7 + 0xce / 0x100 [ 1812.256770 ] [ < ffffffff81076a1a > ] fit_add_newnode + 0xca / 0x170 [ 1812.262974 ] [ < ffffffff81079d10 > ] fit_establish_conn + 0x7b0 / 0xaa0 [ 1812.269568 ] [ < ffffffff81073ce8 > ] ? ibv_add_one + 0x98 / 0x120 [ 1812.275580 ] [ < ffffffff810741f0 > ] ? ibapi_get_node_id + 0x20 / 0x20 [ 1812.282076 ] [ < ffffffff81074258 > ] lego_ib_init + 0x68 / 0xf0 [ 1812.287893 ] [ < ffffffff81023261 > ] kthread + 0x111 / 0x130 [ 1812.293421 ] [ < ffffffff81023150 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1812.299820 ] [ < ffffffff8100eaf2 > ] ret_from_fork + 0x22 / 0x30 [ 1812.305735 ] < EOT > [ 1812.307868 ] --- [ end trace 0000000000000000 ] --- Sep 11 \u00b6 Got this log, 5 machine, p2s_open, S side has this issue. Damn. [ 1672.962279 ] ***** ***** Fail to to get the CQE from send_cq after 20 seconds ! ***** This means the packet was lost and something went wrong ***** with your NIC ... ***** connection_id : 11 dest node : 0 ***** [ 1673.061668 ] ------------ [ cut here ] ------------ [ 1673.074937 ] WARNING : CPU : 10 PID : 4624 at / root / ys / LegoOS_2M / linux - modules / fit / fit_internal . c : 956 fit_internal_poll_sendcq + 0xda / 0x130 [ fit ]() [ 1673.101557 ] Modules linked in : storage ( OF ) fit ( OF ) xt_CHECKSUM iptable_mangle ipt_MASQUERADE iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT tun bridge stp llc ebtable_filter ebtable s ip6table_filter ip6_tables iptable_filter xprtrdma sunrpc ib_isert iscsi_target_mod ib_iser libiscsi scsi_transport_iscsi ib_srpt target_core_mod ib_srp scsi_transport_srp scsi_tgt ib_ipoib rdma_ucm ib_ucm ib_uverbs ib_umad rdma_cm ib_c m iw_cm ib_addr x86_pkg_temp_thermal coretemp kvm_intel kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ipmi_devintf ablk_helper cryptd ipmi_si iTCO_wdt ipmi_msghandler iTCO_vendor_support dcdbas sg pcspkr shpchp acpi_power_meter lpc_ich mfd_core wmi mperf uinput binfmt_misc ip_tables ext4 mbcache jbd2 mlx4_ib [ 1673.182609 ] ib_sa ib_mad ib_core mlx4_en sd_mod crc_t10dif mgag200 syscopyarea sysfillrect sysimgblt i2c_algo_bit drm_kms_helper ttm drm ahci crc32c_intel libahci mlx4_core libata tg3 nvme megaraid_sas ptp i2c_core pps_core dm_mirror dm_region_hash dm_log dm_mod [ 1673.222604 ] CPU : 10 PID : 4624 Comm : lego - storaged Tainted : GF W O 3.11.1 - vanilla # 1 [ 1673.235825 ] Hardware name : Dell Inc . PowerEdge R730 / 05 99 V5 , BIOS 1.5.4 10 / 002 / 2015 [ 1673.248883 ] 000000000000000 9 ffff88102186b9f8 ffffffff8159a5a4 0000000000000000 [ 1673.261795 ] ffff88102186ba30 ffffffff810641bd ffff882027180400 00000004 a817c800 [ 1673.274499 ] 000001 80 dc3abde5 0000000000000000 0000000000000000 ffff88102186ba40 [ 1673.287034 ] Call Trace : [ 1673.299259 ] [ < ffffffff8159a5a4 > ] dump_stack + 0x45 / 0x56 [ 1673.311371 ] [ < ffffffff810641bd > ] warn_slowpath_common + 0x7d / 0xa0 [ 1673.323268 ] [ < ffffffff8106429a > ] warn_slowpath_null + 0x1a / 0x20 [ 1673.334892 ] [ < ffffffffa063669a > ] fit_internal_poll_sendcq + 0xda / 0x130 [ fit ] [ 1673.346348 ] [ < ffffffff81093e25 > ] ? check_preempt_curr + 0x85 / 0xa0 [ 1673.357575 ] [ < ffffffffa06367f7 > ] fit_send_message_with_rdma_write_with_imm_request + 0x107 / 0x3f0 [ fit ] [ 1673.368777 ] [ < ffffffff8107bde4 > ] ? wake_up_worker + 0x24 / 0x30 [ 1673.379741 ] [ < ffffffffa0636ee9 > ] fit_reply_message + 0x89 / 0xa0 [ fit ] [ 1673.390497 ] [ < ffffffffa063507b > ] ibapi_reply_message + 0x1b / 0x20 [ fit ] [ 1673.401039 ] [ < ffffffffa0646785 > ] handle_open_request + 0xa5 / 0xe0 [ storage ] [ 1673.411367 ] [ < ffffffffa0646106 > ] storage_manager + 0x106 / 0x300 [ storage ] [ 1673.421470 ] [ < ffffffffa0646000 > ] ? 0xffffffffa0645fff [ 1673.431297 ] [ < ffffffffa0646000 > ] ? 0xffffffffa0645fff [ 1673.440797 ] [ < ffffffff81085ec0 > ] kthread + 0xc0 / 0xd0 [ 1673.450034 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1673.459063 ] [ < ffffffff815a94ac > ] ret_from_fork + 0x7c / 0xb0 [ 1673.467837 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1673.476400 ] --- [ end trace f9b19a31d409f910 ] --- [ 1695.867276 ] storage_self_monitor () : in_handler = 1 [ 1695.875906 ] handle_replica_flush : 0 [ 1695.884613 ] handle_replica_vma : 0 [ 1695.893265 ] handle_replica_read : 12740 [ 1695.901920 ] handle_replica_write : 0 [ 1713.012565 ] INFO : rcu_sched self - detected stall on CPU { 10 } ( t = 60001 jiffies g = 7646 c = 7645 q = 0 ) [ 1713.013339 ] sending NMI to all CPUs : [ 1713.013573 ] INFO : rcu_sched detected stalls on CPUs / tasks : { 10 } ( detected by 15 , t = 60002 jiffies , g = 7646 , c = 7645 , q = 0 ) [ 1713.014807 ] NMI backtrace for cpu 0 [ 1713.015685 ] CPU : 0 PID : 4591 Comm : wq_handler Tainted : GF W O 3.11.1 - vanilla # 1 [ 1713.016624 ] Hardware name : Dell Inc . PowerEdge R730 / 05 99 V5 , BIOS 1.5.4 10 / 002 / 2015 [ 1713.017575 ] task : ffff88201f193b40 ti : ffff88101a34a000 task . ti : ffff88101a34a000 [ 1713.018530 ] RIP : 0010 : [ < ffffffffa0636b55 > ] [ < ffffffffa0636b55 > ] waiting_queue_handler + 0x75 / 0x140 [ fit ] [ 1713.018530 ] RIP : 0010 : [ < ffffffffa0636b55 > ] [ < ffffffffa0636b55 > ] waiting_queue_handler + 0x75 / 0x140 [ fit ] [ 1713.019512 ] RSP : 001 8 : ffff88101a34be78 EFLAGS : 000002 96 [ 1713.020444 ] RAX : 00000000000 80080 RBX : ffff8820200253f0 RCX : ffff88201f193b40 [ 1713.021364 ] RDX : 0000000000000001 RSI : ffff88103f414760 RDI : ffff88103f4146c0 [ 1713.022260 ] RBP : ffff88101a34bec8 R08 : 0000000000000000 R09 : 0000000000000001 [ 1713.023138 ] R10 : 0000000000000001 R11 : ffffffffa0636b55 R12 : ffff8820200253c0 [ 1713.024000 ] R13 : ffff881022005000 R14 : ffffffffa063b8e4 R15 : ffffffffa063b8e4 [ 1713.024839 ] FS : 0000000000000000 ( 0000 ) GS : ffff88103f400000 ( 0000 ) knlGS : 0000000000000000 [ 1713.025676 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1713.026481 ] CR2 : 00007f d77ef46000 CR3 : 0000000001 876000 CR4 : 00000000001407f 0 [ 1713.027273 ] Stack : [ 1713.028035 ] ffff881000000000 ffff881000300660 ffff882000000003 0000000000000000 [ 1713.028818 ] ffff881000000000 ffff881021eafc38 ffff881022005000 ffffffffa0636ae0 [ 1713.029585 ] 0000000000000000 0000000000000000 ffff88101a34bf48 ffffffff81085ec0 [ 1713.030350 ] Call Trace : [ 1713.031098 ] [ < ffffffffa0636ae0 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x3f0 / 0x3f0 [ fit ] [ 1713.031875 ] [ < ffffffff81085ec0 > ] kthread + 0xc0 / 0xd0 [ 1713.032641 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1713.033407 ] [ < ffffffff815a94ac > ] ret_from_fork + 0x7c / 0xb0 [ 1713.034171 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 Sep 08 \u00b6 Check this log out: ]--- [ 427.218569] STDOUT: ---[ INFO:tensorflow:Graph was finalized. ]--- [ 427.416043] BUG: unable to handle kernel NULL pointer dereference at (null) [ 427.424583] IP: [<ffffffff810748fb>] fit_poll_recv_cq+0x5cb/0x860 [ 427.431370] mlx4_msi_x_interrupt(): IRQ: 27 CPU: 0 [ 427.436702] PGD 0 [ 427.438932] CQ_ERROR CQ overrun on CQN 000082 [ 427.443780] Oops: 0002 [#1] SMP PROCESSOR [ 427.448240] event qp_event arrived [ 427.452022] CPU: 6 PID: 18 Comm: FIT_RecvCQ-0 4.0.0-lego+ #23 [ 427.458421] event qp_event arrived [ 427.462203] RIP: 0010:[<ffffffff810748fb>] [<ffffffff810748fb>] fit_poll_recv_cq+0x5cb/0x860 [ 427.471704] RSP: 0000:ffff881023e3fe60 EFLAGS: 00010287 [ 427.477618] RAX: 0000000000000000 RBX: 000000002aaaaaab RCX: 0000000000000004 [ 427.485570] RDX: 0000000000000000 RSI: 0000000000000053 RDI: 0000000000000000 [ 427.493520] RBP: ffff881023e3fec0 R08: 0000000000000001 R09: ffff881039900000 [ 427.501470] R10: 0000000000000000 R11: ffff881039918000 R12: ffff8810398f2000 [ 427.509421] R13: 0000000000000000 R14: 0000000000000001 R15: ffff881023e25008 [ 427.517371] event qp_event arrived [ 427.521153] FS: 0000000000000000(0000) GS:ffff88107fc60000(0000) knlGS:0000000000000000 [ 427.530169] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 427.536569] CR2: 0000000000000000 CR3: 000000000117a000 CR4: 00000000000406a0 [ 427.544519] event qp_event arrived Trying to tune FIT\u2019s number polling threads. This could be the throughput/latency killer. 128M P num_polling M worker M num_polling Runtime (s) 1 1 1 46.8s 1 4 1 Sep 07 \u00b6 Set up Infiniswap again. What a fucking crap code, and crash the kernel out of nowhere. crap crap crap. Hmm, Linux will tune the CPU freq during runtime, will be higher than 2.4GHz. So disable it, make it a fair comparison with Lego. intel_pstate=disable. Sep 06 \u00b6 Did two optimizations on pcache, both are buffer management. Especially the pcache rmap case. In both opts, we kind of use static/pre-allocated array to serve dynamic allocation. This is a better solution than using kmem_cache, faster. kmem_cache will be a more general solution here. kmem_cache, FIFO queue (thpool buffer), static preallocated array (rmap, clflush)\u2026 Buffer management is really a very important thing in system building. I should be aware at the beginning next time. These changes are in commits: 6e0cf6c5c64edbe445a27cf55f86ac51f8a897b3 73377cafce95ffa0cfb155f77cac97456a5e4a71 Sep 05 \u00b6 Alright. Besides some flaws/bugs in some kfree stuff, LegoOS now actually is very robust! Ran a quick git summary: project : LegoOS repo age : 1 year, 11 months active : 358 days commits : 1540 files : 1161 authors : 1317 Yizhou Shan 85.5% 120 root 7.8% 36 hythzz 2.3% 27 yilun 1.8% 16 Yutong Huang 1.0% 10 Build Android 0.6% 8 Yiying Zhang 0.5% 4 sumukh1991 0.3% 1 Yizhou SHan 0.1% 1 Sumukh Hallymysore Ravindra 0.1% Of course, there are still PLENY room for improvement, and I know where. At this time, I really think we need something like kmem_cache, which is so fucking useful. It can probably further reduce much overhead. Sep 04 \u00b6 Trying the perset eviction list mechanism, instead of victim cache. The benefit of using this is: we will no longer be bottelnecked by victim cache anymore. Each faulting thread will do eviction/flush within its own context. For 4 threads MNIST, I saw 3 seconds reduction. Removed the bitmap, use per pcache set counter for quick reference. Sep 03 \u00b6 With DEBUG_MM, try enable HAVE_FREE directory by directory - \u00b6 - update_wall_time+0x44 is where we call tsc_read. And this has been called many times (HZ per second). All of a sudden, the pointer got crashed. Who wrote to this code memory?? Remote RDMA? [ 1052.470714 ] general protection fault : 0000 [ # 1 ] SMP PROCESSOR [ 1052.477113 ] CPU : 0 PID : 15 Comm : ib_mad1 4.0.0 - lego + # 509 [ 1052.483125 ] RIP : 0010 : [ < ffffffff81015764 > ] [ < ffffffff81015764 > ] update_wall_time + 0x44 / 0x6f0 [ 1052.492530 ] RSP : 0000 : ffff88103ad9fc88 EFLAGS : 00010046 [ 1052.498445 ] RAX : 4510ff ffffff8118 RBX : 0380ff ffffffffff RCX : 0000000000000001 [ 1052.506396 ] RDX : ffff88103ad9fd28 RSI : 0000000000000000 RDI : 4510ff ffffff8118 [ 1052.514346 ] RBP : ffff88103ad9fcd0 R08 : 000000000000001f R09 : 0000000000000000 [ 1052.522298 ] R10 : 000000000000002 9 R11 : ffff881013f8e130 R12 : aaff0000024a2677 [ 1052.530248 ] R13 : 0000000000000000 R14 : ffff88103ad85228 R15 : ffff88103ae0c000 [ 1052.538199 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 1052.547216 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1052.553616 ] CR2 : 0000000000000000 CR3 : 000000000117 b000 CR4 : 00000000000406 b0 [ 1052.561567 ] Stack : [ 1052.563797 ] 00000000000000 86 ffff88107fc05d80 ffff88103ad85000 0000000000000000 [ 1052.571941 ] ffff88107fc04980 0000000000000000 0000000000000000 ffff88103ad85228 [ 1052.580085 ] ffff88103ae0c000 ffff88103ad9fce8 ffffffff81017557 000000003 ad9fe10 [ 1052.588230 ] ffff88103ad9fd10 ffffffff810067a4 ffffffff81088040 ffff88107fc05d80 [ 1052.596375 ] ffff88103ad85000 ffff88103ad9fdf8 ffffffff8100e8ea ffff88103ad9fd28 [ 1052.604520 ] Call Trace : [ 1052.607236 ] < TSK > [ 1052.609368 ] [ < ffffffff81017557 > ] tick_handle_periodic + 0x67 / 0x70 [ 1052.615961 ] [ < ffffffff810067a4 > ] apic_timer_interrupt + 0x54 / 0x90 [ 1052.622555 ] [ < ffffffff8100e8ea > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 1052.629633 ] [ < ffffffff8107b488 > ] ? __schedule + 0xf8 / 0x1e0 [ 1052.635548 ] [ < ffffffff8107b583 > ] schedule + 0x13 / 0x30 [ 1052.640978 ] [ < ffffffff8106c98e > ] ib_mad_completion_handler + 0x5de / 0xc20 [ 1052.648250 ] [ < ffffffff8101de3b > ] ? dequeue_task_rt + 0x1b / 0x180 [ 1052.654648 ] [ < ffffffff8106c3b0 > ] ? ib_mad_send_done_handler . isra .22 + 0x4e0 / 0x4e0 [ 1052.662793 ] [ < ffffffff81022af6 > ] kthread + 0xf6 / 0x110 [ 1052.668223 ] [ < ffffffff81022a00 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1052.674622 ] [ < ffffffff8100eb72 > ] ret_from_fork + 0x22 / 0x30 [ 1052.680538 ] < EOT > [ 1052.682670 ] Code : db e4 16 00 79 0 d f3 90 80 3 d d0 e4 16 00 00 7 e f5 eb ea 48 8 b 1 d fd fa 1f 00 48 8 b 05 e6 fa 1f 00 4 c 8 b 25 f7 fa 1f 00 48 89 c7 < ff > 50 28 49 89 c7 48 89 d8 4 d 29 e7 48 d1 e8 49 21 df 48 f7 d0 [ 1052.703711 ] RIP [ < ffffffff81015764 > ] update_wall_time + 0x44 / 0x6f0 [ 1052.710498 ] RSP < ffff88103ad9fc88 >","title":"Sep 2018"},{"location":"lego/log/log-09-2018/#sep-2018","text":"","title":"Sep 2018"},{"location":"lego/log/log-09-2018/#sep-20","text":"[ 54.602054 ] nr_pcache_pee_free : 0 [ 54.602537 ] nr_pcache_pee_free_kmalloc : 0 [ 1468.765410 ] mlx4_msi_x_interrupt () : IRQ : 27 CPU : 1 [ 1468.766956 ] event PORT_MNG_CHG arrived [ 1468.768193 ] < mlx4_ib > handle_port_mgmt_change_event : rereg [ 1468.813660 ] ib_cache : ib_cache_update () : Updated port 1 of dev 0000 : 00 : 08.0 [ 1468.815097 ] ib_sa_event () : TODO [ 1479.178651 ] mlx4_msi_x_interrupt () : IRQ : 27 CPU : 1 [ 1479.180201 ] event PORT_MNG_CHG arrived [ 1479.181430 ] < mlx4_ib > handle_port_mgmt_change_event : rereg [ 1479.190813 ] bad : scheduling from the idle thread ! [ 1479.192158 ] CPU : 1 PID : 0 Comm : swapper / 1 4.0.0 - lego + # 146 [ 1479.193622 ] Stack : [ 1479.194408 ] ffff88083fddf980 ffffffff8101eefc ffff88083fc45d80 ffff88083fc45d80 [ 1479.196826 ] ffff88083fddf9a8 ffffffff8101ace4 00000001 810067 d4 ffff88083fe43000 [ 1479.199226 ] ffffffffffff0000 ffff88083fddf9e0 ffffffff81078bf6 ffffffff8100e8ea [ 1479.203615 ] ffffffffffff0000 0000000000000000 ffff88083fe43000 ffff88083fe43000 [ 1479.206532 ] ffff88083fddf9f8 ffffffff81078ca3 7ff fffffffffffff ffff88083fddfa68 [ 1479.208791 ] Call Trace : [ 1479.209606 ] < TSK > [ 1479.210322 ] [ < ffffffff8101ef08 > ] dequeue_task_idle + 0x48 / 0x60 [ 1479.211726 ] [ < ffffffff8101ace4 > ] deactivate_task + 0x44 / 0x50 [ 1479.213092 ] [ < ffffffff81078bf6 > ] __schedule + 0x146 / 0x1e0 [ 1479.214410 ] [ < ffffffff8100e8ea > ] ? smp__apic_timer_interrupt + 0x6a / 0x70 [ 1479.215960 ] [ < ffffffff81078ca3 > ] schedule + 0x13 / 0x30 [ 1479.217211 ] [ < ffffffff810789da > ] schedule_timeout + 0x12a / 0x1a0 [ 1479.218625 ] [ < ffffffff81079e54 > ] __down_common + 0xaa / 0x103 [ 1479.219904 ] [ < ffffffff81079ec5 > ] __down + 0x18 / 0x1a [ 1479.221046 ] [ < ffffffff8101f24c > ] down + 0x3c / 0x40 [ 1479.222163 ] [ < ffffffff8104dba7 > ] __mlx4_cmd + 0x1d7 / 0x3c0 [ 1479.223397 ] [ < ffffffff810619de > ] mlx4_MAD_IFC + 0x22e / 0x490 [ 1479.224666 ] [ < ffffffff8105d321 > ] __mlx4_ib_query_pkey + 0x181 / 0x240 [ 1479.226045 ] [ < ffffffff8105d3f3 > ] mlx4_ib_query_pkey + 0x13 / 0x20 [ 1479.227365 ] [ < ffffffff81064cb4 > ] ib_query_pkey + 0x14 / 0x20 [ 1479.228617 ] [ < ffffffff810651a7 > ] ib_cache_update + 0x237 / 0x480 [ 1479.229862 ] [ < ffffffff810657f8 > ] ib_cache_event + 0x28 / 0x30 [ 1479.231026 ] [ < ffffffff81064bf0 > ] ib_dispatch_event + 0x40 / 0x70 [ 1479.232222 ] [ < ffffffff810627c8 > ] handle_port_mgmt_change_event + 0x158 / 0x1c0 [ 1479.233602 ] [ < ffffffff8105b5ac > ] mlx4_ib_event + 0x7c / 0xa0 [ 1479.234744 ] [ < ffffffff8104ee55 > ] mlx4_dispatch_event + 0x65 / 0x90 [ 1479.235968 ] [ < ffffffff8104f2c3 > ] mlx4_eq_int + 0x273 / 0x4f0 [ 1479.237113 ] [ < ffffffff8104f616 > ] mlx4_msi_x_interrupt + 0x36 / 0x40 [ 1479.238352 ] [ < ffffffff81017894 > ] handle_irq_event_percpu + 0x24 / 0xa0 [ 1479.239584 ] [ < ffffffff81017938 > ] handle_irq_event + 0x28 / 0x50 [ 1479.240696 ] [ < ffffffff810180fe > ] handle_edge_irq + 0x5e / 0xc0 [ 1479.241794 ] [ < ffffffff810054c3 > ] do_IRQ + 0x43 / 0xd0 [ 1479.242779 ] [ < ffffffff810067d4 > ] ? apic_timer_interrupt + 0x54 / 0x90 [ 1479.243971 ] [ < ffffffff8100e0aa > ] common_interrupt + 0x6a / 0x6a [ 1479.245084 ] [ < ffffffff8101c6b0 > ] ? cpu_idle + 0x10 / 0x30 [ 1479.246123 ] [ < ffffffff81003425 > ] start_secondary_cpu + 0x55 / 0x60 [ 1479.247278 ] < EOT >","title":"Sep 20"},{"location":"lego/log/log-09-2018/#sep-17","text":"Can not believe I\u2019m wasting time on this crap X again.","title":"Sep 17"},{"location":"lego/log/log-09-2018/#sep-16","text":"Tests done today: Setting Log nr_workers Tracing (strace/counter/profiling) Runtime (s) pcache_flush_net (us) TF-MNIST, Linux 13.2s TF4-MNIST, 128MB 0916-w14-1 1 ON avg 48.5s 9891 TF4-MNIST, 128MB 0916-w14-2 1 OFF (46.1+44.6+45.5+45.7+44)/5 = 45.2s N/A TF4-MNIST, 128MB 0916-w14-4 4 ON (43.4+44+43.9+42.6+42.1)/5=43.2 8351 TF4-MNIST, 128MB 0916-w14-3 4 OFF (40.1+42.1+42.0+41.7+42.1)/5 = 41.6 N/A TF4-Cifar, Linux 235.5s TF4-Cifar, 128MB 0916-w14-5 4 OFF (636.2+635.0+636.8+637.2+634.1)/5=635.8 N/A TF4-Cifar, 128MB 0916-w14-6 1 OFF (660.2+662.2+662.8+663.8+661+5)/5=663s N/A TF4-Cifar, 256MB 0916-w14-7 1 OFF 486s N/A","title":"Sep 16"},{"location":"lego/log/log-09-2018/#sep-15","text":"DAMN. Let us summarize today. Okay. Fixed the double-post-cqe issue. Hehe. The post part is the only fucking left code that I did not look into at fit_poll_recv_cq. And, ironically, there is no error checking for ib_post_recv(), which won\u2019t generate any error/warning. error checking error checking\u2026 Anyway fuck it. Today I created a new tag v0.0.9, hope we have a stable net. The RPC profile code is very stressing, and fit survived. The following wanring is fixed by post rx_depth/2. [ 1812.017204 ] fit : To align first QPN , we skipped : # 72 # 72 # 73 # 74 # 75 # 76 # 77 # 78 # 79 [ 1812.157570 ] fit : fit_post_receives_message () - 628 CPU 2 Fail to post recv conn_id : 12 [ 1812.166013 ] ------------ [ cut here ] ------------ [ 1812.171152 ] WARNING : CPU : 2 PID : 16 at net / lego / fit_internal . c : 629 fit_post_receives_message . isra .7 + 0xce / 0x100 [ 1812.182302 ] CPU : 2 PID : 16 Comm : ib - initd 4.0.0 - lego + # 95 [ 1812.188314 ] Stack : [ 1812.190544 ] ffff880ff98bfd50 ffffffff8101299b 0000000000000 cff 0000000000000060 [ 1812.198689 ] 0000000000000 d00 0000000000000100 ffff880ff98dc030 ffff880ff98bfd60 [ 1812.206834 ] ffffffff81012a8f ffff880ff98bfdc8 ffffffff810743de fffffff4fffffff4 [ 1812.214978 ] ffff880ff98bfd80 0000000000000000 0000000000000 cff 0000000000000000 [ 1812.223124 ] 0000000000000000 ffff880ff98dc000 0000000000000000 000000000000000 c [ 1812.231269 ] Call Trace : [ 1812.233984 ] < TSK > [ 1812.236116 ] [ < ffffffff810129a7 > ] __warn . constprop .0 + 0xa7 / 0x100 [ 1812.242613 ] [ < ffffffff81012a8f > ] warn_slowpath_null + 0xf / 0x20 [ 1812.248915 ] [ < ffffffff810743de > ] fit_post_receives_message . isra .7 + 0xce / 0x100 [ 1812.256770 ] [ < ffffffff81076a1a > ] fit_add_newnode + 0xca / 0x170 [ 1812.262974 ] [ < ffffffff81079d10 > ] fit_establish_conn + 0x7b0 / 0xaa0 [ 1812.269568 ] [ < ffffffff81073ce8 > ] ? ibv_add_one + 0x98 / 0x120 [ 1812.275580 ] [ < ffffffff810741f0 > ] ? ibapi_get_node_id + 0x20 / 0x20 [ 1812.282076 ] [ < ffffffff81074258 > ] lego_ib_init + 0x68 / 0xf0 [ 1812.287893 ] [ < ffffffff81023261 > ] kthread + 0x111 / 0x130 [ 1812.293421 ] [ < ffffffff81023150 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1812.299820 ] [ < ffffffff8100eaf2 > ] ret_from_fork + 0x22 / 0x30 [ 1812.305735 ] < EOT > [ 1812.307868 ] --- [ end trace 0000000000000000 ] ---","title":"Sep 15"},{"location":"lego/log/log-09-2018/#sep-11","text":"Got this log, 5 machine, p2s_open, S side has this issue. Damn. [ 1672.962279 ] ***** ***** Fail to to get the CQE from send_cq after 20 seconds ! ***** This means the packet was lost and something went wrong ***** with your NIC ... ***** connection_id : 11 dest node : 0 ***** [ 1673.061668 ] ------------ [ cut here ] ------------ [ 1673.074937 ] WARNING : CPU : 10 PID : 4624 at / root / ys / LegoOS_2M / linux - modules / fit / fit_internal . c : 956 fit_internal_poll_sendcq + 0xda / 0x130 [ fit ]() [ 1673.101557 ] Modules linked in : storage ( OF ) fit ( OF ) xt_CHECKSUM iptable_mangle ipt_MASQUERADE iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT tun bridge stp llc ebtable_filter ebtable s ip6table_filter ip6_tables iptable_filter xprtrdma sunrpc ib_isert iscsi_target_mod ib_iser libiscsi scsi_transport_iscsi ib_srpt target_core_mod ib_srp scsi_transport_srp scsi_tgt ib_ipoib rdma_ucm ib_ucm ib_uverbs ib_umad rdma_cm ib_c m iw_cm ib_addr x86_pkg_temp_thermal coretemp kvm_intel kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ipmi_devintf ablk_helper cryptd ipmi_si iTCO_wdt ipmi_msghandler iTCO_vendor_support dcdbas sg pcspkr shpchp acpi_power_meter lpc_ich mfd_core wmi mperf uinput binfmt_misc ip_tables ext4 mbcache jbd2 mlx4_ib [ 1673.182609 ] ib_sa ib_mad ib_core mlx4_en sd_mod crc_t10dif mgag200 syscopyarea sysfillrect sysimgblt i2c_algo_bit drm_kms_helper ttm drm ahci crc32c_intel libahci mlx4_core libata tg3 nvme megaraid_sas ptp i2c_core pps_core dm_mirror dm_region_hash dm_log dm_mod [ 1673.222604 ] CPU : 10 PID : 4624 Comm : lego - storaged Tainted : GF W O 3.11.1 - vanilla # 1 [ 1673.235825 ] Hardware name : Dell Inc . PowerEdge R730 / 05 99 V5 , BIOS 1.5.4 10 / 002 / 2015 [ 1673.248883 ] 000000000000000 9 ffff88102186b9f8 ffffffff8159a5a4 0000000000000000 [ 1673.261795 ] ffff88102186ba30 ffffffff810641bd ffff882027180400 00000004 a817c800 [ 1673.274499 ] 000001 80 dc3abde5 0000000000000000 0000000000000000 ffff88102186ba40 [ 1673.287034 ] Call Trace : [ 1673.299259 ] [ < ffffffff8159a5a4 > ] dump_stack + 0x45 / 0x56 [ 1673.311371 ] [ < ffffffff810641bd > ] warn_slowpath_common + 0x7d / 0xa0 [ 1673.323268 ] [ < ffffffff8106429a > ] warn_slowpath_null + 0x1a / 0x20 [ 1673.334892 ] [ < ffffffffa063669a > ] fit_internal_poll_sendcq + 0xda / 0x130 [ fit ] [ 1673.346348 ] [ < ffffffff81093e25 > ] ? check_preempt_curr + 0x85 / 0xa0 [ 1673.357575 ] [ < ffffffffa06367f7 > ] fit_send_message_with_rdma_write_with_imm_request + 0x107 / 0x3f0 [ fit ] [ 1673.368777 ] [ < ffffffff8107bde4 > ] ? wake_up_worker + 0x24 / 0x30 [ 1673.379741 ] [ < ffffffffa0636ee9 > ] fit_reply_message + 0x89 / 0xa0 [ fit ] [ 1673.390497 ] [ < ffffffffa063507b > ] ibapi_reply_message + 0x1b / 0x20 [ fit ] [ 1673.401039 ] [ < ffffffffa0646785 > ] handle_open_request + 0xa5 / 0xe0 [ storage ] [ 1673.411367 ] [ < ffffffffa0646106 > ] storage_manager + 0x106 / 0x300 [ storage ] [ 1673.421470 ] [ < ffffffffa0646000 > ] ? 0xffffffffa0645fff [ 1673.431297 ] [ < ffffffffa0646000 > ] ? 0xffffffffa0645fff [ 1673.440797 ] [ < ffffffff81085ec0 > ] kthread + 0xc0 / 0xd0 [ 1673.450034 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1673.459063 ] [ < ffffffff815a94ac > ] ret_from_fork + 0x7c / 0xb0 [ 1673.467837 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1673.476400 ] --- [ end trace f9b19a31d409f910 ] --- [ 1695.867276 ] storage_self_monitor () : in_handler = 1 [ 1695.875906 ] handle_replica_flush : 0 [ 1695.884613 ] handle_replica_vma : 0 [ 1695.893265 ] handle_replica_read : 12740 [ 1695.901920 ] handle_replica_write : 0 [ 1713.012565 ] INFO : rcu_sched self - detected stall on CPU { 10 } ( t = 60001 jiffies g = 7646 c = 7645 q = 0 ) [ 1713.013339 ] sending NMI to all CPUs : [ 1713.013573 ] INFO : rcu_sched detected stalls on CPUs / tasks : { 10 } ( detected by 15 , t = 60002 jiffies , g = 7646 , c = 7645 , q = 0 ) [ 1713.014807 ] NMI backtrace for cpu 0 [ 1713.015685 ] CPU : 0 PID : 4591 Comm : wq_handler Tainted : GF W O 3.11.1 - vanilla # 1 [ 1713.016624 ] Hardware name : Dell Inc . PowerEdge R730 / 05 99 V5 , BIOS 1.5.4 10 / 002 / 2015 [ 1713.017575 ] task : ffff88201f193b40 ti : ffff88101a34a000 task . ti : ffff88101a34a000 [ 1713.018530 ] RIP : 0010 : [ < ffffffffa0636b55 > ] [ < ffffffffa0636b55 > ] waiting_queue_handler + 0x75 / 0x140 [ fit ] [ 1713.018530 ] RIP : 0010 : [ < ffffffffa0636b55 > ] [ < ffffffffa0636b55 > ] waiting_queue_handler + 0x75 / 0x140 [ fit ] [ 1713.019512 ] RSP : 001 8 : ffff88101a34be78 EFLAGS : 000002 96 [ 1713.020444 ] RAX : 00000000000 80080 RBX : ffff8820200253f0 RCX : ffff88201f193b40 [ 1713.021364 ] RDX : 0000000000000001 RSI : ffff88103f414760 RDI : ffff88103f4146c0 [ 1713.022260 ] RBP : ffff88101a34bec8 R08 : 0000000000000000 R09 : 0000000000000001 [ 1713.023138 ] R10 : 0000000000000001 R11 : ffffffffa0636b55 R12 : ffff8820200253c0 [ 1713.024000 ] R13 : ffff881022005000 R14 : ffffffffa063b8e4 R15 : ffffffffa063b8e4 [ 1713.024839 ] FS : 0000000000000000 ( 0000 ) GS : ffff88103f400000 ( 0000 ) knlGS : 0000000000000000 [ 1713.025676 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1713.026481 ] CR2 : 00007f d77ef46000 CR3 : 0000000001 876000 CR4 : 00000000001407f 0 [ 1713.027273 ] Stack : [ 1713.028035 ] ffff881000000000 ffff881000300660 ffff882000000003 0000000000000000 [ 1713.028818 ] ffff881000000000 ffff881021eafc38 ffff881022005000 ffffffffa0636ae0 [ 1713.029585 ] 0000000000000000 0000000000000000 ffff88101a34bf48 ffffffff81085ec0 [ 1713.030350 ] Call Trace : [ 1713.031098 ] [ < ffffffffa0636ae0 > ] ? fit_send_message_with_rdma_write_with_imm_request + 0x3f0 / 0x3f0 [ fit ] [ 1713.031875 ] [ < ffffffff81085ec0 > ] kthread + 0xc0 / 0xd0 [ 1713.032641 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40 [ 1713.033407 ] [ < ffffffff815a94ac > ] ret_from_fork + 0x7c / 0xb0 [ 1713.034171 ] [ < ffffffff81085e00 > ] ? insert_kthread_work + 0x40 / 0x40","title":"Sep 11"},{"location":"lego/log/log-09-2018/#sep-08","text":"Check this log out: ]--- [ 427.218569] STDOUT: ---[ INFO:tensorflow:Graph was finalized. ]--- [ 427.416043] BUG: unable to handle kernel NULL pointer dereference at (null) [ 427.424583] IP: [<ffffffff810748fb>] fit_poll_recv_cq+0x5cb/0x860 [ 427.431370] mlx4_msi_x_interrupt(): IRQ: 27 CPU: 0 [ 427.436702] PGD 0 [ 427.438932] CQ_ERROR CQ overrun on CQN 000082 [ 427.443780] Oops: 0002 [#1] SMP PROCESSOR [ 427.448240] event qp_event arrived [ 427.452022] CPU: 6 PID: 18 Comm: FIT_RecvCQ-0 4.0.0-lego+ #23 [ 427.458421] event qp_event arrived [ 427.462203] RIP: 0010:[<ffffffff810748fb>] [<ffffffff810748fb>] fit_poll_recv_cq+0x5cb/0x860 [ 427.471704] RSP: 0000:ffff881023e3fe60 EFLAGS: 00010287 [ 427.477618] RAX: 0000000000000000 RBX: 000000002aaaaaab RCX: 0000000000000004 [ 427.485570] RDX: 0000000000000000 RSI: 0000000000000053 RDI: 0000000000000000 [ 427.493520] RBP: ffff881023e3fec0 R08: 0000000000000001 R09: ffff881039900000 [ 427.501470] R10: 0000000000000000 R11: ffff881039918000 R12: ffff8810398f2000 [ 427.509421] R13: 0000000000000000 R14: 0000000000000001 R15: ffff881023e25008 [ 427.517371] event qp_event arrived [ 427.521153] FS: 0000000000000000(0000) GS:ffff88107fc60000(0000) knlGS:0000000000000000 [ 427.530169] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033 [ 427.536569] CR2: 0000000000000000 CR3: 000000000117a000 CR4: 00000000000406a0 [ 427.544519] event qp_event arrived Trying to tune FIT\u2019s number polling threads. This could be the throughput/latency killer. 128M P num_polling M worker M num_polling Runtime (s) 1 1 1 46.8s 1 4 1","title":"Sep 08"},{"location":"lego/log/log-09-2018/#sep-07","text":"Set up Infiniswap again. What a fucking crap code, and crash the kernel out of nowhere. crap crap crap. Hmm, Linux will tune the CPU freq during runtime, will be higher than 2.4GHz. So disable it, make it a fair comparison with Lego. intel_pstate=disable.","title":"Sep 07"},{"location":"lego/log/log-09-2018/#sep-06","text":"Did two optimizations on pcache, both are buffer management. Especially the pcache rmap case. In both opts, we kind of use static/pre-allocated array to serve dynamic allocation. This is a better solution than using kmem_cache, faster. kmem_cache will be a more general solution here. kmem_cache, FIFO queue (thpool buffer), static preallocated array (rmap, clflush)\u2026 Buffer management is really a very important thing in system building. I should be aware at the beginning next time. These changes are in commits: 6e0cf6c5c64edbe445a27cf55f86ac51f8a897b3 73377cafce95ffa0cfb155f77cac97456a5e4a71","title":"Sep 06"},{"location":"lego/log/log-09-2018/#sep-05","text":"Alright. Besides some flaws/bugs in some kfree stuff, LegoOS now actually is very robust! Ran a quick git summary: project : LegoOS repo age : 1 year, 11 months active : 358 days commits : 1540 files : 1161 authors : 1317 Yizhou Shan 85.5% 120 root 7.8% 36 hythzz 2.3% 27 yilun 1.8% 16 Yutong Huang 1.0% 10 Build Android 0.6% 8 Yiying Zhang 0.5% 4 sumukh1991 0.3% 1 Yizhou SHan 0.1% 1 Sumukh Hallymysore Ravindra 0.1% Of course, there are still PLENY room for improvement, and I know where. At this time, I really think we need something like kmem_cache, which is so fucking useful. It can probably further reduce much overhead.","title":"Sep 05"},{"location":"lego/log/log-09-2018/#sep-04","text":"Trying the perset eviction list mechanism, instead of victim cache. The benefit of using this is: we will no longer be bottelnecked by victim cache anymore. Each faulting thread will do eviction/flush within its own context. For 4 threads MNIST, I saw 3 seconds reduction. Removed the bitmap, use per pcache set counter for quick reference.","title":"Sep 04"},{"location":"lego/log/log-09-2018/#sep-03","text":"With DEBUG_MM, try enable HAVE_FREE directory by directory","title":"Sep 03"},{"location":"lego/log/log-09-2018/#-","text":"- update_wall_time+0x44 is where we call tsc_read. And this has been called many times (HZ per second). All of a sudden, the pointer got crashed. Who wrote to this code memory?? Remote RDMA? [ 1052.470714 ] general protection fault : 0000 [ # 1 ] SMP PROCESSOR [ 1052.477113 ] CPU : 0 PID : 15 Comm : ib_mad1 4.0.0 - lego + # 509 [ 1052.483125 ] RIP : 0010 : [ < ffffffff81015764 > ] [ < ffffffff81015764 > ] update_wall_time + 0x44 / 0x6f0 [ 1052.492530 ] RSP : 0000 : ffff88103ad9fc88 EFLAGS : 00010046 [ 1052.498445 ] RAX : 4510ff ffffff8118 RBX : 0380ff ffffffffff RCX : 0000000000000001 [ 1052.506396 ] RDX : ffff88103ad9fd28 RSI : 0000000000000000 RDI : 4510ff ffffff8118 [ 1052.514346 ] RBP : ffff88103ad9fcd0 R08 : 000000000000001f R09 : 0000000000000000 [ 1052.522298 ] R10 : 000000000000002 9 R11 : ffff881013f8e130 R12 : aaff0000024a2677 [ 1052.530248 ] R13 : 0000000000000000 R14 : ffff88103ad85228 R15 : ffff88103ae0c000 [ 1052.538199 ] FS : 0000000000000000 ( 0000 ) GS : ffff88107fc00000 ( 0000 ) knlGS : 0000000000000000 [ 1052.547216 ] CS : 0010 DS : 0000 ES : 0000 CR0 : 00000000 80050033 [ 1052.553616 ] CR2 : 0000000000000000 CR3 : 000000000117 b000 CR4 : 00000000000406 b0 [ 1052.561567 ] Stack : [ 1052.563797 ] 00000000000000 86 ffff88107fc05d80 ffff88103ad85000 0000000000000000 [ 1052.571941 ] ffff88107fc04980 0000000000000000 0000000000000000 ffff88103ad85228 [ 1052.580085 ] ffff88103ae0c000 ffff88103ad9fce8 ffffffff81017557 000000003 ad9fe10 [ 1052.588230 ] ffff88103ad9fd10 ffffffff810067a4 ffffffff81088040 ffff88107fc05d80 [ 1052.596375 ] ffff88103ad85000 ffff88103ad9fdf8 ffffffff8100e8ea ffff88103ad9fd28 [ 1052.604520 ] Call Trace : [ 1052.607236 ] < TSK > [ 1052.609368 ] [ < ffffffff81017557 > ] tick_handle_periodic + 0x67 / 0x70 [ 1052.615961 ] [ < ffffffff810067a4 > ] apic_timer_interrupt + 0x54 / 0x90 [ 1052.622555 ] [ < ffffffff8100e8ea > ] smp__apic_timer_interrupt + 0x6a / 0x70 [ 1052.629633 ] [ < ffffffff8107b488 > ] ? __schedule + 0xf8 / 0x1e0 [ 1052.635548 ] [ < ffffffff8107b583 > ] schedule + 0x13 / 0x30 [ 1052.640978 ] [ < ffffffff8106c98e > ] ib_mad_completion_handler + 0x5de / 0xc20 [ 1052.648250 ] [ < ffffffff8101de3b > ] ? dequeue_task_rt + 0x1b / 0x180 [ 1052.654648 ] [ < ffffffff8106c3b0 > ] ? ib_mad_send_done_handler . isra .22 + 0x4e0 / 0x4e0 [ 1052.662793 ] [ < ffffffff81022af6 > ] kthread + 0xf6 / 0x110 [ 1052.668223 ] [ < ffffffff81022a00 > ] ? __kthread_parkme + 0x70 / 0x70 [ 1052.674622 ] [ < ffffffff8100eb72 > ] ret_from_fork + 0x22 / 0x30 [ 1052.680538 ] < EOT > [ 1052.682670 ] Code : db e4 16 00 79 0 d f3 90 80 3 d d0 e4 16 00 00 7 e f5 eb ea 48 8 b 1 d fd fa 1f 00 48 8 b 05 e6 fa 1f 00 4 c 8 b 25 f7 fa 1f 00 48 89 c7 < ff > 50 28 49 89 c7 48 89 d8 4 d 29 e7 48 d1 e8 49 21 df 48 f7 d0 [ 1052.703711 ] RIP [ < ffffffff81015764 > ] update_wall_time + 0x44 / 0x6f0 [ 1052.710498 ] RSP < ffff88103ad9fc88 >","title":"-"},{"location":"lego/log/misc/","text":"MISC \u00b6 /etc/ld.so.preload : GLIBC uses access() to check if this file exist (normally it does not exist) 1 . This is something related to LD_PRELOAD : If both LD_PRELOAD and /etc/ld.so.preload are employed, the libraries specified by LD_PRELOAD are preloaded first. / etc/ld.so.preload has a system-wide effect, causing the specified libraries to be preloaded for all programs that are executed on the system 2 . I was reading a FAST18 paper (Fail-Slow Datacenter). I found it quite interesting and some suggestions are very useful for all system designers. Especially: Make implicit error-masking explicit. DO NOT FAIL SILENTLY . Since this is not a fail-stop ( binary ) issue, normally system designers will not raise exceptions. System designers should be aware of uncommon situations, raise explicit exceptions to convert a fail-slow ( non-binary ) case to a fail-stop ( binary ) case .Actually, this also reminds the email by Linus Torvards on BUG_ON usage 3 . Exposing performance statistic information for all-level (device, firmware, system software, application) . However, based on my own experience, do not generate too much useless logs, it will just help to hide the root cause. Testing of applications is often done on a testing environment, smaller in size (perhaps only a single server) and less loaded than the \u201clive\u201d environment. The replication behavior of such an installation may differ from a live environment in ways that mean that replication lag is unlikely to be observed in testing - masking replication-sensitive bugs. mmap PROT_NONE is really used by applications, or library. They have their special usage. etc/ld.so.preload \u21a9 ld.so.8.html \u21a9 LKML:BUG_ON \u21a9","title":"MISC"},{"location":"lego/log/misc/#misc","text":"/etc/ld.so.preload : GLIBC uses access() to check if this file exist (normally it does not exist) 1 . This is something related to LD_PRELOAD : If both LD_PRELOAD and /etc/ld.so.preload are employed, the libraries specified by LD_PRELOAD are preloaded first. / etc/ld.so.preload has a system-wide effect, causing the specified libraries to be preloaded for all programs that are executed on the system 2 . I was reading a FAST18 paper (Fail-Slow Datacenter). I found it quite interesting and some suggestions are very useful for all system designers. Especially: Make implicit error-masking explicit. DO NOT FAIL SILENTLY . Since this is not a fail-stop ( binary ) issue, normally system designers will not raise exceptions. System designers should be aware of uncommon situations, raise explicit exceptions to convert a fail-slow ( non-binary ) case to a fail-stop ( binary ) case .Actually, this also reminds the email by Linus Torvards on BUG_ON usage 3 . Exposing performance statistic information for all-level (device, firmware, system software, application) . However, based on my own experience, do not generate too much useless logs, it will just help to hide the root cause. Testing of applications is often done on a testing environment, smaller in size (perhaps only a single server) and less loaded than the \u201clive\u201d environment. The replication behavior of such an installation may differ from a live environment in ways that mean that replication lag is unlikely to be observed in testing - masking replication-sensitive bugs. mmap PROT_NONE is really used by applications, or library. They have their special usage. etc/ld.so.preload \u21a9 ld.so.8.html \u21a9 LKML:BUG_ON \u21a9","title":"MISC"},{"location":"lego/log/test-note/","text":"Scripts \u00b6 Scripts used to run OSDI\u201818 LegoOS experiments. CPU Freq \u00b6 For fair comparision, we disable cpu freq tuning (because lego does not have it. shame!): Add this to boot kernel command parameter: intel_pstate=disable swap-to-ssd \u00b6 Please remember to clear the page cache! echo 3 > /proc/sys/vm/drop_caches rm -rf /tmp/mnist_model/ lxc-execute -n test -s lxc.cgroup.memory.limit_in_bytes=128M -- python mnist.py swap-to-ramdisk \u00b6 Please note we are using BLK_DEV_RAM, a block device based on RAM. We are NOT using tmpfs or ramfs. The difference is:. modprobe brd rd_size=16777216 dd if=/dev/zero of=/dev/ram0 bs=4K mkswap /dev/ram0 swapon /dev/ram0 swapoff others Accelio and nbdX \u00b6 Follow this , and this . Tested with CentOS 7.2 kernel 3.13.1 wuklab14, wuklab18 Side notes Server side, the block device created for client, can not be raw disk/SSD. I created a file from SSD Stick with 3.13 at both client and server. Client with 3.19 will crash Server: touch /mnt/ssd/swap truncate -s +4G /mnt/ssd/swap raio_server -a 10.0.0.X -p 5555 -t rdma -f 0 Client: modprobe xio_rdma; modprobe xio_tcp modprobe nbdx nbdxadm -o create_host -i 0 -p \"10.0.0.X:5555\" nbdxadm -o create_device -i 0 -d 0 -f \"/mnt/ssd/swap\" nbdxadm -o show_all_devices mkswap /dev/nbdx0 swapon /dev/nbdx0 swapoff others Infiniswap \u00b6 Tested with CentOS 7.2 MLNX_OFED_LINUX-3.3-1.0.4.0-rhel7.2-x86_64 kernel 3.13.1 Note 1. At server side, use server ib0\u2019s IP address: # ifconfig ib0 : flags = 4163 < UP , BROADCAST , RUNNING , MULTICAST > mtu 2044 inet 10.0.0.67 netmask 255.255.255.0 broadcast 10.0.0.255 . / infiniswap - daemon 10.0.0.67 9400 At client side, use server ib0\u2019s IP in portal.list: 1 10.0.0.67:9400 2. At client side, change the BACKUP_DISK to an unused disk, and use a CORRECT one! Otherwise, wait for kernel panic, ugh. Use HDD such as /dev/sdb A SSD will kill Infiniswap. 3. Also, looks like we need to remove memmap from kernel parameter.","title":"Scripts"},{"location":"lego/log/test-note/#scripts","text":"Scripts used to run OSDI\u201818 LegoOS experiments.","title":"Scripts"},{"location":"lego/log/test-note/#cpu-freq","text":"For fair comparision, we disable cpu freq tuning (because lego does not have it. shame!): Add this to boot kernel command parameter: intel_pstate=disable","title":"CPU Freq"},{"location":"lego/log/test-note/#swap-to-ssd","text":"Please remember to clear the page cache! echo 3 > /proc/sys/vm/drop_caches rm -rf /tmp/mnist_model/ lxc-execute -n test -s lxc.cgroup.memory.limit_in_bytes=128M -- python mnist.py","title":"swap-to-ssd"},{"location":"lego/log/test-note/#swap-to-ramdisk","text":"Please note we are using BLK_DEV_RAM, a block device based on RAM. We are NOT using tmpfs or ramfs. The difference is:. modprobe brd rd_size=16777216 dd if=/dev/zero of=/dev/ram0 bs=4K mkswap /dev/ram0 swapon /dev/ram0 swapoff others","title":"swap-to-ramdisk"},{"location":"lego/log/test-note/#accelio-and-nbdx","text":"Follow this , and this . Tested with CentOS 7.2 kernel 3.13.1 wuklab14, wuklab18 Side notes Server side, the block device created for client, can not be raw disk/SSD. I created a file from SSD Stick with 3.13 at both client and server. Client with 3.19 will crash Server: touch /mnt/ssd/swap truncate -s +4G /mnt/ssd/swap raio_server -a 10.0.0.X -p 5555 -t rdma -f 0 Client: modprobe xio_rdma; modprobe xio_tcp modprobe nbdx nbdxadm -o create_host -i 0 -p \"10.0.0.X:5555\" nbdxadm -o create_device -i 0 -d 0 -f \"/mnt/ssd/swap\" nbdxadm -o show_all_devices mkswap /dev/nbdx0 swapon /dev/nbdx0 swapoff others","title":"Accelio and nbdX"},{"location":"lego/log/test-note/#infiniswap","text":"Tested with CentOS 7.2 MLNX_OFED_LINUX-3.3-1.0.4.0-rhel7.2-x86_64 kernel 3.13.1 Note 1. At server side, use server ib0\u2019s IP address: # ifconfig ib0 : flags = 4163 < UP , BROADCAST , RUNNING , MULTICAST > mtu 2044 inet 10.0.0.67 netmask 255.255.255.0 broadcast 10.0.0.255 . / infiniswap - daemon 10.0.0.67 9400 At client side, use server ib0\u2019s IP in portal.list: 1 10.0.0.67:9400 2. At client side, change the BACKUP_DISK to an unused disk, and use a CORRECT one! Otherwise, wait for kernel panic, ugh. Use HDD such as /dev/sdb A SSD will kill Infiniswap. 3. Also, looks like we need to remove memmap from kernel parameter.","title":"Infiniswap"},{"location":"lego/paper/genz/","text":"Interconnect Technology Comparison \u00b6 Interconnect Technology Products or Vendor Physical Domain Cache Coherent Access Semantic Maximum Bandwidth Medium Latency Gen-Z 7 8 N/A Cross components Memory 32 GBps ~ 400+ GBps Unidirectional <100ns OpenCAPI 7 IBM Power9 Motherboard Memory 50 GBps per lane Bidirectional ? CCIX 7 N/A Motherboard Memory 32/40/50 GBps/lane Bidirectional ? OmniPath 9 10 Intel KnightsLanding Cross networrk Network 25 GBps/port Bidirectional ? PCIe 3.0 A Lot Motherboard PCIe ~1GBps/lane 12 4B Read ~756ns 11 PCIe 4.0 Soon Motherboard PCIe ~2GBps/lane ? IB EDR Mellanox ConnectX4,X5 Cross network Network 100Gbps 0.5us IB HDR Mellanox ConnectX6 Cross network Network 200Gbps <0.5us HyperTransport 4 AMD Motherboard Memory 51.2 GBps per link Bidirectional ? NVLink 2 NVIDIA V100 IBM Power9 Motherboard Memory 50GBps per link Bidirectional ? QPI 5 6 Intel Motherboard Memory ? ? Intel Main Memory Bus Intel Processor Memory E7-8894 v4 85 GB/s E5-2620 v3 59 GB/s ? Ethernet 3 A Lot Motherboard Network Mellanox 200Gbps Cisco ASR 100 Gbps 1 ? POWER9, NVLink 2.0, 300GB/s \u2013 Created: Feb 28, 2018 Last Updated: March 01, 2018 Ethernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet \u21a9 Terabit Ethernet https://en.wikipedia.org/wiki/NVLink \u21a9 NVLink \u21a9 HyperTransport \u21a9 https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect \u21a9 https://communities.intel.com/thread/21872 \u21a9 https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf \u21a9 \u21a9 \u21a9 Gen-Z Overview \u21a9 http://www.hoti.org/hoti23/slides/rimmer.pdf \u21a9 https://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html \u21a9 https://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf \u21a9 https://www.xilinx.com/support/documentation/white_papers/wp350.pdf \u21a9","title":"Gen-Z"},{"location":"lego/paper/genz/#interconnect-technology-comparison","text":"Interconnect Technology Products or Vendor Physical Domain Cache Coherent Access Semantic Maximum Bandwidth Medium Latency Gen-Z 7 8 N/A Cross components Memory 32 GBps ~ 400+ GBps Unidirectional <100ns OpenCAPI 7 IBM Power9 Motherboard Memory 50 GBps per lane Bidirectional ? CCIX 7 N/A Motherboard Memory 32/40/50 GBps/lane Bidirectional ? OmniPath 9 10 Intel KnightsLanding Cross networrk Network 25 GBps/port Bidirectional ? PCIe 3.0 A Lot Motherboard PCIe ~1GBps/lane 12 4B Read ~756ns 11 PCIe 4.0 Soon Motherboard PCIe ~2GBps/lane ? IB EDR Mellanox ConnectX4,X5 Cross network Network 100Gbps 0.5us IB HDR Mellanox ConnectX6 Cross network Network 200Gbps <0.5us HyperTransport 4 AMD Motherboard Memory 51.2 GBps per link Bidirectional ? NVLink 2 NVIDIA V100 IBM Power9 Motherboard Memory 50GBps per link Bidirectional ? QPI 5 6 Intel Motherboard Memory ? ? Intel Main Memory Bus Intel Processor Memory E7-8894 v4 85 GB/s E5-2620 v3 59 GB/s ? Ethernet 3 A Lot Motherboard Network Mellanox 200Gbps Cisco ASR 100 Gbps 1 ? POWER9, NVLink 2.0, 300GB/s \u2013 Created: Feb 28, 2018 Last Updated: March 01, 2018 Ethernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet \u21a9 Terabit Ethernet https://en.wikipedia.org/wiki/NVLink \u21a9 NVLink \u21a9 HyperTransport \u21a9 https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect \u21a9 https://communities.intel.com/thread/21872 \u21a9 https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf \u21a9 \u21a9 \u21a9 Gen-Z Overview \u21a9 http://www.hoti.org/hoti23/slides/rimmer.pdf \u21a9 https://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html \u21a9 https://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf \u21a9 https://www.xilinx.com/support/documentation/white_papers/wp350.pdf \u21a9","title":"Interconnect Technology Comparison"},{"location":"lego/paper/nmp/","text":"Near Memory Processing \u00b6 NMP: Near Memory Processing NDC: Near Data Computing PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory, ISCA'16 High-performance acceleration of NN requires high memory bandwidth since the PUs are hungry for fetching the synaptic weights [17] . To address this challenge, recent special-purpose chip designs have adopted large on-chip memory to store the synaptic weights. For example, DaDianNao [18] employed a large on-chip eDRAM for both high bandwidth and data locality; TrueNorth utilized an SRAM crossbar memory for synapses in each core [19]. DianNao and DaDianNao \u2026 memory bandwidth requirements of two important layer types: convolutional layers with private kernels (used in DNNs) and classifier layers used in both CNNs and DNNs. For these types of layers, the total number of required synapses can be massive, in the millions of parameters, or even tens or hundreds thereof. providing sufficient eDRAM capacity to hold all synapse on the combined eDRAM of all chips will save on off-chip DRAM accesses , which are particularly costly energy-wise Synapses . In a perceptron layer, all synapses are usually unique, and thus there is no reuse within the layer. On the other hand, the synapses are reused across network invocations, i.e., for each new input data (also called \u201cinput row\u201d) presented to the neural network. So a sufficiently large L2 could store all network synapses and take advantage of that locality. For DNNs with private kernels, this is not possible as the total number of synapses are in the tens or hundreds of millions (the largest network to date has a billion synapses [26]). However, for both CNNs and DNNs with shared kernels, the total number of synapses range in the millions, which is within the reach of an L2 cache. In Figure 6, see CLASS1 - Tiled+L2, we emulate the case where reuse across network invocations is possible by considering only the perceptron layer; as a result, the total bandwidth requirements are now drastically reduced. So, ML workloads do need large memory bandwidth, and need a lot memory. But how about temporary working set size ? It\u2019s the best if it has a reasonable working set size that can fit the cache. TPU Each model needs between 5M and 100M weights (9 th column of Table 1), which can take a lot of time and energy to access. To amortize the access costs, the same weights are reused across a batch of independent examples during inference or training , which improves performance. The weights for the matrix unit are staged through an onchip Weight FIFO that reads from an off-chip 8 GiB DRAM called Weight Memory (for inference, weights are read-only; 8 GiB supports many simultaneously active models). The weight FIFO is four tiles deep. The intermediate results are held in the 24 MiB on-chip Unified Buffer , which can serve as inputs to the Matrix Unit. In virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache! To conclude: a) ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the intermediate data is important. Most ML accelerators use some kind of on-chip memory ( Weighted FIFO, Unified Cache in TPU ) to buffer those data. This fits the HBM+Disaggregated Memory model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory. b) Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.","title":"NMP"},{"location":"lego/paper/nmp/#near-memory-processing","text":"NMP: Near Memory Processing NDC: Near Data Computing PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory, ISCA'16 High-performance acceleration of NN requires high memory bandwidth since the PUs are hungry for fetching the synaptic weights [17] . To address this challenge, recent special-purpose chip designs have adopted large on-chip memory to store the synaptic weights. For example, DaDianNao [18] employed a large on-chip eDRAM for both high bandwidth and data locality; TrueNorth utilized an SRAM crossbar memory for synapses in each core [19]. DianNao and DaDianNao \u2026 memory bandwidth requirements of two important layer types: convolutional layers with private kernels (used in DNNs) and classifier layers used in both CNNs and DNNs. For these types of layers, the total number of required synapses can be massive, in the millions of parameters, or even tens or hundreds thereof. providing sufficient eDRAM capacity to hold all synapse on the combined eDRAM of all chips will save on off-chip DRAM accesses , which are particularly costly energy-wise Synapses . In a perceptron layer, all synapses are usually unique, and thus there is no reuse within the layer. On the other hand, the synapses are reused across network invocations, i.e., for each new input data (also called \u201cinput row\u201d) presented to the neural network. So a sufficiently large L2 could store all network synapses and take advantage of that locality. For DNNs with private kernels, this is not possible as the total number of synapses are in the tens or hundreds of millions (the largest network to date has a billion synapses [26]). However, for both CNNs and DNNs with shared kernels, the total number of synapses range in the millions, which is within the reach of an L2 cache. In Figure 6, see CLASS1 - Tiled+L2, we emulate the case where reuse across network invocations is possible by considering only the perceptron layer; as a result, the total bandwidth requirements are now drastically reduced. So, ML workloads do need large memory bandwidth, and need a lot memory. But how about temporary working set size ? It\u2019s the best if it has a reasonable working set size that can fit the cache. TPU Each model needs between 5M and 100M weights (9 th column of Table 1), which can take a lot of time and energy to access. To amortize the access costs, the same weights are reused across a batch of independent examples during inference or training , which improves performance. The weights for the matrix unit are staged through an onchip Weight FIFO that reads from an off-chip 8 GiB DRAM called Weight Memory (for inference, weights are read-only; 8 GiB supports many simultaneously active models). The weight FIFO is four tiles deep. The intermediate results are held in the 24 MiB on-chip Unified Buffer , which can serve as inputs to the Matrix Unit. In virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache! To conclude: a) ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the intermediate data is important. Most ML accelerators use some kind of on-chip memory ( Weighted FIFO, Unified Cache in TPU ) to buffer those data. This fits the HBM+Disaggregated Memory model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory. b) Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.","title":"Near Memory Processing"},{"location":"lego/paper/os/","text":"Operating System \u00b6 Multics Hydra (operating system), CMU 70\u2019s Firefly DEC, 80\u2019s HIVE Disco IBM K42 Tessellation, UCB Akaros, UCB Amoeba The Amoeba distributed operating system Amoeba A Distributed Operating System for the 1990s Corey Corey: An Operating System for Many Cores Barrelfish Decoupling Cores, Kernels, and Operating Systems The Multikernel: A new OS architecture for scalable multicore systems Drawbridge Graphene L4 FOS An Operating System for Multicore and Clouds: Mechanisms and Implementation","title":"Operating System"},{"location":"lego/paper/os/#operating-system","text":"Multics Hydra (operating system), CMU 70\u2019s Firefly DEC, 80\u2019s HIVE Disco IBM K42 Tessellation, UCB Akaros, UCB Amoeba The Amoeba distributed operating system Amoeba A Distributed Operating System for the 1990s Corey Corey: An Operating System for Many Cores Barrelfish Decoupling Cores, Kernels, and Operating Systems The Multikernel: A new OS architecture for scalable multicore systems Drawbridge Graphene L4 FOS An Operating System for Multicore and Clouds: Mechanisms and Implementation","title":"Operating System"},{"location":"lego/paper/processor_oom/","text":"Process/Memory Kernel Memory \u00b6 This document is based on discussion with Yiying, about how to deal with processor or memory component\u2019s out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads. Process\u2019s local kernel memory is limited by design. There are several major users: 1) pcache\u2019s rmap, which is propotional to pcache size. 2) IB, which depends on concurrent outgoing messages. 3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g, kernel stack , task_strcut , and so on. Both 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom. When processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions: s1) Swap kernel memory to remote memory component s2) Kill some threads to have some usable memory (OOM killer) s3) Migrate , or checkpoint , threads to processors that have usable kernel memory For solution 3), there is a case where all processors are running out of memory. Then we have to use solution 1) or 2). \u2013 Yizhou Shan Feb 17, 2018","title":"Processor OOM"},{"location":"lego/paper/processor_oom/#processmemory-kernel-memory","text":"This document is based on discussion with Yiying, about how to deal with processor or memory component\u2019s out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads. Process\u2019s local kernel memory is limited by design. There are several major users: 1) pcache\u2019s rmap, which is propotional to pcache size. 2) IB, which depends on concurrent outgoing messages. 3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g, kernel stack , task_strcut , and so on. Both 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom. When processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions: s1) Swap kernel memory to remote memory component s2) Kill some threads to have some usable memory (OOM killer) s3) Migrate , or checkpoint , threads to processors that have usable kernel memory For solution 3), there is a case where all processors are running out of memory. Then we have to use solution 1) or 2). \u2013 Yizhou Shan Feb 17, 2018","title":"Process/Memory Kernel Memory"},{"location":"lego/paper/related/","text":"dRedBox \u00b6 news IBM Advancing cloud with memory disaggregation [Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] ( https://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf ) Slides: Demo","title":"Related"},{"location":"lego/paper/related/#dredbox","text":"news IBM Advancing cloud with memory disaggregation [Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] ( https://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf ) Slides: Demo","title":"dRedBox"},{"location":"lego/paper/replication/","text":"Replication, Checkpoint, Logging, and Recovery \u00b6 Discussion \u00b6 03/25/18: Revisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side. We don\u2019t need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen while we do the batch persist. But we are safe if failure happen between batched persist. That actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don\u2019t know the exact IP and other things. I\u2019m wrong. :-) 03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components. Replication \u00b6 Before started, I spent some time recap, and found Wiki pages 1 2 3 are actually very good. Two main approaches: Optimistic (Lazy, Passive) Replication 4 , in which replicas are allowed to diverge Eventual consistency 5 6 7 , meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time Pessimistic (Active, Multi-master 8 ) Replication , tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along. Lego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB? Checkpointing \u00b6 Some nice reading 9 . Application types: Long-running v.s. Short-lived Built-in checkpoint/journaling v.s. no built-in checkpoint/journaling Two main approaches: Coordinated 2PC Un-coordinated Domino effect We should favor [Long-running && no built-in checkpoint/journaling] applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing. HPC community 10 11 12 has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory). MISC \u00b6 Some other interesting topics: Erasure Coding Less space overhead Parity Calculation is CPU-intensive Increased latency \u2013 Yizhou Shan Created: Mar 19, 2018 Last Updated: Mar 19, 2018 Wiki: Replication \u21a9 Wiki: High-availability_cluster \u21a9 Wiki: Virtual synchrony \u21a9 Wiki: Optimistic Replication \u21a9 Wiki: Quiesce \u21a9 Wiki: Eventual Consistency \u21a9 Wiki: CAP Theorem \u21a9 Wiki: Multi-master replication \u21a9 Wiki: Application Checkpointing \u21a9 Paper: A Survey of Checkpoint/Restart Implementations \u21a9 Paper: The Design and Implementation of Berkeley Lab\u2019s Linux Checkpoint/Restart \u21a9 Berkeley Lab Checkpoint/Restart (BLCR) for LINUX \u21a9","title":"Replication"},{"location":"lego/paper/replication/#replication-checkpoint-logging-and-recovery","text":"","title":"Replication, Checkpoint, Logging, and Recovery"},{"location":"lego/paper/replication/#discussion","text":"03/25/18: Revisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side. We don\u2019t need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen while we do the batch persist. But we are safe if failure happen between batched persist. That actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don\u2019t know the exact IP and other things. I\u2019m wrong. :-) 03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components.","title":"Discussion"},{"location":"lego/paper/replication/#replication","text":"Before started, I spent some time recap, and found Wiki pages 1 2 3 are actually very good. Two main approaches: Optimistic (Lazy, Passive) Replication 4 , in which replicas are allowed to diverge Eventual consistency 5 6 7 , meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time Pessimistic (Active, Multi-master 8 ) Replication , tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along. Lego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB?","title":"Replication"},{"location":"lego/paper/replication/#checkpointing","text":"Some nice reading 9 . Application types: Long-running v.s. Short-lived Built-in checkpoint/journaling v.s. no built-in checkpoint/journaling Two main approaches: Coordinated 2PC Un-coordinated Domino effect We should favor [Long-running && no built-in checkpoint/journaling] applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing. HPC community 10 11 12 has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory).","title":"Checkpointing"},{"location":"lego/paper/replication/#misc","text":"Some other interesting topics: Erasure Coding Less space overhead Parity Calculation is CPU-intensive Increased latency \u2013 Yizhou Shan Created: Mar 19, 2018 Last Updated: Mar 19, 2018 Wiki: Replication \u21a9 Wiki: High-availability_cluster \u21a9 Wiki: Virtual synchrony \u21a9 Wiki: Optimistic Replication \u21a9 Wiki: Quiesce \u21a9 Wiki: Eventual Consistency \u21a9 Wiki: CAP Theorem \u21a9 Wiki: Multi-master replication \u21a9 Wiki: Application Checkpointing \u21a9 Paper: A Survey of Checkpoint/Restart Implementations \u21a9 Paper: The Design and Implementation of Berkeley Lab\u2019s Linux Checkpoint/Restart \u21a9 Berkeley Lab Checkpoint/Restart (BLCR) for LINUX \u21a9","title":"MISC"},{"location":"lego/pcache/config/","text":"Pcache Configuration \u00b6 This doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego\u2019s processor manager and currently it uses DRAM to emulate the last-level cache (or, L4). Kconfig \u00b6 CONFIG_MEMMAP_MEMBLOCK_RESERVED \u00b6 DEFAULT: Y By default, boot command line option memmap $ will reserve a range of physical memory. This reserved memory will be marked reserved in e820 table, which means this range will not be registered into memblock . Only memory that has been registered into memblock will be assigned struct page with it (both memblock.memory and memblock.reserve will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time. In other words, by default (the linux semantic), users need to ioremap the memmap $ reserved physical memory, and use the returned kernel virtual address afterwards. And do note that the ioremap() only support 4KB mapping. In Lego, if this option is enabled, the memory marked by memmap $ will NOT be marked reserved into e820 table, instead, it will be pushed into memblock , which means it is mapped into kernel direct mapping and has struct page . For those who have done DAX, or NVM related stuff, you must have struggled with memmap $ , and complained why it does not have struct page , I guess? So here is the simple code to do so: if ( * p == '@' ) { start_at = memparse ( p + 1 , & p ); e820_add_region ( start_at , mem_size , E820_RAM ); } else if ( * p == '#' ) { start_at = memparse ( p + 1 , & p ); e820_add_region ( start_at , mem_size , E820_ACPI ); } else if ( * p == '$' ) { start_at = memparse ( p + 1 , & p ); #ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED memblock_reserve ( start_at , mem_size ); #else e820_add_region ( start_at , mem_size , E820_RESERVED ); #endif But why we are having this? Because I think the direct 1GB mapping may have better performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown. If unsure, say Y . \u2013 Yizhou Shan Created: Feb 01, 2018 Last Updated: Feb 01, 2018","title":"Config"},{"location":"lego/pcache/config/#pcache-configuration","text":"This doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego\u2019s processor manager and currently it uses DRAM to emulate the last-level cache (or, L4).","title":"Pcache Configuration"},{"location":"lego/pcache/config/#kconfig","text":"","title":"Kconfig"},{"location":"lego/pcache/config/#config_memmap_memblock_reserved","text":"DEFAULT: Y By default, boot command line option memmap $ will reserve a range of physical memory. This reserved memory will be marked reserved in e820 table, which means this range will not be registered into memblock . Only memory that has been registered into memblock will be assigned struct page with it (both memblock.memory and memblock.reserve will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time. In other words, by default (the linux semantic), users need to ioremap the memmap $ reserved physical memory, and use the returned kernel virtual address afterwards. And do note that the ioremap() only support 4KB mapping. In Lego, if this option is enabled, the memory marked by memmap $ will NOT be marked reserved into e820 table, instead, it will be pushed into memblock , which means it is mapped into kernel direct mapping and has struct page . For those who have done DAX, or NVM related stuff, you must have struggled with memmap $ , and complained why it does not have struct page , I guess? So here is the simple code to do so: if ( * p == '@' ) { start_at = memparse ( p + 1 , & p ); e820_add_region ( start_at , mem_size , E820_RAM ); } else if ( * p == '#' ) { start_at = memparse ( p + 1 , & p ); e820_add_region ( start_at , mem_size , E820_ACPI ); } else if ( * p == '$' ) { start_at = memparse ( p + 1 , & p ); #ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED memblock_reserve ( start_at , mem_size ); #else e820_add_region ( start_at , mem_size , E820_RESERVED ); #endif But why we are having this? Because I think the direct 1GB mapping may have better performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown. If unsure, say Y . \u2013 Yizhou Shan Created: Feb 01, 2018 Last Updated: Feb 01, 2018","title":"CONFIG_MEMMAP_MEMBLOCK_RESERVED"},{"location":"lego/pcache/evict_and_ref/","text":"Mumble pcache eviction and refcount \u00b6 This is about how Lego is doing eviction against live references of pcache. Unlike the garbage collection where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business. To describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as zap_rmap() . If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache. The textbook idea is adding refcount. However, this is not enough in C. Because: There is no way to prevent the second thread from getting the pointer to that pcm. A simple inc_refcount() from the second thread can happen anytime in the middle of first thread\u2019s eviction. Solutions: To actually prevent the second thread from getting the pointer, we should think about how it get the pointer? Luckily, in Lego, there is only one entry point, which is from pte to pcm (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references: protect your pointer ! Refcount checking is also necessary. In the eviction routine, we need to use atomic_xchg to reset the refcount. If this fails, it means someone else is using it. Do note, this atomic_xchg is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code. The code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this: pcm = pte_to_pcache_meta ( ptent ); /* * We have a strict lock ordering everyone should obey: * lock pcache * lock pte * The caller already locked pte, thus we should avoid deadlock here * by droping pte lock first and then acquire both of them in order. */ if ( unlikely ( ! trylock_pcache ( pcm ))) { /* in case it got evicted and @pcm becomes invalid */ get_pcache ( pcm ); /* * Once we release the pte lock, this pcm may be * unmapped by another thread who is doing eviction. * Since we have grabbed one extra ref above, so even * it is unmapped, eviction thread will not fail to free it. */ spin_unlock ( ptl ); lock_pcache ( pcm ); spin_lock ( ptl ); /* * Since we dropped the lock, the pcache line might * be got evicted in the middle. */ if ( ! pte_same ( * pte , ptent )) { unlock_pcache ( pcm ); /* * This put maybe decreases the ref to 0 * and eventually free the pcache line. * This happens if the @pcm was selected * to be evicted at the same time. */ put_pcache ( pcm ); return - EAGAIN ; } put_pcache ( pcm ); } As for the eviction thread, it needs to make sure it is the last user using this pcm: /* * Each rmap counts one refcount, plus the one grabbed * during evict_find_line(), we should have (nr_mapped + 1) * here if there are no any other users. * * Furthurmore, others can not go from munmap/mremap/wp to * put_pcache() within pcache_zap_pte(), pcache_move_pte() * or pcache_do_wp_page(). Thus the refcount must larger or * equal to (nr_mapped + 1). * * But if there truly other users (refcount > nr_mapped + 1), * then we should manually sub the refcount. The other users * which are currently holding the ref, will free the pcache * once it call put_pcache. */ PCACHE_BUG_ON_PCM ( pcache_ref_count ( pcm ) < nr_mapped + 1 , pcm ); if ( unlikely ( ! pcache_ref_freeze ( pcm , nr_mapped + 1 ))) { if ( unlikely ( pcache_ref_sub_and_test ( pcm , nr_mapped + 1 ))) { pr_info ( \"BUG: pcm refcount, nr_mapped: %d \\n \" , nr_mapped ); dump_pcache_meta ( pcm , \"ref error\" ); BUG (); } ClearPcacheReclaim ( pcm ); add_to_lru_list ( pcm , pset ); unlock_pcache ( pcm ); inc_pcache_event ( PCACHE_EVICTION_EAGAIN_CONCURRENT ); return PCACHE_EVICT_EAGAIN_CONCURRENT ; } My personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer. In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks. \u2013 Yizhou Shan Created: Mar 15, 2018 Last Updated: Mar 16, 2018","title":"Evict and Refcount"},{"location":"lego/pcache/evict_and_ref/#mumble-pcache-eviction-and-refcount","text":"This is about how Lego is doing eviction against live references of pcache. Unlike the garbage collection where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business. To describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as zap_rmap() . If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache. The textbook idea is adding refcount. However, this is not enough in C. Because: There is no way to prevent the second thread from getting the pointer to that pcm. A simple inc_refcount() from the second thread can happen anytime in the middle of first thread\u2019s eviction. Solutions: To actually prevent the second thread from getting the pointer, we should think about how it get the pointer? Luckily, in Lego, there is only one entry point, which is from pte to pcm (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references: protect your pointer ! Refcount checking is also necessary. In the eviction routine, we need to use atomic_xchg to reset the refcount. If this fails, it means someone else is using it. Do note, this atomic_xchg is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code. The code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this: pcm = pte_to_pcache_meta ( ptent ); /* * We have a strict lock ordering everyone should obey: * lock pcache * lock pte * The caller already locked pte, thus we should avoid deadlock here * by droping pte lock first and then acquire both of them in order. */ if ( unlikely ( ! trylock_pcache ( pcm ))) { /* in case it got evicted and @pcm becomes invalid */ get_pcache ( pcm ); /* * Once we release the pte lock, this pcm may be * unmapped by another thread who is doing eviction. * Since we have grabbed one extra ref above, so even * it is unmapped, eviction thread will not fail to free it. */ spin_unlock ( ptl ); lock_pcache ( pcm ); spin_lock ( ptl ); /* * Since we dropped the lock, the pcache line might * be got evicted in the middle. */ if ( ! pte_same ( * pte , ptent )) { unlock_pcache ( pcm ); /* * This put maybe decreases the ref to 0 * and eventually free the pcache line. * This happens if the @pcm was selected * to be evicted at the same time. */ put_pcache ( pcm ); return - EAGAIN ; } put_pcache ( pcm ); } As for the eviction thread, it needs to make sure it is the last user using this pcm: /* * Each rmap counts one refcount, plus the one grabbed * during evict_find_line(), we should have (nr_mapped + 1) * here if there are no any other users. * * Furthurmore, others can not go from munmap/mremap/wp to * put_pcache() within pcache_zap_pte(), pcache_move_pte() * or pcache_do_wp_page(). Thus the refcount must larger or * equal to (nr_mapped + 1). * * But if there truly other users (refcount > nr_mapped + 1), * then we should manually sub the refcount. The other users * which are currently holding the ref, will free the pcache * once it call put_pcache. */ PCACHE_BUG_ON_PCM ( pcache_ref_count ( pcm ) < nr_mapped + 1 , pcm ); if ( unlikely ( ! pcache_ref_freeze ( pcm , nr_mapped + 1 ))) { if ( unlikely ( pcache_ref_sub_and_test ( pcm , nr_mapped + 1 ))) { pr_info ( \"BUG: pcm refcount, nr_mapped: %d \\n \" , nr_mapped ); dump_pcache_meta ( pcm , \"ref error\" ); BUG (); } ClearPcacheReclaim ( pcm ); add_to_lru_list ( pcm , pset ); unlock_pcache ( pcm ); inc_pcache_event ( PCACHE_EVICTION_EAGAIN_CONCURRENT ); return PCACHE_EVICT_EAGAIN_CONCURRENT ; } My personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer. In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks. \u2013 Yizhou Shan Created: Mar 15, 2018 Last Updated: Mar 16, 2018","title":"Mumble pcache eviction and refcount"},{"location":"lego/pcache/pgtable-lock/","text":"Fine-grain Page Table Lock \u00b6 In old Linux or previous Lego, user page table operations, such as set, clear, are protected by mm->page_table_lock . This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention. But finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting 1 , where each PMD and PTE page table pages has their own lock. The lock is a spinlock embedded in the struct page . As illustrated below: Both Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance a lot . \u2013 Yizhou Shan Created: Mar 22, 2018 Last Updated: April 13, 2018 Split page table locks \u21a9","title":"PageTable Lock"},{"location":"lego/pcache/pgtable-lock/#fine-grain-page-table-lock","text":"In old Linux or previous Lego, user page table operations, such as set, clear, are protected by mm->page_table_lock . This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention. But finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting 1 , where each PMD and PTE page table pages has their own lock. The lock is a spinlock embedded in the struct page . As illustrated below: Both Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance a lot . \u2013 Yizhou Shan Created: Mar 22, 2018 Last Updated: April 13, 2018 Split page table locks \u21a9","title":"Fine-grain Page Table Lock"},{"location":"lego/pcache/replication/","text":"Memory Replication \u00b6 Keep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost. RAMCloud has two components running on a single machine: master , and backup . In lego, master is the handler running on Memory , backup is the handler running on Storage . Because of dual-Memory solution , we don\u2019t need a hash table from <pid, user_vaddr> to objects in log: M1 has its own <VA-PA> mapping table, and it will not be updated on replication. M2 does not need to look up. RAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply. \u2013 Yizhou Shan Created: Mar 31, 2018 Last Updated: Mar 31, 2018","title":"Replication"},{"location":"lego/pcache/replication/#memory-replication","text":"Keep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost. RAMCloud has two components running on a single machine: master , and backup . In lego, master is the handler running on Memory , backup is the handler running on Storage . Because of dual-Memory solution , we don\u2019t need a hash table from <pid, user_vaddr> to objects in log: M1 has its own <VA-PA> mapping table, and it will not be updated on replication. M2 does not need to look up. RAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply. \u2013 Yizhou Shan Created: Mar 31, 2018 Last Updated: Mar 31, 2018","title":"Memory Replication"},{"location":"lego/pcache/rmap/","text":"Reverse Mapping of Pcache \u00b6 This document explains Lego\u2019s reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures. For readers who are not familiar with reverse mapping, I recommend you search what is rmap in Linux first. Design \u00b6 The reverse mapping, or rmap, of our pcache is implemented in a very basic and straightforward way: pointing back to all page table entries (ptes) directly. Shared pcache lines will have a list of ptes that point to this pcache line. We also did this way in Hotpot. rmap is used by 1) a bunch of syscalls, such as fork() , execv() , mmap() , munmap() , mremap() , brk() . 2) page reclaim, which needs to unmap all ptes for a given swapped page. Other than fork() and execv() , other vm related syscalls are invoked very frequently for a typical datacenter application. Moreover, page reclaim and swap also run concurrently to gain exclusive access to rmap. So, rmap operations have to be fast. Directly pointing to pte seems the best solution here. However, this fine-granularity design will consume a lot memory for the per-pte list. Furthermore, vma creation, deletion, split and merge happen frequently, the overhead to manage rmap is quite high. No wonder Linux choses another object-based way to do so, which leverages vma itself to take a longer path towards pte. The important question is: does this naive solution fit current Lego? Yes, it fits, for several reasons. 1) Current Lego run static-linked ELF binary only, thus there will not be any shared hot library pages, which implies rmap list maintenance is simplified. 2) Our targeted applications mostly are single process. Even for multiple process ones, the number of processes stay stable and fork() happen at early init time. 3) major users of rmap such as mremap() and munmap() perform rmap operation explicitly, mmap() perform rmap implicitly via pgfault (or pcache miss), pcache reclaim perform sweep async. All of them, combined with 1) and 2), most of the time will perform rmap operation on a single pte. Internal \u00b6 The following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work. rmap operation Context Related functions and pcache callback Add fork() pgfault copy_pte_range() -> pcache_copy_pte() pcache_add_rmap() Remove munmap() exit_mmap() write_protected zap_pte_range() -> pcache_zap_pte() pcache_do_wp -> pcache_remove_rmap Update mremap() move_ptes() -> pcache_move_pte() Lookup pcache eviction sweep, etc. pcache_referenced() , pcache_wrprotect() pcache_try_to_unmap() Each rmap holds one refcount of pcache. The refcount is increased after pcache_add_rmap , and must be decreased after removing pcache rmap, can from pcache_remove_rmap , pcache_zap_pte or pcache_move_pte_slowpath . Thought \u00b6 One function I personally love the most is rmap_walk() , whose name pretty much tells the story. To use rmap_walk() , caller passes a struct rmap_walk_control , which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon rmap_walk() . struct rmap_walk_control , or struct scan_control , or struct something_control are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying: Do one thing and do it better , while it not always works that perfectly. Coding is nothing different life, it is all about trade-off. \u2013 Yizhou Shan Created: Feb 02, 2018 Last Updated: Mar 10, 2018","title":"Reverse Mapping"},{"location":"lego/pcache/rmap/#reverse-mapping-of-pcache","text":"This document explains Lego\u2019s reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures. For readers who are not familiar with reverse mapping, I recommend you search what is rmap in Linux first.","title":"Reverse Mapping of Pcache"},{"location":"lego/pcache/rmap/#design","text":"The reverse mapping, or rmap, of our pcache is implemented in a very basic and straightforward way: pointing back to all page table entries (ptes) directly. Shared pcache lines will have a list of ptes that point to this pcache line. We also did this way in Hotpot. rmap is used by 1) a bunch of syscalls, such as fork() , execv() , mmap() , munmap() , mremap() , brk() . 2) page reclaim, which needs to unmap all ptes for a given swapped page. Other than fork() and execv() , other vm related syscalls are invoked very frequently for a typical datacenter application. Moreover, page reclaim and swap also run concurrently to gain exclusive access to rmap. So, rmap operations have to be fast. Directly pointing to pte seems the best solution here. However, this fine-granularity design will consume a lot memory for the per-pte list. Furthermore, vma creation, deletion, split and merge happen frequently, the overhead to manage rmap is quite high. No wonder Linux choses another object-based way to do so, which leverages vma itself to take a longer path towards pte. The important question is: does this naive solution fit current Lego? Yes, it fits, for several reasons. 1) Current Lego run static-linked ELF binary only, thus there will not be any shared hot library pages, which implies rmap list maintenance is simplified. 2) Our targeted applications mostly are single process. Even for multiple process ones, the number of processes stay stable and fork() happen at early init time. 3) major users of rmap such as mremap() and munmap() perform rmap operation explicitly, mmap() perform rmap implicitly via pgfault (or pcache miss), pcache reclaim perform sweep async. All of them, combined with 1) and 2), most of the time will perform rmap operation on a single pte.","title":"Design"},{"location":"lego/pcache/rmap/#internal","text":"The following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work. rmap operation Context Related functions and pcache callback Add fork() pgfault copy_pte_range() -> pcache_copy_pte() pcache_add_rmap() Remove munmap() exit_mmap() write_protected zap_pte_range() -> pcache_zap_pte() pcache_do_wp -> pcache_remove_rmap Update mremap() move_ptes() -> pcache_move_pte() Lookup pcache eviction sweep, etc. pcache_referenced() , pcache_wrprotect() pcache_try_to_unmap() Each rmap holds one refcount of pcache. The refcount is increased after pcache_add_rmap , and must be decreased after removing pcache rmap, can from pcache_remove_rmap , pcache_zap_pte or pcache_move_pte_slowpath .","title":"Internal"},{"location":"lego/pcache/rmap/#thought","text":"One function I personally love the most is rmap_walk() , whose name pretty much tells the story. To use rmap_walk() , caller passes a struct rmap_walk_control , which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon rmap_walk() . struct rmap_walk_control , or struct scan_control , or struct something_control are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying: Do one thing and do it better , while it not always works that perfectly. Coding is nothing different life, it is all about trade-off. \u2013 Yizhou Shan Created: Feb 02, 2018 Last Updated: Mar 10, 2018","title":"Thought"},{"location":"lego/pcache/smp_design/","text":"SMP Design Thought \u00b6 Coding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass. I don\u2019t think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back. Pcache and Victim Cache Organization \u00b6 Our pcache and victim cache are allocated and arranged as a big array. As for pcache we look at it in a cache set view , which means consecutive pcache lines are not relevant in natual. As for victim cache, we simply treat it as a big array and walk through it one by one. Allocation/Eviction SMP Consideration \u00b6 The alloc/free of both pcache and victim cache are simple: each pcache line or victim cache line has a Allocated bit to indicate if this line is free or not. The Allocated bit is manipulated by atomic bit operations, thus SMP safe. This further implies that we do not need another spinlock to guard allocation. However, other activities such as explict eviction, background sweep may walk through the cache lines at the same time of cache allocation, a single Allocated bit is not enough. Because an allocated cache line will need some initial setup, such as reset refcount, clear flags (prep_new_pcache), thus there is a small time gap between Allocated bit being set and the cache line being truly safe to use. Other activities must wait the cache line to be usable, and then they can do further operations on this cache line. To solve this race condition, there two possible solutions: 1) Add another bit: Usable , which is set once initial setup is done. In this case, functions excluding alloction code should always check if the Usable bit is set or not. a) If it is set, this means the cache line is safe for further operations b) If not, and Allocated bit is set, this means the cache line is under setup in another core, We should skip it. c) If not, and Allocated bit is not set, this means this cache line is simply free. We should skip it. 2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation code will only look into cache lines within this list. In other words, others will only look into surely usable cache lines. Both solutions try to avoid others looking into un-mature cache lines in SMP envorinment. The rule is simple: function should NOT look into data that is not supposed to be seen. The cache line that has Allocated bit set but under setup is a typical case. As an example, the physical page allocator, page reclaim, page cache in Linux are implemented with the second solution. Pages freshly allocated will be added a LRU list or page cache own list. And page reclaim code will only look into pages within the LRU list, it will not go through all physical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole physical pages to find out pages to operate. Pcache: When it comes to pcache, we use both. In our envision, pcache will have high-associativity such as 64 or 128. It will have very bad performance if our eviction algorithm or sweep thread need to go through every cache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines. However, additional Usable bit is added for debug purpose. Victim Cache: When it comes to victim cache, the first solution seems a better choice. Because victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line walk is fast. While the list deletion and addition seem may introduce some unnecessary overhead. It is all about trade-off. These choices affect the usage of pcache and victim cache, mostly the eviction code. More on above two solutions \u00b6 The first solution is used if evict_random is configured. The second solution is used when evict_lru is configured. I do not have any doubt about second solution, it works, though with a lot SMP pain in ass. But I do have more to say about the first solution, which is adding another usable bit. The Usable bit only ensures other threads will not use unmature pcache, but it can not prevent other threads seeing a going-to-be-freed pcache. What is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction and checked the Usable bit, which is set. Then CPU0 thought this cache line is all set, ready to be torqued. Before doing all the dirty work, CPU0 will get_pcache_unless_zero() first to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did a put_pcache() and a consecutive pcache_alloc() right before CPU0 did called get_pcache_unless_zero() . Bang! CPU0 may use an mature pcache line, cause CPU1\u2019s pcache_init_ref_count() may come before CPU1\u2019s get_pcache_unless_zero() ! How to solve this? CPU0 need to add additional checking after get_pcache_unless_zero() . For more details, please check the code in pcache/evcit_random.c , which has more pretty explanation. \u2013 Yizhou Shan Jan 31, 2018","title":"SMP Design"},{"location":"lego/pcache/smp_design/#smp-design-thought","text":"Coding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass. I don\u2019t think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back.","title":"SMP Design Thought"},{"location":"lego/pcache/smp_design/#pcache-and-victim-cache-organization","text":"Our pcache and victim cache are allocated and arranged as a big array. As for pcache we look at it in a cache set view , which means consecutive pcache lines are not relevant in natual. As for victim cache, we simply treat it as a big array and walk through it one by one.","title":"Pcache and Victim Cache Organization"},{"location":"lego/pcache/smp_design/#allocationeviction-smp-consideration","text":"The alloc/free of both pcache and victim cache are simple: each pcache line or victim cache line has a Allocated bit to indicate if this line is free or not. The Allocated bit is manipulated by atomic bit operations, thus SMP safe. This further implies that we do not need another spinlock to guard allocation. However, other activities such as explict eviction, background sweep may walk through the cache lines at the same time of cache allocation, a single Allocated bit is not enough. Because an allocated cache line will need some initial setup, such as reset refcount, clear flags (prep_new_pcache), thus there is a small time gap between Allocated bit being set and the cache line being truly safe to use. Other activities must wait the cache line to be usable, and then they can do further operations on this cache line. To solve this race condition, there two possible solutions: 1) Add another bit: Usable , which is set once initial setup is done. In this case, functions excluding alloction code should always check if the Usable bit is set or not. a) If it is set, this means the cache line is safe for further operations b) If not, and Allocated bit is set, this means the cache line is under setup in another core, We should skip it. c) If not, and Allocated bit is not set, this means this cache line is simply free. We should skip it. 2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation code will only look into cache lines within this list. In other words, others will only look into surely usable cache lines. Both solutions try to avoid others looking into un-mature cache lines in SMP envorinment. The rule is simple: function should NOT look into data that is not supposed to be seen. The cache line that has Allocated bit set but under setup is a typical case. As an example, the physical page allocator, page reclaim, page cache in Linux are implemented with the second solution. Pages freshly allocated will be added a LRU list or page cache own list. And page reclaim code will only look into pages within the LRU list, it will not go through all physical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole physical pages to find out pages to operate. Pcache: When it comes to pcache, we use both. In our envision, pcache will have high-associativity such as 64 or 128. It will have very bad performance if our eviction algorithm or sweep thread need to go through every cache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines. However, additional Usable bit is added for debug purpose. Victim Cache: When it comes to victim cache, the first solution seems a better choice. Because victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line walk is fast. While the list deletion and addition seem may introduce some unnecessary overhead. It is all about trade-off. These choices affect the usage of pcache and victim cache, mostly the eviction code.","title":"Allocation/Eviction SMP Consideration"},{"location":"lego/pcache/smp_design/#more-on-above-two-solutions","text":"The first solution is used if evict_random is configured. The second solution is used when evict_lru is configured. I do not have any doubt about second solution, it works, though with a lot SMP pain in ass. But I do have more to say about the first solution, which is adding another usable bit. The Usable bit only ensures other threads will not use unmature pcache, but it can not prevent other threads seeing a going-to-be-freed pcache. What is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction and checked the Usable bit, which is set. Then CPU0 thought this cache line is all set, ready to be torqued. Before doing all the dirty work, CPU0 will get_pcache_unless_zero() first to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did a put_pcache() and a consecutive pcache_alloc() right before CPU0 did called get_pcache_unless_zero() . Bang! CPU0 may use an mature pcache line, cause CPU1\u2019s pcache_init_ref_count() may come before CPU1\u2019s get_pcache_unless_zero() ! How to solve this? CPU0 need to add additional checking after get_pcache_unless_zero() . For more details, please check the code in pcache/evcit_random.c , which has more pretty explanation. \u2013 Yizhou Shan Jan 31, 2018","title":"More on above two solutions"},{"location":"lego/pcache/sweep/","text":"Pcache Sweep \u00b6 Some notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly. Data Worth a Billion \u00b6 Pcache-reclaim, or any other object reclaim, need some data to algorithm about. So specific algorithm can select the best candidate to reclaim. In reality, algorithms are designed quite well, but how to get the data part becomes extremely hard. I think this applies to many different systems. For example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a Referenced bit for system software to reason about. To make it worse, Referenced bit is cached in TLB, which means CPU will NOT set the Referenced bit even you reset in PTE, because CPU think the bit is already set. In order to get an accurate hot pages tracking, you probably need a TLB flush after reset Referenced bit. But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well: static inline int ptep_clear_flush_young ( pte_t * ptep ) { /* * On x86 CPUs, clearing the accessed bit without a TLB flush * doesn't cause data corruption. [ It could cause incorrect * page aging and the (mistaken) reclaim of hot pages, but the * chance of that should be relatively low. ] * * So as a performance optimization don't flush the TLB when * clearing the accessed bit, it will eventually be flushed by * a context switch or a VM operation anyway. [ In the rare * event of it not getting flushed for a long time the delay * shouldn't really matter because there's no real memory * pressure for swapout to react to. ] */ return ptep_test_and_clear_young ( ptep ); } Aggressiveness \u00b6 An aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness: Time interval between each run Number of sets to look at during each run Skip if it is not full Skip if it is under eviction Number of lines to look at for each set Smaller or equal to associativity Number of lines to adjust for each set Smaller or equal to lines to look at \u2013 Yizhou Shan Created: Mar 18, 2018 Last Updated: Mar 18, 2018","title":"Sweep"},{"location":"lego/pcache/sweep/#pcache-sweep","text":"Some notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly.","title":"Pcache Sweep"},{"location":"lego/pcache/sweep/#data-worth-a-billion","text":"Pcache-reclaim, or any other object reclaim, need some data to algorithm about. So specific algorithm can select the best candidate to reclaim. In reality, algorithms are designed quite well, but how to get the data part becomes extremely hard. I think this applies to many different systems. For example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a Referenced bit for system software to reason about. To make it worse, Referenced bit is cached in TLB, which means CPU will NOT set the Referenced bit even you reset in PTE, because CPU think the bit is already set. In order to get an accurate hot pages tracking, you probably need a TLB flush after reset Referenced bit. But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well: static inline int ptep_clear_flush_young ( pte_t * ptep ) { /* * On x86 CPUs, clearing the accessed bit without a TLB flush * doesn't cause data corruption. [ It could cause incorrect * page aging and the (mistaken) reclaim of hot pages, but the * chance of that should be relatively low. ] * * So as a performance optimization don't flush the TLB when * clearing the accessed bit, it will eventually be flushed by * a context switch or a VM operation anyway. [ In the rare * event of it not getting flushed for a long time the delay * shouldn't really matter because there's no real memory * pressure for swapout to react to. ] */ return ptep_test_and_clear_young ( ptep ); }","title":"Data Worth a Billion"},{"location":"lego/pcache/sweep/#aggressiveness","text":"An aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness: Time interval between each run Number of sets to look at during each run Skip if it is not full Skip if it is under eviction Number of lines to look at for each set Smaller or equal to associativity Number of lines to adjust for each set Smaller or equal to lines to look at \u2013 Yizhou Shan Created: Mar 18, 2018 Last Updated: Mar 18, 2018","title":"Aggressiveness"},{"location":"lego/pcache/tlb/","text":"TLB Coherence \u00b6 x86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I\u2019ve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed: munmap (optional, can be optimized by holding the old VA range) mremap (required) fork (RW->RO) (required) CoW (RO->RW) (required) mprotect (required) migration (required) Unfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown 1 2 3 is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as TODO ). \u2013 Yizhou Shan Created: Mar 19, 2018 Last Updated: Mar 19, 2018 Optimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC\u201818 \u21a9 LATR: Lazy Translation Coherence, ASPLOS\u201818 \u21a9 Hardware Translation Coherence for Virtualized Systems, ISCA\u201817 \u21a9","title":"TLB Coherence"},{"location":"lego/pcache/tlb/#tlb-coherence","text":"x86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I\u2019ve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed: munmap (optional, can be optimized by holding the old VA range) mremap (required) fork (RW->RO) (required) CoW (RO->RW) (required) mprotect (required) migration (required) Unfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown 1 2 3 is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as TODO ). \u2013 Yizhou Shan Created: Mar 19, 2018 Last Updated: Mar 19, 2018 Optimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC\u201818 \u21a9 LATR: Lazy Translation Coherence, ASPLOS\u201818 \u21a9 Hardware Translation Coherence for Virtualized Systems, ISCA\u201817 \u21a9","title":"TLB Coherence"},{"location":"lego/pcache/victim/","text":"Victim Cache \u00b6 \u2013 Yizhou Shan Created: Mar 12, 2018 Last Updated: Mar 12, 2018","title":"Victim Cache"},{"location":"lego/pcache/victim/#victim-cache","text":"\u2013 Yizhou Shan Created: Mar 12, 2018 Last Updated: Mar 12, 2018","title":"Victim Cache"},{"location":"lego/pcache/virtual_cache/","text":"Virtual Cache \u00b6 Synonymous impact Cache coherence impact TLB shootdown The good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address. What is this? What is bad about this? When does this happen? (All cases: kernel, shared vma mapping) How to solve this? Software solution: OS level detection, global virtual address, identical virtual address etc. Hardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas. A nice summary can be found on A new perspective for efficient virtual-cache coherence, ISCA'13 . Let me share my reading list. I think I\u2019ve collected most of the important virtual cache papers: TLB and cache line lifetime: Enigma, ISC'10 : For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit. GPU virtual cache, ASPLOS'18 : We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries Reading the GPU virtual cache ASPLOS'18 paper today. I mostly interested in how they handle synonymous and mremap issue. Synonymous: Their solution for synonymous is quite simple (not sure if practical or effective): use a leading virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from different VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines. mremap: They did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss OVC also does not need to care about this because they are doing a similar way (I guess). Lego need to handle mremap differently. Because we don\u2019t want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case. \u2013 Yizhou Shan Created: Mar 28, 2018 Last Updated: Mar 29, 2018","title":"Virtual Cache"},{"location":"lego/pcache/virtual_cache/#virtual-cache","text":"Synonymous impact Cache coherence impact TLB shootdown The good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address. What is this? What is bad about this? When does this happen? (All cases: kernel, shared vma mapping) How to solve this? Software solution: OS level detection, global virtual address, identical virtual address etc. Hardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas. A nice summary can be found on A new perspective for efficient virtual-cache coherence, ISCA'13 . Let me share my reading list. I think I\u2019ve collected most of the important virtual cache papers: TLB and cache line lifetime: Enigma, ISC'10 : For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit. GPU virtual cache, ASPLOS'18 : We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries Reading the GPU virtual cache ASPLOS'18 paper today. I mostly interested in how they handle synonymous and mremap issue. Synonymous: Their solution for synonymous is quite simple (not sure if practical or effective): use a leading virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from different VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines. mremap: They did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss OVC also does not need to care about this because they are doing a similar way (I guess). Lego need to handle mremap differently. Because we don\u2019t want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case. \u2013 Yizhou Shan Created: Mar 28, 2018 Last Updated: Mar 29, 2018","title":"Virtual Cache"},{"location":"lego/syscall/compat/","text":"Compat SYSCALL in Lego \u00b6 Lego does not support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don\u2019t break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic. Kconfig \u00b6 If one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess: CONFIG_COMPAT CONFIG_IA32_EMULATION Internal \u00b6 Entry Points \u00b6 The assembly entry points are defined in entry/entry_64_compat.S : ENTRY ( entry_SYSENTER_compat ) ... call do_fast_syscall_32 GLOBAL ( __end_entry_SYSENTER_compat ) ENDPROC ( entry_SYSENTER_compat ) ENTRY ( entry_SYSCALL_compat ) ... call do_fast_syscall_32 END ( entry_SYSCALL_compat ) ENTRY ( entry_INT80_compat ) ... call do_int80_syscall_32 END ( entry_INT80_compat ) Entry Points Setup \u00b6 The assembly entry points are filled to system registers and IDT table. So users can actually issue those calls, Lego is able to catch them: static void syscall_init ( void ) { wrmsr ( MSR_STAR , 0 , ( __USER32_CS << 16 ) | __KERNEL_CS ); wrmsrl ( MSR_LSTAR , ( unsigned long ) entry_SYSCALL_64 ); #ifdef CONFIG_IA32_EMULATION wrmsrl ( MSR_CSTAR , ( unsigned long ) entry_SYSCALL_compat ); /* * This only works on Intel CPUs. * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP. * This does not cause SYSENTER to jump to the wrong location, because * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit). */ wrmsrl_safe ( MSR_IA32_SYSENTER_CS , ( u64 ) __KERNEL_CS ); wrmsrl_safe ( MSR_IA32_SYSENTER_ESP , 0ULL ); wrmsrl_safe ( MSR_IA32_SYSENTER_EIP , ( u64 ) entry_SYSENTER_compat ); #else wrmsrl ( MSR_CSTAR , ( unsigned long ) ignore_sysret ); wrmsrl_safe ( MSR_IA32_SYSENTER_CS , ( u64 ) GDT_ENTRY_INVALID_SEG ); wrmsrl_safe ( MSR_IA32_SYSENTER_ESP , 0ULL ); wrmsrl_safe ( MSR_IA32_SYSENTER_EIP , 0ULL ); #endif /* Flags to clear on syscall */ wrmsrl ( MSR_SYSCALL_MASK , X86_EFLAGS_TF | X86_EFLAGS_DF | X86_EFLAGS_IF | X86_EFLAGS_IOPL | X86_EFLAGS_AC | X86_EFLAGS_NT ); } arch / x86 / kernel / cpu / common . c void __init trap_init ( void ) { ... #ifdef CONFIG_IA32_EMULATION set_system_intr_gate ( IA32_SYSCALL_VECTOR , entry_INT80_compat ); set_bit ( IA32_SYSCALL_VECTOR , used_vectors ); #endif ... } arch / x86 / kernel / traps . c C code \u00b6 The actual C code is in entry/common.c : #if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION) static __always_inline void do_syscall_32_irqs_on ( struct pt_regs * regs ) { #ifdef CONFIG_IA32_EMULATION current -> thread . status |= TS_COMPAT ; #endif BUG (); } /* Handles int $0x80 */ __visible void do_int80_syscall_32 ( struct pt_regs * regs ) { BUG (); } /* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */ __visible long do_fast_syscall_32 ( struct pt_regs * regs ) { BUG (); } #endif \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 22, 2018","title":"compat"},{"location":"lego/syscall/compat/#compat-syscall-in-lego","text":"Lego does not support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don\u2019t break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic.","title":"Compat SYSCALL in Lego"},{"location":"lego/syscall/compat/#kconfig","text":"If one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess: CONFIG_COMPAT CONFIG_IA32_EMULATION","title":"Kconfig"},{"location":"lego/syscall/compat/#internal","text":"","title":"Internal"},{"location":"lego/syscall/compat/#entry-points","text":"The assembly entry points are defined in entry/entry_64_compat.S : ENTRY ( entry_SYSENTER_compat ) ... call do_fast_syscall_32 GLOBAL ( __end_entry_SYSENTER_compat ) ENDPROC ( entry_SYSENTER_compat ) ENTRY ( entry_SYSCALL_compat ) ... call do_fast_syscall_32 END ( entry_SYSCALL_compat ) ENTRY ( entry_INT80_compat ) ... call do_int80_syscall_32 END ( entry_INT80_compat )","title":"Entry Points"},{"location":"lego/syscall/compat/#entry-points-setup","text":"The assembly entry points are filled to system registers and IDT table. So users can actually issue those calls, Lego is able to catch them: static void syscall_init ( void ) { wrmsr ( MSR_STAR , 0 , ( __USER32_CS << 16 ) | __KERNEL_CS ); wrmsrl ( MSR_LSTAR , ( unsigned long ) entry_SYSCALL_64 ); #ifdef CONFIG_IA32_EMULATION wrmsrl ( MSR_CSTAR , ( unsigned long ) entry_SYSCALL_compat ); /* * This only works on Intel CPUs. * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP. * This does not cause SYSENTER to jump to the wrong location, because * AMD doesn't allow SYSENTER in long mode (either 32- or 64-bit). */ wrmsrl_safe ( MSR_IA32_SYSENTER_CS , ( u64 ) __KERNEL_CS ); wrmsrl_safe ( MSR_IA32_SYSENTER_ESP , 0ULL ); wrmsrl_safe ( MSR_IA32_SYSENTER_EIP , ( u64 ) entry_SYSENTER_compat ); #else wrmsrl ( MSR_CSTAR , ( unsigned long ) ignore_sysret ); wrmsrl_safe ( MSR_IA32_SYSENTER_CS , ( u64 ) GDT_ENTRY_INVALID_SEG ); wrmsrl_safe ( MSR_IA32_SYSENTER_ESP , 0ULL ); wrmsrl_safe ( MSR_IA32_SYSENTER_EIP , 0ULL ); #endif /* Flags to clear on syscall */ wrmsrl ( MSR_SYSCALL_MASK , X86_EFLAGS_TF | X86_EFLAGS_DF | X86_EFLAGS_IF | X86_EFLAGS_IOPL | X86_EFLAGS_AC | X86_EFLAGS_NT ); } arch / x86 / kernel / cpu / common . c void __init trap_init ( void ) { ... #ifdef CONFIG_IA32_EMULATION set_system_intr_gate ( IA32_SYSCALL_VECTOR , entry_INT80_compat ); set_bit ( IA32_SYSCALL_VECTOR , used_vectors ); #endif ... } arch / x86 / kernel / traps . c","title":"Entry Points Setup"},{"location":"lego/syscall/compat/#c-code","text":"The actual C code is in entry/common.c : #if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION) static __always_inline void do_syscall_32_irqs_on ( struct pt_regs * regs ) { #ifdef CONFIG_IA32_EMULATION current -> thread . status |= TS_COMPAT ; #endif BUG (); } /* Handles int $0x80 */ __visible void do_int80_syscall_32 ( struct pt_regs * regs ) { BUG (); } /* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */ __visible long do_fast_syscall_32 ( struct pt_regs * regs ) { BUG (); } #endif \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 22, 2018","title":"C code"},{"location":"lego/syscall/facts/","text":"Lego SYSCALL Facts \u00b6 This document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first. Interrupts Enabled \u00b6 Each syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by syscall_return_slowpath() : void syscall_return_slowpath ( struct pt_regs * regs ) { if ( WARN ( irqs_disabled (), \"syscall %ld left IRQs disabled\" , regs -> orig_ax )) local_irq_enable (); local_irq_disable (); prepare_exit_to_usermode ( regs ); } void do_syscall_64 ( struct pt_regs * regs ) { .. local_irq_enable (); if ( likely ( nr < NR_syscalls )) { regs -> ax = sys_call_table [ nr ]( regs -> di , regs -> si , regs -> dx , regs -> r10 , regs -> r8 , regs -> r9 ); } syscall_return_slowpath ( regs ); .. } Get User Entry pt_regs \u00b6 The macro task_pt_regs() always return the pt_regs , that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call task_pt_regs() . This is based on the fact that kernel stack is empty at syscall entry, thus this user pt_regs was saved at the top of kernel stack: #define task_pt_regs(tsk) ((struct pt_regs *)(tsk)->thread.sp0 - 1) ENTRY ( entry_SYSCALL_64 ) SWAPGS /* * SYSCALL does not change rsp for us! * Save the previous rsp and load the top of kernel stack. * It must be the top of kernel stack, since we came here * from *userspace*. */ movq %rsp , PER_CPU_VAR ( rsp_scratch ) movq PER_CPU_VAR ( cpu_current_top_of_stack ), %rsp /* * Construct struct pt_regs on stack * * In any syscall handler, you can use * current_pt_regs() * to get these registers. */ pushq $__USER_DS /* pt_regs->ss */ pushq PER_CPU_VAR ( rsp_scratch ) /* pt_regs->sp */ pushq %r11 /* pt_regs->flags */ pushq $__USER_CS /* pt_regs->cs */ pushq %rcx /* pt_regs->ip */ pushq %rax /* pt_regs->orig_ax */ pushq %rdi /* pt_regs->di */ pushq %rsi /* pt_regs->si */ pushq %rdx /* pt_regs->dx */ pushq %rcx /* pt_regs->cx */ pushq $-ENOSYS /* pt_regs->ax */ pushq %r8 /* pt_regs->r8 */ pushq %r9 /* pt_regs->r9 */ pushq %r10 /* pt_regs->r10 */ pushq %r11 /* pt_regs->r11 */ sub $ ( 6 * 8 ), %rsp /* pt_regs->bp, bx, r12-15 */ .... \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 22, 2018","title":"Facts"},{"location":"lego/syscall/facts/#lego-syscall-facts","text":"This document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first.","title":"Lego SYSCALL Facts"},{"location":"lego/syscall/facts/#interrupts-enabled","text":"Each syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by syscall_return_slowpath() : void syscall_return_slowpath ( struct pt_regs * regs ) { if ( WARN ( irqs_disabled (), \"syscall %ld left IRQs disabled\" , regs -> orig_ax )) local_irq_enable (); local_irq_disable (); prepare_exit_to_usermode ( regs ); } void do_syscall_64 ( struct pt_regs * regs ) { .. local_irq_enable (); if ( likely ( nr < NR_syscalls )) { regs -> ax = sys_call_table [ nr ]( regs -> di , regs -> si , regs -> dx , regs -> r10 , regs -> r8 , regs -> r9 ); } syscall_return_slowpath ( regs ); .. }","title":"Interrupts Enabled"},{"location":"lego/syscall/facts/#get-user-entry-pt_regs","text":"The macro task_pt_regs() always return the pt_regs , that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call task_pt_regs() . This is based on the fact that kernel stack is empty at syscall entry, thus this user pt_regs was saved at the top of kernel stack: #define task_pt_regs(tsk) ((struct pt_regs *)(tsk)->thread.sp0 - 1) ENTRY ( entry_SYSCALL_64 ) SWAPGS /* * SYSCALL does not change rsp for us! * Save the previous rsp and load the top of kernel stack. * It must be the top of kernel stack, since we came here * from *userspace*. */ movq %rsp , PER_CPU_VAR ( rsp_scratch ) movq PER_CPU_VAR ( cpu_current_top_of_stack ), %rsp /* * Construct struct pt_regs on stack * * In any syscall handler, you can use * current_pt_regs() * to get these registers. */ pushq $__USER_DS /* pt_regs->ss */ pushq PER_CPU_VAR ( rsp_scratch ) /* pt_regs->sp */ pushq %r11 /* pt_regs->flags */ pushq $__USER_CS /* pt_regs->cs */ pushq %rcx /* pt_regs->ip */ pushq %rax /* pt_regs->orig_ax */ pushq %rdi /* pt_regs->di */ pushq %rsi /* pt_regs->si */ pushq %rdx /* pt_regs->dx */ pushq %rcx /* pt_regs->cx */ pushq $-ENOSYS /* pt_regs->ax */ pushq %r8 /* pt_regs->r8 */ pushq %r9 /* pt_regs->r9 */ pushq %r10 /* pt_regs->r10 */ pushq %r11 /* pt_regs->r11 */ sub $ ( 6 * 8 ), %rsp /* pt_regs->bp, bx, r12-15 */ .... \u2013 Yizhou Shan Created: Feb 22, 2018 Last Updated: Feb 22, 2018","title":"Get User Entry pt_regs"},{"location":"lego/syscall/fork/","text":"fork() \u00b6 Memory Manager \u00b6 We need to duplicate the address space in the memory manager side. Follow the traditional fork() semantic, both the existing and newly created address space will be write-protected. Since we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new pgd and copy existing pgtables, and further downgrade permission to read-only. This is now performed by lego_copy_page_range() . The final write-protect is performed by lego_copy_one_pte() : static inline int lego_copy_one_pte (..) { .. /* * If it's a COW mapping, write protect it both * in the parent and the child */ if ( is_cow_mapping ( vm_flags )) { ptep_set_wrprotect ( src_pte ); pte = pte_wrprotect ( pte ); } ... } Duplicate VM Free Pool \u00b6 TODO Yutong Processor Manager \u00b6 Boring implementation details in the processor manager side. Entry Points \u00b6 fork() vfork() clone() kernel_thread() All of them land on do_fork() , which is Lego\u2019s main fork function. do_fork() \u00b6 There are mainly three parts within do_fork() : 1) copy_process() , which duplicates a new task based on current , including allocate new kernel stack, new task_struct, increase mm reference counter, etc. 2) If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures. 3) wake_up_new_task() , which gives away the newly created task to local scheduler. copy_process() \u00b6 The routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are task_struct , mm_sturct , sighand , and so on. This section only talks about few of them, and leave others to readers who are interested. Sanity Checking \u00b6 Mainly check if clone_flags are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If CLONE_THREAD is passed, then CLONE_SIGHAND , CLONE_VM , and so on must be set as well. /* * Thread groups must share signals as well, and detached threads * can only be started up within the thread group. */ if (( clone_flags & CLONE_THREAD ) && ! ( clone_flags & CLONE_SIGHAND )) return ERR_PTR ( - EINVAL ); /* * Shared signal handlers imply shared VM. By way of the above, * thread groups also imply shared VM. Blocking this case allows * for various simplifications in other code. */ if (( clone_flags & CLONE_SIGHAND ) && ! ( clone_flags & CLONE_VM )) return ERR_PTR ( - EINVAL ); dup_task_struct() \u00b6 Two main things: 1) duplicate a new task_struct , 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of task_struct depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info. int arch_dup_task_struct ( struct task_struct * dst , struct task_struct * src ) { memcpy ( dst , src , arch_task_struct_size ); return fpu__copy ( & dst -> thread . fpu , & src -> thread . fpu ); } The stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the thread_info to points to this new thread, so the current macro will work. static void setup_thread_stack ( struct task_struct * p , struct task_struct * org ) { /* Duplicate whole stack! */ * task_thread_info ( p ) = * task_thread_info ( org ); /* Make the `current' macro work */ task_thread_info ( p ) -> task = p ; } copy_mm() \u00b6 This is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new mm_struct , and also a new pgd : /* * pgd_alloc() will duplicate the identity kernel mapping * but leaves other entries empty: */ mm -> pgd = pgd_alloc ( mm ); if ( unlikely ( ! mm -> pgd )) { kfree ( mm ); return NULL ; } Duplicate pcache data \u00b6 TODO TODO: hook with pcache We need to duplicate the pcache vm_range array, once Yutong finished the code. setup_sched_fork() \u00b6 Callback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task\u2019s scheduler class: int setup_sched_fork ( unsigned long clone_flags , struct task_struct * p ) { int cpu = get_cpu (); __sched_fork ( clone_flags , p ); p -> state = TASK_NEW ; ... if ( unlikely ( p -> sched_reset_on_fork )) { if ( task_has_rt_policy ( p )) { p -> policy = SCHED_NORMAL ; p -> static_prio = NICE_TO_PRIO ( 0 ); p -> rt_priority = 0 ; } else if ( PRIO_TO_NICE ( p -> static_prio ) < 0 ) p -> static_prio = NICE_TO_PRIO ( 0 ); p -> prio = p -> normal_prio = __normal_prio ( p ); set_load_weight ( p ); ... } if ( rt_prio ( p -> prio )) p -> sched_class = & rt_sched_class ; else { p -> sched_class = & fair_sched_class ; set_load_weight ( p ); } __set_task_cpu ( p , cpu ); if ( p -> sched_class -> task_fork ) p -> sched_class -> task_fork ( p ); ... } Allocate new pid \u00b6 In both Lego and Linux, we don\u2019t allocate new pid for a new thread, if that thread is an idle thread . So callers of do_fork needs to pass something to let do_fork know. In Linux, they use struct pid, init_struct_pid to check. In Lego, we introduce an new clone_flag CLONE_IDLE_THREAD . If that flag is set, do_fork() will try to allocate a new pid for the new thread. Otherwise, it will be 0: /* clone idle thread, whose pid is 0 */ if ( ! ( clone_flags & CLONE_IDLE_THREAD )) { pid = alloc_pid ( p ); if ( ! pid ) goto out_cleanup_thread ; } So, only the init_idle() function can pass this CLONE_IDLE_THREAD down. All other usages are wrong and should be reported. In order to avoid conflict with Linux clone_flag, we define it as: #define CLONE_IDLE_THREAD 0x100000000 SETTID/CLEARTID \u00b6 These are some futex related stuff. I will cover these stuff in futex document: p -> set_child_tid = ( clone_flags & CLONE_CHILD_SETTID ) ? child_tidptr : NULL ; /* * Clear TID on mm_release()? */ p -> clear_child_tid = ( clone_flags & CLONE_CHILD_CLEARTID ) ? child_tidptr : NULL ; #ifdef CONFIG_FUTEX p -> robust_list = NULL ; #endif copy_thread_tls() \u00b6 This is the most interesting function. Cover later. p2m_fork() \u00b6 In order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode. /* Tell remote memory component */ #ifdef CONFIG_COMP_PROCESSOR if ( clone_flags & CLONE_GLOBAL_THREAD ) { ... p2m_fork ( p , clone_flags ); ... } #endif The CLONE_GLOBAL_THREAD should only be set, if the following cases happen: fork() vfork() clone(), without CLONE_THREAD being set In order to avoid conflict with Linux clone_flag, we define it as: #define CLONE_GLOBAL_THREAD 0x200000000 wake_up_new_task() \u00b6 The last step of do_fork is waking up the new thread or process, which is performed by wake_up_new_task() function. The first question this function will ask is: which cpu to land? The answer comes from select_task_rq() : static inline int select_task_rq ( struct task_struct * p , int cpu , int sd_flags , int wake_flags ) { if ( p -> nr_cpus_allowed > 1 ) cpu = p -> sched_class -> select_task_rq ( p , cpu , sd_flags , wake_flags ); else cpu = cpumask_any ( & p -> cpus_allowed ); ... } Clearly, this is determined by cpus_allowed , which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux: A child created via fork(2) inherits its parent's CPU affinity mask. The affinity mask is preserved across an execve(2). After landing CPU is selected, following operation is simple: just enqueue this task into landing CPU\u2019s runqueue, and we are done: void wake_up_new_task ( struct task_struct * p ) { ... /* Select a CPU for new thread to run */ #ifdef CONFIG_SMP /* * Fork balancing, do it here and not earlier because: * - cpus_allowed can change in the fork path * - any previously selected cpu might disappear through hotplug */ set_task_cpu ( p , select_task_rq ( p , task_cpu ( p ), SD_BALANCE_FORK , 0 )); #endif rq = __task_rq_lock ( p ); activate_task ( rq , p , 0 ); p -> on_rq = TASK_ON_RQ_QUEUED ; ... } \u2013 Yizhou Shan Created: Feb 11, 2018 Last Updated: Feb 27, 2018","title":"fork()"},{"location":"lego/syscall/fork/#fork","text":"","title":"fork()"},{"location":"lego/syscall/fork/#memory-manager","text":"We need to duplicate the address space in the memory manager side. Follow the traditional fork() semantic, both the existing and newly created address space will be write-protected. Since we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new pgd and copy existing pgtables, and further downgrade permission to read-only. This is now performed by lego_copy_page_range() . The final write-protect is performed by lego_copy_one_pte() : static inline int lego_copy_one_pte (..) { .. /* * If it's a COW mapping, write protect it both * in the parent and the child */ if ( is_cow_mapping ( vm_flags )) { ptep_set_wrprotect ( src_pte ); pte = pte_wrprotect ( pte ); } ... }","title":"Memory Manager"},{"location":"lego/syscall/fork/#duplicate-vm-free-pool","text":"TODO Yutong","title":"Duplicate VM Free Pool"},{"location":"lego/syscall/fork/#processor-manager","text":"Boring implementation details in the processor manager side.","title":"Processor Manager"},{"location":"lego/syscall/fork/#entry-points","text":"fork() vfork() clone() kernel_thread() All of them land on do_fork() , which is Lego\u2019s main fork function.","title":"Entry Points"},{"location":"lego/syscall/fork/#do_fork","text":"There are mainly three parts within do_fork() : 1) copy_process() , which duplicates a new task based on current , including allocate new kernel stack, new task_struct, increase mm reference counter, etc. 2) If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures. 3) wake_up_new_task() , which gives away the newly created task to local scheduler.","title":"do_fork()"},{"location":"lego/syscall/fork/#copy_process","text":"The routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are task_struct , mm_sturct , sighand , and so on. This section only talks about few of them, and leave others to readers who are interested.","title":"copy_process()"},{"location":"lego/syscall/fork/#sanity-checking","text":"Mainly check if clone_flags are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If CLONE_THREAD is passed, then CLONE_SIGHAND , CLONE_VM , and so on must be set as well. /* * Thread groups must share signals as well, and detached threads * can only be started up within the thread group. */ if (( clone_flags & CLONE_THREAD ) && ! ( clone_flags & CLONE_SIGHAND )) return ERR_PTR ( - EINVAL ); /* * Shared signal handlers imply shared VM. By way of the above, * thread groups also imply shared VM. Blocking this case allows * for various simplifications in other code. */ if (( clone_flags & CLONE_SIGHAND ) && ! ( clone_flags & CLONE_VM )) return ERR_PTR ( - EINVAL );","title":"Sanity Checking"},{"location":"lego/syscall/fork/#dup_task_struct","text":"Two main things: 1) duplicate a new task_struct , 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of task_struct depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info. int arch_dup_task_struct ( struct task_struct * dst , struct task_struct * src ) { memcpy ( dst , src , arch_task_struct_size ); return fpu__copy ( & dst -> thread . fpu , & src -> thread . fpu ); } The stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the thread_info to points to this new thread, so the current macro will work. static void setup_thread_stack ( struct task_struct * p , struct task_struct * org ) { /* Duplicate whole stack! */ * task_thread_info ( p ) = * task_thread_info ( org ); /* Make the `current' macro work */ task_thread_info ( p ) -> task = p ; }","title":"dup_task_struct()"},{"location":"lego/syscall/fork/#copy_mm","text":"This is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new mm_struct , and also a new pgd : /* * pgd_alloc() will duplicate the identity kernel mapping * but leaves other entries empty: */ mm -> pgd = pgd_alloc ( mm ); if ( unlikely ( ! mm -> pgd )) { kfree ( mm ); return NULL ; }","title":"copy_mm()"},{"location":"lego/syscall/fork/#duplicate-pcache-data","text":"TODO TODO: hook with pcache We need to duplicate the pcache vm_range array, once Yutong finished the code.","title":"Duplicate pcache data"},{"location":"lego/syscall/fork/#setup_sched_fork","text":"Callback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task\u2019s scheduler class: int setup_sched_fork ( unsigned long clone_flags , struct task_struct * p ) { int cpu = get_cpu (); __sched_fork ( clone_flags , p ); p -> state = TASK_NEW ; ... if ( unlikely ( p -> sched_reset_on_fork )) { if ( task_has_rt_policy ( p )) { p -> policy = SCHED_NORMAL ; p -> static_prio = NICE_TO_PRIO ( 0 ); p -> rt_priority = 0 ; } else if ( PRIO_TO_NICE ( p -> static_prio ) < 0 ) p -> static_prio = NICE_TO_PRIO ( 0 ); p -> prio = p -> normal_prio = __normal_prio ( p ); set_load_weight ( p ); ... } if ( rt_prio ( p -> prio )) p -> sched_class = & rt_sched_class ; else { p -> sched_class = & fair_sched_class ; set_load_weight ( p ); } __set_task_cpu ( p , cpu ); if ( p -> sched_class -> task_fork ) p -> sched_class -> task_fork ( p ); ... }","title":"setup_sched_fork()"},{"location":"lego/syscall/fork/#allocate-new-pid","text":"In both Lego and Linux, we don\u2019t allocate new pid for a new thread, if that thread is an idle thread . So callers of do_fork needs to pass something to let do_fork know. In Linux, they use struct pid, init_struct_pid to check. In Lego, we introduce an new clone_flag CLONE_IDLE_THREAD . If that flag is set, do_fork() will try to allocate a new pid for the new thread. Otherwise, it will be 0: /* clone idle thread, whose pid is 0 */ if ( ! ( clone_flags & CLONE_IDLE_THREAD )) { pid = alloc_pid ( p ); if ( ! pid ) goto out_cleanup_thread ; } So, only the init_idle() function can pass this CLONE_IDLE_THREAD down. All other usages are wrong and should be reported. In order to avoid conflict with Linux clone_flag, we define it as: #define CLONE_IDLE_THREAD 0x100000000","title":"Allocate new pid"},{"location":"lego/syscall/fork/#settidcleartid","text":"These are some futex related stuff. I will cover these stuff in futex document: p -> set_child_tid = ( clone_flags & CLONE_CHILD_SETTID ) ? child_tidptr : NULL ; /* * Clear TID on mm_release()? */ p -> clear_child_tid = ( clone_flags & CLONE_CHILD_CLEARTID ) ? child_tidptr : NULL ; #ifdef CONFIG_FUTEX p -> robust_list = NULL ; #endif","title":"SETTID/CLEARTID"},{"location":"lego/syscall/fork/#copy_thread_tls","text":"This is the most interesting function. Cover later.","title":"copy_thread_tls()"},{"location":"lego/syscall/fork/#p2m_fork","text":"In order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode. /* Tell remote memory component */ #ifdef CONFIG_COMP_PROCESSOR if ( clone_flags & CLONE_GLOBAL_THREAD ) { ... p2m_fork ( p , clone_flags ); ... } #endif The CLONE_GLOBAL_THREAD should only be set, if the following cases happen: fork() vfork() clone(), without CLONE_THREAD being set In order to avoid conflict with Linux clone_flag, we define it as: #define CLONE_GLOBAL_THREAD 0x200000000","title":"p2m_fork()"},{"location":"lego/syscall/fork/#wake_up_new_task","text":"The last step of do_fork is waking up the new thread or process, which is performed by wake_up_new_task() function. The first question this function will ask is: which cpu to land? The answer comes from select_task_rq() : static inline int select_task_rq ( struct task_struct * p , int cpu , int sd_flags , int wake_flags ) { if ( p -> nr_cpus_allowed > 1 ) cpu = p -> sched_class -> select_task_rq ( p , cpu , sd_flags , wake_flags ); else cpu = cpumask_any ( & p -> cpus_allowed ); ... } Clearly, this is determined by cpus_allowed , which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux: A child created via fork(2) inherits its parent's CPU affinity mask. The affinity mask is preserved across an execve(2). After landing CPU is selected, following operation is simple: just enqueue this task into landing CPU\u2019s runqueue, and we are done: void wake_up_new_task ( struct task_struct * p ) { ... /* Select a CPU for new thread to run */ #ifdef CONFIG_SMP /* * Fork balancing, do it here and not earlier because: * - cpus_allowed can change in the fork path * - any previously selected cpu might disappear through hotplug */ set_task_cpu ( p , select_task_rq ( p , task_cpu ( p ), SD_BALANCE_FORK , 0 )); #endif rq = __task_rq_lock ( p ); activate_task ( rq , p , 0 ); p -> on_rq = TASK_ON_RQ_QUEUED ; ... } \u2013 Yizhou Shan Created: Feb 11, 2018 Last Updated: Feb 27, 2018","title":"wake_up_new_task()"},{"location":"lego/syscall/getrusage/","text":"getrusage \u00b6 The syscall getrusage is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting. The syscall is added recently due to wait family syscalls, which use and bookkeep some of rusage . As on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others. \u2013 Yizhou Shan Created: Mar 7, 2018 Last Updated: Mar 7, 2018","title":"getrusage()"},{"location":"lego/syscall/getrusage/#getrusage","text":"The syscall getrusage is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting. The syscall is added recently due to wait family syscalls, which use and bookkeep some of rusage . As on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others. \u2013 Yizhou Shan Created: Mar 7, 2018 Last Updated: Mar 7, 2018","title":"getrusage"},{"location":"lego/syscall/mremap/","text":"mremap() \u00b6","title":"mremap()"},{"location":"lego/syscall/mremap/#mremap","text":"","title":"mremap()"},{"location":"lego/syscall/msync/","text":"msync() \u00b6 The document is a summary I wrote after reading Failure-atomic msync() paper, which help me understand several questions related to msync() . msync() is not atomic. During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure. msync() need concurrency control . This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. At the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages\u2019 PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back. Probably some nice reading. fsync, fdatasync 1 . \u2013 Yizhou Shan Created: Feb 01, 2018 Last Updated: Mar 23, 2018 RFLUSH: Rethink the Flush \u21a9","title":"msync()"},{"location":"lego/syscall/msync/#msync","text":"The document is a summary I wrote after reading Failure-atomic msync() paper, which help me understand several questions related to msync() . msync() is not atomic. During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure. msync() need concurrency control . This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. At the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages\u2019 PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back. Probably some nice reading. fsync, fdatasync 1 . \u2013 Yizhou Shan Created: Feb 01, 2018 Last Updated: Mar 23, 2018 RFLUSH: Rethink the Flush \u21a9","title":"msync()"},{"location":"lego/syscall/wait_and_exit/","text":"wait4(), waitid(), and exit() \u00b6 Lego supports wait4() and waitid() syscalls, and they are compatible with Linux programs. These two syscalls rely on exit_notify() function when a thread exit() . Basically, when a thread exit, it will notify its parent, and reparent 3 its children if necessary. Facts in Lego: Lego does not have process group and session 2 concept. Each process is within its own process group and session. This implies Lego will not have Orphaned Process Group 1 when a process exit. Orphan process 3 is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father\u2019s process. This is performed by function forget_original_parent() . wait, signal, exec, fork are close related. \u2013 Yizhou Shan Created: Mar 8, 2018 Last Updated: Mar 10, 2018 Orphaned Process Groups \u21a9 Process Group \u21a9 Orphan Process \u21a9 \u21a9","title":"wait4 and exit()"},{"location":"lego/syscall/wait_and_exit/#wait4-waitid-and-exit","text":"Lego supports wait4() and waitid() syscalls, and they are compatible with Linux programs. These two syscalls rely on exit_notify() function when a thread exit() . Basically, when a thread exit, it will notify its parent, and reparent 3 its children if necessary. Facts in Lego: Lego does not have process group and session 2 concept. Each process is within its own process group and session. This implies Lego will not have Orphaned Process Group 1 when a process exit. Orphan process 3 is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father\u2019s process. This is performed by function forget_original_parent() . wait, signal, exec, fork are close related. \u2013 Yizhou Shan Created: Mar 8, 2018 Last Updated: Mar 10, 2018 Orphaned Process Groups \u21a9 Process Group \u21a9 Orphan Process \u21a9 \u21a9","title":"wait4(), waitid(), and exit()"},{"location":"misc/cheatsheet/","text":"Cheatsheet \u00b6 Python \u00b6 f'{0x0c180606:032b}' VNC \u00b6 Server side: Start server on certain port with certain geometry: vncserver :66 -geometry 1920x1080 Client side: for safety, use SSH tunnel. -p 22 : ssh port is 22 -L 7777:localhost:5966 : Forward localhost\u2019s 7777 to server\u2019s 5966 Step 1) ssh -p 22 -v -C -L 7777:localhost:5966 root@yourserver.com Step 2) Use VNC client to establish connection with localhost:7777 virsh \u00b6 Pass commands to QEMU in the virsh bash: # qemu-monitor-command guest_os_id --hmp \"info cpus\" Markdown \u00b6 Emoji cheatsheet tmux \u00b6 Install tmux-plugins , it makes your terminal bling bling. bash \u00b6 Show current git branch in PS1: parse_git_branch () { git branch 2 > /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ git:(\\1)/' } PS1 = \"\\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\] $ \" Forward man pages to vim : vman () { man $* | col -b | vim -c 'set ft=man nomod nolist' - ; } alias man = \"vman\" Git \u00b6 git log --pretty=\"%C(Yellow)%h %C(auto)%d (%C(Green)%cr%C(reset))%x09 %C(Cyan)%an: %C(reset)%s\" --date=short --graph QEMU \u00b6 Run standalone kernel: # Create a new directory to store the serial output from printk(). OUTPUT_DIR = \"test-output\" if [ -e $OUTPUT_DIR ] ; then if [ -f $OUTPUT_DIR ] ; then echo \"ERROR: $OUTPUT_DIR is not a directly\" exit 1 fi else mkdir -p $OUTPUT_DIR fi KERNEL = \"arch/x86_64/boot/bzImage\" KERNEL_PARAM = \"console=ttyS0 earlyprintk=serial,ttyS0,115200\" SERIAL = \"-serial file: $OUTPUT_DIR /ttyS0 -serial file: $OUTPUT_DIR /ttyS1\" # -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\ # Above -cpu option may not work with some kernels. qemu-system-x86_64 -s \\ -nographic \\ -kernel $KERNEL -append \" $KERNEL_PARAM \" \\ -no-reboot \\ -d int,cpu_reset -D $OUTPUT_DIR /qemu.log \\ $SERIAL \\ -m 16G \\ -monitor stdio \\ -smp cpus = 24 ,cores = 12 ,threads = 2 ,sockets = 2 \\ -numa node,cpus = 0 -11,mem = 8G,nodeid = 0 \\ -numa node,cpus = 12 -23,mem = 8G,nodeid = 1 Install CentOS on Dell PowerEdge \u00b6 Enable SR-IOV for future usage Press F11 Boot Manager during boot Find Integrated Devices Enable SR-IOV Global Enable Partition /boot : e.g, 50GB swap : e.g, 4G / : all left Don\u2019t forget to enable Network during installation. Change SSH port Disable firewalld systemctl stop firewalld systemctl disable firewalld If SELinux is enabled yum install policycoreutils-python semanage port -a -t ssh_port_t -p tcp #PORTNUMBER Change /etc/ssh/sshd_config systemctl restart sshd Avoid Typing SSH Password \u00b6 Generate keys: ssh-keygen -t rsa Copy to remote: ssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22 GRUB2 on Ubuntu \u00b6 Nothing like grubby?! Shame on you. Step I: cat /boot/grub/grub.cfg | grep menuentry menuentry 'Ubuntu, with Linux 4.16.0' --class ubuntu ... menuentry 'Ubuntu, with Linux 4.9.92' --class ubuntu ... Step II: Open /etc/default/grub , change GRUB_DEFAULT=\u201dAdvanced options for Ubuntu>Ubuntu, with Linux 4.16.0\u201d GRUB_DEFAULT=\u201dAdvanced options for Ubuntu>Ubuntu, with Linux 4.9.92\u201d Step III: sudo update-grub Migrate to Ubuntu From MacOS \u00b6 Disable [Super+p] . This is my tmux prefix somehow. xmodmap to switch Super and CTRL. 1","title":"Cheatsheet"},{"location":"misc/cheatsheet/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"misc/cheatsheet/#python","text":"f'{0x0c180606:032b}'","title":"Python"},{"location":"misc/cheatsheet/#vnc","text":"Server side: Start server on certain port with certain geometry: vncserver :66 -geometry 1920x1080 Client side: for safety, use SSH tunnel. -p 22 : ssh port is 22 -L 7777:localhost:5966 : Forward localhost\u2019s 7777 to server\u2019s 5966 Step 1) ssh -p 22 -v -C -L 7777:localhost:5966 root@yourserver.com Step 2) Use VNC client to establish connection with localhost:7777","title":"VNC"},{"location":"misc/cheatsheet/#virsh","text":"Pass commands to QEMU in the virsh bash: # qemu-monitor-command guest_os_id --hmp \"info cpus\"","title":"virsh"},{"location":"misc/cheatsheet/#markdown","text":"Emoji cheatsheet","title":"Markdown"},{"location":"misc/cheatsheet/#tmux","text":"Install tmux-plugins , it makes your terminal bling bling.","title":"tmux"},{"location":"misc/cheatsheet/#bash","text":"Show current git branch in PS1: parse_git_branch () { git branch 2 > /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ git:(\\1)/' } PS1 = \"\\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\] $ \" Forward man pages to vim : vman () { man $* | col -b | vim -c 'set ft=man nomod nolist' - ; } alias man = \"vman\"","title":"bash"},{"location":"misc/cheatsheet/#git","text":"git log --pretty=\"%C(Yellow)%h %C(auto)%d (%C(Green)%cr%C(reset))%x09 %C(Cyan)%an: %C(reset)%s\" --date=short --graph","title":"Git"},{"location":"misc/cheatsheet/#qemu","text":"Run standalone kernel: # Create a new directory to store the serial output from printk(). OUTPUT_DIR = \"test-output\" if [ -e $OUTPUT_DIR ] ; then if [ -f $OUTPUT_DIR ] ; then echo \"ERROR: $OUTPUT_DIR is not a directly\" exit 1 fi else mkdir -p $OUTPUT_DIR fi KERNEL = \"arch/x86_64/boot/bzImage\" KERNEL_PARAM = \"console=ttyS0 earlyprintk=serial,ttyS0,115200\" SERIAL = \"-serial file: $OUTPUT_DIR /ttyS0 -serial file: $OUTPUT_DIR /ttyS1\" # -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\ # Above -cpu option may not work with some kernels. qemu-system-x86_64 -s \\ -nographic \\ -kernel $KERNEL -append \" $KERNEL_PARAM \" \\ -no-reboot \\ -d int,cpu_reset -D $OUTPUT_DIR /qemu.log \\ $SERIAL \\ -m 16G \\ -monitor stdio \\ -smp cpus = 24 ,cores = 12 ,threads = 2 ,sockets = 2 \\ -numa node,cpus = 0 -11,mem = 8G,nodeid = 0 \\ -numa node,cpus = 12 -23,mem = 8G,nodeid = 1","title":"QEMU"},{"location":"misc/cheatsheet/#install-centos-on-dell-poweredge","text":"Enable SR-IOV for future usage Press F11 Boot Manager during boot Find Integrated Devices Enable SR-IOV Global Enable Partition /boot : e.g, 50GB swap : e.g, 4G / : all left Don\u2019t forget to enable Network during installation. Change SSH port Disable firewalld systemctl stop firewalld systemctl disable firewalld If SELinux is enabled yum install policycoreutils-python semanage port -a -t ssh_port_t -p tcp #PORTNUMBER Change /etc/ssh/sshd_config systemctl restart sshd","title":"Install CentOS on Dell PowerEdge"},{"location":"misc/cheatsheet/#avoid-typing-ssh-password","text":"Generate keys: ssh-keygen -t rsa Copy to remote: ssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22","title":"Avoid Typing SSH Password"},{"location":"misc/cheatsheet/#grub2-on-ubuntu","text":"Nothing like grubby?! Shame on you. Step I: cat /boot/grub/grub.cfg | grep menuentry menuentry 'Ubuntu, with Linux 4.16.0' --class ubuntu ... menuentry 'Ubuntu, with Linux 4.9.92' --class ubuntu ... Step II: Open /etc/default/grub , change GRUB_DEFAULT=\u201dAdvanced options for Ubuntu>Ubuntu, with Linux 4.16.0\u201d GRUB_DEFAULT=\u201dAdvanced options for Ubuntu>Ubuntu, with Linux 4.9.92\u201d Step III: sudo update-grub","title":"GRUB2 on Ubuntu"},{"location":"misc/cheatsheet/#migrate-to-ubuntu-from-macos","text":"Disable [Super+p] . This is my tmux prefix somehow. xmodmap to switch Super and CTRL. 1","title":"Migrate to Ubuntu From MacOS"},{"location":"misc/essential/","text":"System Developing Essentials \u00b6 Misc Advice \u00b6 Elevator Pitches, John Wilkes Creating an effective poster, John Wilkes How to Get a Paper Accepted at OOPSLA Tools \u00b6 Stack and Register Dumper NMI and software Watchdog Tracepoint and Ring Buffer Profilers Counters Whiskey and Luck Keep in mind \u00b6 Stress your system Every single critical subsystem Confident with your base subsystem Fix bug/Improve perf at early stage Plan ahead Single thread, or thread pool? How to avoid using lock ? What lock to use? How to reduce lock contention ? Does this data structure need reference counter ? Should I use per-cpu data structures? Should I pad this lock $-line aligned to avoid pingpong? Decent Cleanup I\u2019m fucking hate a crap kernel module just kill my machine, either stuck or bug. Free buffer/structure Remove the pointer from friends\u2019 list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization. Report error. Do not be SILENT. Clever Buffer Management kmem_cache? static pre-allocated array? Ring buffer? Other than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up! System Building Advice \u00b6 John Ousterhout If you don\u2019t know what the problem was, you haven\u2019t fixed it If it hasn\u2019t been used, it doesn\u2019t work","title":"Essential"},{"location":"misc/essential/#system-developing-essentials","text":"","title":"System Developing Essentials"},{"location":"misc/essential/#misc-advice","text":"Elevator Pitches, John Wilkes Creating an effective poster, John Wilkes How to Get a Paper Accepted at OOPSLA","title":"Misc Advice"},{"location":"misc/essential/#tools","text":"Stack and Register Dumper NMI and software Watchdog Tracepoint and Ring Buffer Profilers Counters Whiskey and Luck","title":"Tools"},{"location":"misc/essential/#keep-in-mind","text":"Stress your system Every single critical subsystem Confident with your base subsystem Fix bug/Improve perf at early stage Plan ahead Single thread, or thread pool? How to avoid using lock ? What lock to use? How to reduce lock contention ? Does this data structure need reference counter ? Should I use per-cpu data structures? Should I pad this lock $-line aligned to avoid pingpong? Decent Cleanup I\u2019m fucking hate a crap kernel module just kill my machine, either stuck or bug. Free buffer/structure Remove the pointer from friends\u2019 list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization. Report error. Do not be SILENT. Clever Buffer Management kmem_cache? static pre-allocated array? Ring buffer? Other than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up!","title":"Keep in mind"},{"location":"misc/essential/#system-building-advice","text":"John Ousterhout If you don\u2019t know what the problem was, you haven\u2019t fixed it If it hasn\u2019t been used, it doesn\u2019t work","title":"System Building Advice"},{"location":"misc/jasmine/","text":"Cool and good-to-know stuff: GigaIO RSS (Receiver Side Scaling) Intel Flow Director E2, ClickOS, NetVM, and Metron. AutoML What is \u2018Site Reliability Engineering\u2019? Deduplication: Content Based Page Sharing (CBPS) + COW Linux\u2019s Kernel Same-page Merging (KSM) Multi-Queue SSD, MQ-SSD 2 Write Anywhere File Layout, WAFL . NetApp Paper 1 . Journaling Shadow Paging Soft Updates Shared-nothing architecture -> no single point of contention ZeptoOS Tail-tolerant Distributed Systems S.M.A.R.T (Self-Monitoring, Analysis, and Reporting Technology) linux-mm . Actually, not that interesting. Serveless AWS Lambda Google Cloud Function Azure Functions SDS TidalScale ScaleMP Apache Crail Redis Lab Journaling of journal Concurrent Data Structures NUMA-aware data structures linearizability lock-free skip list blog WAFL Iron: Repairing Live Enterprise File Systems, FAST\u201818 \u21a9 Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems, SYSTOR\u201813 \u21a9","title":"Jasmine"},{"location":"misc/talk/","text":"DASH dynamic ad over HTTP (video) rate adaption algorithm: estimate net bw? Multipath TCP https://www.multipath-tcp.org/ cost/byte, perf/$","title":"Talk"},{"location":"notes/benchmark/","text":"Benchmarks \u00b6 Version History Date Description Apr 2, 2020 Update Aug 13, 2019 Update Aug 03, 2019 Initial draft Areas \u00b6 Synchronization/Concurrency Community \u00b6 Phoenix HPCA (heavy mmap/munmap, i.e., mm-sem usage) MOSBENCH/Metis (same as Phoenix) LevelDB (a popular workload) Linux locktorture Filesystems (fs) LiTL, ATC\u201816, https://github.com/multicore-locks/litl References ShuffleLock, SOSP\u201819 Compact NUMA-aware Locks, EuroSys\u201819 fill me in OS \u00b6 will-it-scale lmbench sysbench FPGA \u00b6 Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\u201818 AmophOS has a lot more. Misc Information \u00b6 Systems Benchmarking Crimes","title":"Benchmarks"},{"location":"notes/benchmark/#benchmarks","text":"Version History Date Description Apr 2, 2020 Update Aug 13, 2019 Update Aug 03, 2019 Initial draft","title":"Benchmarks"},{"location":"notes/benchmark/#areas","text":"","title":"Areas"},{"location":"notes/benchmark/#synchronizationconcurrency-community","text":"Phoenix HPCA (heavy mmap/munmap, i.e., mm-sem usage) MOSBENCH/Metis (same as Phoenix) LevelDB (a popular workload) Linux locktorture Filesystems (fs) LiTL, ATC\u201816, https://github.com/multicore-locks/litl References ShuffleLock, SOSP\u201819 Compact NUMA-aware Locks, EuroSys\u201819 fill me in","title":"Synchronization/Concurrency Community"},{"location":"notes/benchmark/#os","text":"will-it-scale lmbench sysbench","title":"OS"},{"location":"notes/benchmark/#fpga","text":"Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\u201818 AmophOS has a lot more.","title":"FPGA"},{"location":"notes/benchmark/#misc-information","text":"Systems Benchmarking Crimes","title":"Misc Information"},{"location":"notes/cache_coherence/","text":"Practical Cache Coherence \u00b6 Version History Date Description Feb 24, 2020 Kobe and Gigi. Add Intel CCIP. Oct 3, 2019 Add FPGA related discussion Jun 28, 2019 Initial draft Practical Cache Coherence Summary and Thoughs Readings Misc Facts Case Study Intel AMD ARM OpenCAPI and CCIX OpenPiton FPGA Formal Verification A general collection of resources on cache coherence. I started this when I was having a hard time optimizing lock delegation. This note is not about acadamic new ideas, but rather for a concrete understanding of current cache coherence implementations. Summary and Thoughs \u00b6 The textbooks tough us the basic concept of MESI. And realizations like snoop and directory. But what usually missing is the implementation details when it comes to: 1) conflicts, 2) no single shared bus. Modern processors have Network-on-Chip (NoC). Cores, cache slices, and memory controllers are connected via an on-chip network. The model is no different from a datacenter cluster connected by real network. Cache requests generated by MESI protocols should appear atomic to cores. Given the distributed nature of all resources, those cache requests will have to be implemented like distributed transactions . For example, the MESIF is the cache coherence protocol used by Intel. When a read is made to an invalid line, the corresponding cache will perform a cache read transaction to read the data from either other caches or memory. This transaction consists multiple steps such as: send requests, collect responses, send ACKs. Those transactions will conflict if multiple reads and writes happen at the same time. Someone has to resolve it. It can be resolved by different cache controllers, or by a single serialization point like home agent. Just like you can have many ways to implement transactions for distributed systems, there are also many ways to do cache coherence transactions. And there are many. Atomic Read-Modify-Write (RMW) instructions will make cache coherence implementations even more complex. Those instructions include read-and-inc , test-and-set , and lock; -prefixed. I think, there will some \u201clock the bus\u201d, or \u201clocked state\u201d at the home agent per cache line. Having atomic RMW instructions will add more complexity to the overall transaction design. While reading Intel related cache coherence diagrams/transactions, you might find many different descriptions. Don\u2019t panic. They are just different implementations proposed by Intel. Different implementations will have different trade-offs and performance, you can check Frank\u2019s post for more details. Directory-based cache coherence protocol and implementation will be the future for multicore machines. Because it incurs much less coherence traffic than snoop-based ones, thus more scalable. The trend is confirmed by recent Intel UPI directory-based approach. Related readings: [1]: Why On-Chip Cache Coherence Is Here to Stay [2]: QPI 1.1 Invovled [3]: Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \u201899 [4]: Paper: Using Destination-Set Prediction to Improve the Latency/Bandwidth Tradeoff in Shared-Memory Multiprocessors, ISCA\u201803 [5]: The trade-off: Left questions: - Do cache coherence implementations ensure fairness among cores? Readings \u00b6 The Architecture of the Nehalem Processor and Nehalem-EP SMP Platforms , chapter 5.2 Cache-Coherence Protocol for Multi-Processors. Intel: Performance Analysis Guide for Intel\u00ae Core\u2122 i7 Processor and Intel\u00ae Xeon\u2122 5500 processors Dr.Bandwidth on Core2Core cache coherence flows when running producer-consumer type of workload. . 100% recommended. Blog: NUMA Deep Dive Part 3: Cache Coherency The BEST blog I\u2019ve seen on the topic of Intel snoop models . Intel is using MESIF cache coherence protocol, but it has multiple cache coherence implementations. The first one is Source Snoop (or Early Snoop ), which is more like a traditional snoop-based cache coherence implementation. Upon miss, the caching agent will broadcast to other agents. The second one is Home Snoop , which is more like a directory-based cache coherence implementation. Upon miss, the caching agent will contact home agent, and then the home agent will send requests to other caching agents who have the requested cache line. There are other implementations like Cluster-on-Die. Intel UPI gets rid of all this complexity, it is only using directory-based, in the hope to reduce cache coherence traffic, which make sense. Related: Broadwell EP Snoop Models Related: Skylay UPI Paper: MESIF: A Two-Hop Cache Coherency Protocol for Point-to-Point Interconnects (2009)__ A MUST read. This paper has the most extensive description of the MESIF protocol implementation. It has many timing diagrams than describe how cache requests actually proceed. Those diagrams can help us understand what is needed to finish a cache request. Their slides has more timing diagrams. But do note: the implementation described by this paper is different from what Intel QPI has in products. The difference is discussed at chapter 4. MESIF and QPI, namely, other caching agents will send responses to Home agent rather than to requesting agent. QPI relies on Home agent to solve conflict. Also note: this is just one of the possible implementations to realize MESIF protocol. There could be many other ways, e.g., QPI source snooping, QPI home snooping. But all of them share the essential and general concepts and ideas. Appendix I: Large-Scale Multiprocessors and Scientific Applications , chapter 7 Implementing Cache Coherence. This is probably some most insightful discussion about real implementation of cache coherence. With the distributed nature and Network-on-Chip, implementing cache coherence in modern processors is no different than implementing a distributed transaction protocol. Cache activities like read miss or write miss have multi-step operations, but they need to appear as \u201catomic\u201d to users. Put in another way, misses are like transactions, they have multiple steps but they must be atomic. They can be retried. Having directory for cache coherence will make implementation easier. Because the place (e.g., L3) where directory resides can serve as the serialization point. They can solve write races. Home directory controller and cache controller will exchange messages like a set of distributed machines. In fact, with NoC, they are actually distributed system. Intel: An Introduction to the Intel\u00ae QuickPath Interconnect , page 15 MESIF. HotChips slide , has timing diagrams. It explains the Home Snoop and Source Snoop used by Intel. Based on their explanation, it seems both Home Snoop and Source Snoop are using a combination of snoop and directory. The Processor#4 (pg 17 and 18) maintains the directory. And this is a perfect demonstration of the details described in Appendix I: Large-Scale Multiprocessors and Scientific Applications . Related patent: Extending a cache coherency snoop broadcast protocol with directory information Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \u201899 A hybrid snoop and directory cache coherence implementation. The insight is snoop cause too much bandwidth, directory incurs longer latency. So this paper proposed Multicast snoop , where it multicasts coherence transactions to selected processors, lowering the address bandwidth required for snooping. Paper: Why On-Chip Cache Coherence Is Here to Stay, Communications of ACM\u201802 This paper discusses why cache coherence can scale. A nice read. R1: Coherence\u2019s interconnection network traffic per miss scales when precisely tracking sharers. (Okay increased directory bits, what about those storage cost? See R2). R2: Hierarchy combined with inclusion enables efficient scaling of the storage cost for exact encoding of sharers. R3: private evictions should send explicit messages to shared cache to enable precise tracking. Thus the recall ( back invalidation ) traffic can be reduced when shared cache is evicting (assume inclusion cache). R4: Latencies of cache request can be amortized. Book: Parallel Computer Organization and Design , Chapter 7. Links coherence and consistency together. This chapter uses detailed graphs to show how different cache coherence implementations affect consistency. Book: A Primer on Memory Consistency and Cache Coherence Best book for this topic. Dr.Bandwidth on explaining core-to-core communication transactions! Seriously, it\u2019s so good! Although, I just feel there are so many unpublished details about the exact coherence transactions. Dr.Bandwidth himself used a lot \u201cmaybe\u201d, and listed possible actions. Transactional Memory Coherence and Consistency, ISCA\u201804 Programming with Transactional Coherence and Consistency (TCC) Slide1 Awarded the most influential paper at ISCA 2019. I took a read today (Jul 21, 2019). I feels like it\u2019s using the \u201cbatch\u201d optimization for all time. The TCC design, kind of combines both cache coherence and memory consistency: how transactions commit or orders, determins the coherence and consistency. It seems the load/store speculative execution used in their context is so similar to what Dr.Bandwidth said about Intel\u2019s implementation. Basically, the processor might read some data from L1/L2 and continue execution, but there is a chance, that the data is modifed by others, and the L3 caching agent or home agent could decide to revoke it. Once receiving such revoke message, the processor must cancel all executions that use the speculatively read data. It mentions couple Thread-Level Speculation papers, I think they should on this topic. Misc Facts \u00b6 Intel Caching Agent (Cbox) is per core (or per LLC slice). Intel Home Agent is per memory controller. \u201cThe LLC coherence engine (CBo) manages the interface between the core and the last level cache (LLC). All core transactions that access the LLC are directed from the core to a CBo via the ring interconnect. The CBo is responsible for managing data delivery from the LLC to the requesting core. It is also responsible for maintaining coherence between the cores within the socket that share the LLC; generating snoops and collecting snoop responses from the local cores when the MESIF protocol requires it.\u201d \u201cEvery physical memory address in the system is uniquely associated with a single Cbox instance via a proprietary hashing algorithm that is designed to keep the distribution of traffic across the CBox instances relatively uniform for a wide range of possible address patterns.\u201d Read more here, chapter 2.3 . Starting from Intel UPI, Caching Agent and Home Agent are combined as CHA. A good discussion about why QPI gradually drop Source Snoop and solely use Home Snoop . The motivation is scalability. It turns out the new UPI only supports directory-based protocol. This makes sense because 1) inter socket bandwidth is precious, 2) snoop will consume a lot bandwidth. Intel UPI is using directory-based home snoop coherency protocol Intel\u00ae Xeon\u00ae Processor Scalable Family Technical Overview To provide sufficient bandwidth, shared caches are typically interleaved by addresses with banks physically distributed across the chip. Case Study \u00b6 Intel \u00b6 Intel does not disclose too much details about their cache coherence implementations. The most valuable information is extracted from uncore PMU manuals, and discussions from Dr. Bandwidth. According to Dr. Bandwidth, the Intel CPU could dynamically adapt its coherence strategy during runtime according to workload. There won\u2019t be one fixed cache coherence implementation, there will be many. It depends on workload which one is used at runtime. List below might not be completely true. Just my understanding. Physical addresses are uniquely hashed into L3 slices. That means each individual physical address belongs to a L3 slice, and also belongs to a home agent. Upon L2 miss, it will send requests to corresponding L3 slice. If the L3 slice is in the local socket, the request can be delievered within the same socket. If the L3 slice belongs to another remote socket, the L2 miss request will be sent over QPI/UPI. Also note that the L2 controller will not send snoop requests. (This is answering the question of \u201cwhy using local memory is faster than remote\u201d from the cache coherence perspective.) At L3, when received the request from a L2, If it\u2019s in source snoop model, it will send snoop messages to other sockets. If it\u2019s in home snoop model, it will send read message to other sockets. The another socket will generate snoop and collect responses. (R3QPI or home?) Quote Dr. Bandwidth: Maintaining consistency is easier if the data is sent to the L3 first, and then to the requesting core, but it is also possible to send to both at the same time (e.g., \u201cDirect2Core\u201d). In recent processors, these return paths are chosen dynamically based on undocumented states and settings of the processor. I\u2019m not sure who will ACK L2 at last. L3 or home agent? Both are possible. I think both L3 and home agent have directory information. They know where to send snoop/read messages. And both of them can serialize coherence transactions! It\u2019s just undocumented who is doing what. In generall, we need to bear the fact that we cannot just figure out how Intel cache coherence works underlying. We maybe just need to \u201cvaguely\u201d know the fact that: Both directory and snoop will be used in combination. L3/home agent will serialize conflicting transactions L3/home agent will send data to requesting core L3/home agent will send final ACK to requesting L2 A coherence transaction is a multi-step distributed transaction. It involes sending requests, serialize conflicts, receiving responses/ACKs. When in doubt, read the discussion posted by Dr. Bandwidth, especially this one on detailed cache coherence flows on a procuder-consumer case , which is essential if you are trying to implement high-performance spinlocks and concurrency data structures. AMD \u00b6 AMD HyperTransport Assit for Cache Coherence Slide Slide ARM \u00b6 AMBA CHI Specifications This is probabaly the most comprehensive document I\u2019ve ever seen about cache coherence. Although terms used by ARM differs from the ones used by Intel, still, you can map them. Chapter 5 Interconnect Protocol Flows has a lot timing diagrams regarding read/write/atomic coherence transactions. It\u2019s a good reference to know, but it would be hard to actually understand the details. OpenCAPI and CCIX \u00b6 CCIX White Paper OpenCAPI OpenPiton \u00b6 OpenPiton Microarchitecture Specification Directory-based MESI This spec has detailed coherence message packet format and type. Unfortunately, it does not say anything about how they deal with coherence transaction conflicts. E.g., some timeline diagrams like Figrue \u2154 in this paper . BYOC: A \u201cBring Your Own Core\u201d Framework for Heterogeneous-ISA Research, ASPLOS\u201820 FPGA \u00b6 Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device, FPL\u201819 LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\u201814. This work is built on their earlier work, which basically add the data caching concept to FPGA: using BRAM as L1, on-board DRAM as L2, host or remote DRAM as L3. In their earlier work, each FPGA application (or bitstream) has a private L1 cache. In this work, the add MESI coherence to these private L1 caches, as in they can make multiple L1 cache cache-coherent. The techniques and protocols from this paper are similar to the exisiting ones. For example, 1) they use a global serializing point to serialize transactions, 2) they designed a lot messaging types such as INV, RESP and so on. VMware Research Project PBerry A very interesting and promising project. Intel FPGA PAC Intel itself is building a FPGA-CPU cache coherent setting. They use the Intel UPI interconnect to natually the spectrum. The FPGA shell has some modules to handle this. Intel FPGA CCIP Maybe the ASPLOS\u201820 Optimus paper is uing CCIP-related research platform? Also some pointer chasing related stuff A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\u201816 Formal Verification \u00b6 TODO FIll me in.","title":"Practical-Cache-Coherence"},{"location":"notes/cache_coherence/#practical-cache-coherence","text":"Version History Date Description Feb 24, 2020 Kobe and Gigi. Add Intel CCIP. Oct 3, 2019 Add FPGA related discussion Jun 28, 2019 Initial draft Practical Cache Coherence Summary and Thoughs Readings Misc Facts Case Study Intel AMD ARM OpenCAPI and CCIX OpenPiton FPGA Formal Verification A general collection of resources on cache coherence. I started this when I was having a hard time optimizing lock delegation. This note is not about acadamic new ideas, but rather for a concrete understanding of current cache coherence implementations.","title":"Practical Cache Coherence"},{"location":"notes/cache_coherence/#summary-and-thoughs","text":"The textbooks tough us the basic concept of MESI. And realizations like snoop and directory. But what usually missing is the implementation details when it comes to: 1) conflicts, 2) no single shared bus. Modern processors have Network-on-Chip (NoC). Cores, cache slices, and memory controllers are connected via an on-chip network. The model is no different from a datacenter cluster connected by real network. Cache requests generated by MESI protocols should appear atomic to cores. Given the distributed nature of all resources, those cache requests will have to be implemented like distributed transactions . For example, the MESIF is the cache coherence protocol used by Intel. When a read is made to an invalid line, the corresponding cache will perform a cache read transaction to read the data from either other caches or memory. This transaction consists multiple steps such as: send requests, collect responses, send ACKs. Those transactions will conflict if multiple reads and writes happen at the same time. Someone has to resolve it. It can be resolved by different cache controllers, or by a single serialization point like home agent. Just like you can have many ways to implement transactions for distributed systems, there are also many ways to do cache coherence transactions. And there are many. Atomic Read-Modify-Write (RMW) instructions will make cache coherence implementations even more complex. Those instructions include read-and-inc , test-and-set , and lock; -prefixed. I think, there will some \u201clock the bus\u201d, or \u201clocked state\u201d at the home agent per cache line. Having atomic RMW instructions will add more complexity to the overall transaction design. While reading Intel related cache coherence diagrams/transactions, you might find many different descriptions. Don\u2019t panic. They are just different implementations proposed by Intel. Different implementations will have different trade-offs and performance, you can check Frank\u2019s post for more details. Directory-based cache coherence protocol and implementation will be the future for multicore machines. Because it incurs much less coherence traffic than snoop-based ones, thus more scalable. The trend is confirmed by recent Intel UPI directory-based approach. Related readings: [1]: Why On-Chip Cache Coherence Is Here to Stay [2]: QPI 1.1 Invovled [3]: Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \u201899 [4]: Paper: Using Destination-Set Prediction to Improve the Latency/Bandwidth Tradeoff in Shared-Memory Multiprocessors, ISCA\u201803 [5]: The trade-off: Left questions: - Do cache coherence implementations ensure fairness among cores?","title":"Summary and Thoughs"},{"location":"notes/cache_coherence/#readings","text":"The Architecture of the Nehalem Processor and Nehalem-EP SMP Platforms , chapter 5.2 Cache-Coherence Protocol for Multi-Processors. Intel: Performance Analysis Guide for Intel\u00ae Core\u2122 i7 Processor and Intel\u00ae Xeon\u2122 5500 processors Dr.Bandwidth on Core2Core cache coherence flows when running producer-consumer type of workload. . 100% recommended. Blog: NUMA Deep Dive Part 3: Cache Coherency The BEST blog I\u2019ve seen on the topic of Intel snoop models . Intel is using MESIF cache coherence protocol, but it has multiple cache coherence implementations. The first one is Source Snoop (or Early Snoop ), which is more like a traditional snoop-based cache coherence implementation. Upon miss, the caching agent will broadcast to other agents. The second one is Home Snoop , which is more like a directory-based cache coherence implementation. Upon miss, the caching agent will contact home agent, and then the home agent will send requests to other caching agents who have the requested cache line. There are other implementations like Cluster-on-Die. Intel UPI gets rid of all this complexity, it is only using directory-based, in the hope to reduce cache coherence traffic, which make sense. Related: Broadwell EP Snoop Models Related: Skylay UPI Paper: MESIF: A Two-Hop Cache Coherency Protocol for Point-to-Point Interconnects (2009)__ A MUST read. This paper has the most extensive description of the MESIF protocol implementation. It has many timing diagrams than describe how cache requests actually proceed. Those diagrams can help us understand what is needed to finish a cache request. Their slides has more timing diagrams. But do note: the implementation described by this paper is different from what Intel QPI has in products. The difference is discussed at chapter 4. MESIF and QPI, namely, other caching agents will send responses to Home agent rather than to requesting agent. QPI relies on Home agent to solve conflict. Also note: this is just one of the possible implementations to realize MESIF protocol. There could be many other ways, e.g., QPI source snooping, QPI home snooping. But all of them share the essential and general concepts and ideas. Appendix I: Large-Scale Multiprocessors and Scientific Applications , chapter 7 Implementing Cache Coherence. This is probably some most insightful discussion about real implementation of cache coherence. With the distributed nature and Network-on-Chip, implementing cache coherence in modern processors is no different than implementing a distributed transaction protocol. Cache activities like read miss or write miss have multi-step operations, but they need to appear as \u201catomic\u201d to users. Put in another way, misses are like transactions, they have multiple steps but they must be atomic. They can be retried. Having directory for cache coherence will make implementation easier. Because the place (e.g., L3) where directory resides can serve as the serialization point. They can solve write races. Home directory controller and cache controller will exchange messages like a set of distributed machines. In fact, with NoC, they are actually distributed system. Intel: An Introduction to the Intel\u00ae QuickPath Interconnect , page 15 MESIF. HotChips slide , has timing diagrams. It explains the Home Snoop and Source Snoop used by Intel. Based on their explanation, it seems both Home Snoop and Source Snoop are using a combination of snoop and directory. The Processor#4 (pg 17 and 18) maintains the directory. And this is a perfect demonstration of the details described in Appendix I: Large-Scale Multiprocessors and Scientific Applications . Related patent: Extending a cache coherency snoop broadcast protocol with directory information Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \u201899 A hybrid snoop and directory cache coherence implementation. The insight is snoop cause too much bandwidth, directory incurs longer latency. So this paper proposed Multicast snoop , where it multicasts coherence transactions to selected processors, lowering the address bandwidth required for snooping. Paper: Why On-Chip Cache Coherence Is Here to Stay, Communications of ACM\u201802 This paper discusses why cache coherence can scale. A nice read. R1: Coherence\u2019s interconnection network traffic per miss scales when precisely tracking sharers. (Okay increased directory bits, what about those storage cost? See R2). R2: Hierarchy combined with inclusion enables efficient scaling of the storage cost for exact encoding of sharers. R3: private evictions should send explicit messages to shared cache to enable precise tracking. Thus the recall ( back invalidation ) traffic can be reduced when shared cache is evicting (assume inclusion cache). R4: Latencies of cache request can be amortized. Book: Parallel Computer Organization and Design , Chapter 7. Links coherence and consistency together. This chapter uses detailed graphs to show how different cache coherence implementations affect consistency. Book: A Primer on Memory Consistency and Cache Coherence Best book for this topic. Dr.Bandwidth on explaining core-to-core communication transactions! Seriously, it\u2019s so good! Although, I just feel there are so many unpublished details about the exact coherence transactions. Dr.Bandwidth himself used a lot \u201cmaybe\u201d, and listed possible actions. Transactional Memory Coherence and Consistency, ISCA\u201804 Programming with Transactional Coherence and Consistency (TCC) Slide1 Awarded the most influential paper at ISCA 2019. I took a read today (Jul 21, 2019). I feels like it\u2019s using the \u201cbatch\u201d optimization for all time. The TCC design, kind of combines both cache coherence and memory consistency: how transactions commit or orders, determins the coherence and consistency. It seems the load/store speculative execution used in their context is so similar to what Dr.Bandwidth said about Intel\u2019s implementation. Basically, the processor might read some data from L1/L2 and continue execution, but there is a chance, that the data is modifed by others, and the L3 caching agent or home agent could decide to revoke it. Once receiving such revoke message, the processor must cancel all executions that use the speculatively read data. It mentions couple Thread-Level Speculation papers, I think they should on this topic.","title":"Readings"},{"location":"notes/cache_coherence/#misc-facts","text":"Intel Caching Agent (Cbox) is per core (or per LLC slice). Intel Home Agent is per memory controller. \u201cThe LLC coherence engine (CBo) manages the interface between the core and the last level cache (LLC). All core transactions that access the LLC are directed from the core to a CBo via the ring interconnect. The CBo is responsible for managing data delivery from the LLC to the requesting core. It is also responsible for maintaining coherence between the cores within the socket that share the LLC; generating snoops and collecting snoop responses from the local cores when the MESIF protocol requires it.\u201d \u201cEvery physical memory address in the system is uniquely associated with a single Cbox instance via a proprietary hashing algorithm that is designed to keep the distribution of traffic across the CBox instances relatively uniform for a wide range of possible address patterns.\u201d Read more here, chapter 2.3 . Starting from Intel UPI, Caching Agent and Home Agent are combined as CHA. A good discussion about why QPI gradually drop Source Snoop and solely use Home Snoop . The motivation is scalability. It turns out the new UPI only supports directory-based protocol. This makes sense because 1) inter socket bandwidth is precious, 2) snoop will consume a lot bandwidth. Intel UPI is using directory-based home snoop coherency protocol Intel\u00ae Xeon\u00ae Processor Scalable Family Technical Overview To provide sufficient bandwidth, shared caches are typically interleaved by addresses with banks physically distributed across the chip.","title":"Misc Facts"},{"location":"notes/cache_coherence/#case-study","text":"","title":"Case Study"},{"location":"notes/cache_coherence/#intel","text":"Intel does not disclose too much details about their cache coherence implementations. The most valuable information is extracted from uncore PMU manuals, and discussions from Dr. Bandwidth. According to Dr. Bandwidth, the Intel CPU could dynamically adapt its coherence strategy during runtime according to workload. There won\u2019t be one fixed cache coherence implementation, there will be many. It depends on workload which one is used at runtime. List below might not be completely true. Just my understanding. Physical addresses are uniquely hashed into L3 slices. That means each individual physical address belongs to a L3 slice, and also belongs to a home agent. Upon L2 miss, it will send requests to corresponding L3 slice. If the L3 slice is in the local socket, the request can be delievered within the same socket. If the L3 slice belongs to another remote socket, the L2 miss request will be sent over QPI/UPI. Also note that the L2 controller will not send snoop requests. (This is answering the question of \u201cwhy using local memory is faster than remote\u201d from the cache coherence perspective.) At L3, when received the request from a L2, If it\u2019s in source snoop model, it will send snoop messages to other sockets. If it\u2019s in home snoop model, it will send read message to other sockets. The another socket will generate snoop and collect responses. (R3QPI or home?) Quote Dr. Bandwidth: Maintaining consistency is easier if the data is sent to the L3 first, and then to the requesting core, but it is also possible to send to both at the same time (e.g., \u201cDirect2Core\u201d). In recent processors, these return paths are chosen dynamically based on undocumented states and settings of the processor. I\u2019m not sure who will ACK L2 at last. L3 or home agent? Both are possible. I think both L3 and home agent have directory information. They know where to send snoop/read messages. And both of them can serialize coherence transactions! It\u2019s just undocumented who is doing what. In generall, we need to bear the fact that we cannot just figure out how Intel cache coherence works underlying. We maybe just need to \u201cvaguely\u201d know the fact that: Both directory and snoop will be used in combination. L3/home agent will serialize conflicting transactions L3/home agent will send data to requesting core L3/home agent will send final ACK to requesting L2 A coherence transaction is a multi-step distributed transaction. It involes sending requests, serialize conflicts, receiving responses/ACKs. When in doubt, read the discussion posted by Dr. Bandwidth, especially this one on detailed cache coherence flows on a procuder-consumer case , which is essential if you are trying to implement high-performance spinlocks and concurrency data structures.","title":"Intel"},{"location":"notes/cache_coherence/#amd","text":"AMD HyperTransport Assit for Cache Coherence Slide Slide","title":"AMD"},{"location":"notes/cache_coherence/#arm","text":"AMBA CHI Specifications This is probabaly the most comprehensive document I\u2019ve ever seen about cache coherence. Although terms used by ARM differs from the ones used by Intel, still, you can map them. Chapter 5 Interconnect Protocol Flows has a lot timing diagrams regarding read/write/atomic coherence transactions. It\u2019s a good reference to know, but it would be hard to actually understand the details.","title":"ARM"},{"location":"notes/cache_coherence/#opencapi-and-ccix","text":"CCIX White Paper OpenCAPI","title":"OpenCAPI and CCIX"},{"location":"notes/cache_coherence/#openpiton","text":"OpenPiton Microarchitecture Specification Directory-based MESI This spec has detailed coherence message packet format and type. Unfortunately, it does not say anything about how they deal with coherence transaction conflicts. E.g., some timeline diagrams like Figrue \u2154 in this paper . BYOC: A \u201cBring Your Own Core\u201d Framework for Heterogeneous-ISA Research, ASPLOS\u201820","title":"OpenPiton"},{"location":"notes/cache_coherence/#fpga","text":"Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device, FPL\u201819 LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\u201814. This work is built on their earlier work, which basically add the data caching concept to FPGA: using BRAM as L1, on-board DRAM as L2, host or remote DRAM as L3. In their earlier work, each FPGA application (or bitstream) has a private L1 cache. In this work, the add MESI coherence to these private L1 caches, as in they can make multiple L1 cache cache-coherent. The techniques and protocols from this paper are similar to the exisiting ones. For example, 1) they use a global serializing point to serialize transactions, 2) they designed a lot messaging types such as INV, RESP and so on. VMware Research Project PBerry A very interesting and promising project. Intel FPGA PAC Intel itself is building a FPGA-CPU cache coherent setting. They use the Intel UPI interconnect to natually the spectrum. The FPGA shell has some modules to handle this. Intel FPGA CCIP Maybe the ASPLOS\u201820 Optimus paper is uing CCIP-related research platform? Also some pointer chasing related stuff A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\u201816","title":"FPGA"},{"location":"notes/cache_coherence/#formal-verification","text":"TODO FIll me in.","title":"Formal Verification"},{"location":"notes/cgroup-swap/","text":"How cgroup Trigger SwAp \u00b6 Notes on how cgroup mm triggers swap on a user-defined limit_in_bytes . This notes assume you have adequate knowledge on overall linux mm code. For more information about cgroup in general, please check the document from RedHat . There are several cgroup callbacks at mm/memory.c . Those functions are called to check if cgroup can honor this page allocation. All of these functions are located in mm/memcontrol.c mem_cgroup_try_charge() mem_cgroup_commit_charge() mem_cgroup_cancel_charge() Some facts about the implementation (up to linux 5.2) Each memory cgroup has its own LRU list vector All memory cgroup\u2019s LRU lists and even the global LRU lists, they share a global LRU lock on a per-node basis. (Weird! Why?). Take a closer look of mem_cgroup_try_charge() , whose behavior is actually quite similar to the case of a real OOM: check if we still available memory (here means memory usage is smaller than limit_in_bytes ), if unfortunately we run out of memory, it will then try to reclaim form the memory cgroup\u2019s LRU lists. If that did not work either, final step would be do OOM actions. mem_cgroup_try_charge() try_charge() page_counter_try_charge(): Check if we hit limit_in_bytes counter. Hierarchically charge pages, costly. try_to_free_mem_cgroup_pages() Callback to mm/vmscan.c to shrink the list ( Bingo! ) Also, reclaimer will establish swap pte entries mem_cgroup_oom() mem_cgroup_lruvec() Other than the global zone-wide LRU lists vector, each cgroup has its own LRU lists vector. Choose the vector that will be passed down to do shrink_page_list() etc. LRU Lists Maintainence \u00b6 Insertion to LRU lists is performed as follows: first, it will be inserted into a per-cpu array ( lru_add_pvec ). Once the array is full (default 15 entries), it will do a batch insertion into proper LRU lists (depends on mem_cgroup_lruvec we mentioned above). Why Linux is doing this way? To scale. \u2013 Yizhou Shan Created: Dec 3, 2018 Last Updated: Jul 30, 2019","title":"Linux-Cgroup-and-Swap"},{"location":"notes/cgroup-swap/#how-cgroup-trigger-swap","text":"Notes on how cgroup mm triggers swap on a user-defined limit_in_bytes . This notes assume you have adequate knowledge on overall linux mm code. For more information about cgroup in general, please check the document from RedHat . There are several cgroup callbacks at mm/memory.c . Those functions are called to check if cgroup can honor this page allocation. All of these functions are located in mm/memcontrol.c mem_cgroup_try_charge() mem_cgroup_commit_charge() mem_cgroup_cancel_charge() Some facts about the implementation (up to linux 5.2) Each memory cgroup has its own LRU list vector All memory cgroup\u2019s LRU lists and even the global LRU lists, they share a global LRU lock on a per-node basis. (Weird! Why?). Take a closer look of mem_cgroup_try_charge() , whose behavior is actually quite similar to the case of a real OOM: check if we still available memory (here means memory usage is smaller than limit_in_bytes ), if unfortunately we run out of memory, it will then try to reclaim form the memory cgroup\u2019s LRU lists. If that did not work either, final step would be do OOM actions. mem_cgroup_try_charge() try_charge() page_counter_try_charge(): Check if we hit limit_in_bytes counter. Hierarchically charge pages, costly. try_to_free_mem_cgroup_pages() Callback to mm/vmscan.c to shrink the list ( Bingo! ) Also, reclaimer will establish swap pte entries mem_cgroup_oom() mem_cgroup_lruvec() Other than the global zone-wide LRU lists vector, each cgroup has its own LRU lists vector. Choose the vector that will be passed down to do shrink_page_list() etc.","title":"How cgroup Trigger SwAp"},{"location":"notes/cgroup-swap/#lru-lists-maintainence","text":"Insertion to LRU lists is performed as follows: first, it will be inserted into a per-cpu array ( lru_add_pvec ). Once the array is full (default 15 entries), it will do a batch insertion into proper LRU lists (depends on mem_cgroup_lruvec we mentioned above). Why Linux is doing this way? To scale. \u2013 Yizhou Shan Created: Dec 3, 2018 Last Updated: Jul 30, 2019","title":"LRU Lists Maintainence"},{"location":"notes/hardware_pl/","text":"Hardware Design Languages \u00b6 Version History Date Description Nov 13, 2020 Initial Version Sep 28, 2020 Initial Version Introduction \u00b6 There is an increasing interest from both industry and acadamic on designing high-level domain-specific languages for hardware development (both FPGA and ASIC). These advancements would benefit both software and hardware developers. This document reflects my effort on configuring/running these systems and my thoughts on their pros and cons (if any). System Language Sponsor/Status Xilinx High-Level Synthesis C++ Industry. Mature Chisel Scala Industy and Acadamic. Mature SpinalHDL Scala Industry (solo effort). Mature Dahlia Scala Acadamic Google XLS Rust-like Industry. Pre-mature SpinalHDL \u00b6 SpinalHDL is a scala-based meta HLD programming language. SpinalHDL will convert Scala into Verilog. The generated Verilog is very simple and matches what we write in Scala. Besides, you can use Scala Functional Programming to express hardware, really powerful! I found the following stuff very convenient: 1. Connection . I need to connect a lot of AxiStream interfaces very frequently. To connect an input port onto an output port, we can do something like the following snippets. io . in >> io . out . 2. Functional Programming . I can do something like this to get the sum of an array: array . foldLeft ( 0 )( _ + _ ) Google XLS \u00b6 The XLS (Accelerated HW Synthesis) project is a Rust-like DSL for hardware development. Build \u00b6 I used their docker build . , which is extremely lengthy. This is my first using Bazel. Once the build is done, use docker images to check the new docker image ID. To run, docker run -i -t <ID> /bin/bash . After that, follow their quick-guide . The whole project is pre-mature. There are not too many examples, the building process is too long, and even the basic .x -> .v generation needs quite some manual typing. Following its simple_adder quick-start instructions, the following Verilog code is generated: module __simple_add__add ( input wire clk , input wire [ 31 : 0 ] x , input wire [ 31 : 0 ] y , output wire [ 31 : 0 ] out ); // ===== Pipe stage 0: // Registers for pipe stage 0: reg [ 31 : 0 ] p0_x ; reg [ 31 : 0 ] p0_y ; always_ff @ ( posedge clk ) begin p0_x <= x ; p0_y <= y ; end // ===== Pipe stage 1: wire [ 31 : 0 ] p1_add_3_comb ; assign p1_add_3_comb = p0_x + p0_y ; // Registers for pipe stage 1: reg [ 31 : 0 ] p1_add_3 ; always_ff @ ( posedge clk ) begin p1_add_3 <= p1_add_3_comb ; end assign out = p1_add_3 ; endmodule LLVM CIRCT \u00b6 \u201cCIRCT\u201d stands for \u201cCircuit IR Compilers and Tools\u201d . This is also an early-stage LLVM project.","title":"Hardware-Design-Languages"},{"location":"notes/hardware_pl/#hardware-design-languages","text":"Version History Date Description Nov 13, 2020 Initial Version Sep 28, 2020 Initial Version","title":"Hardware Design Languages"},{"location":"notes/hardware_pl/#introduction","text":"There is an increasing interest from both industry and acadamic on designing high-level domain-specific languages for hardware development (both FPGA and ASIC). These advancements would benefit both software and hardware developers. This document reflects my effort on configuring/running these systems and my thoughts on their pros and cons (if any). System Language Sponsor/Status Xilinx High-Level Synthesis C++ Industry. Mature Chisel Scala Industy and Acadamic. Mature SpinalHDL Scala Industry (solo effort). Mature Dahlia Scala Acadamic Google XLS Rust-like Industry. Pre-mature","title":"Introduction"},{"location":"notes/hardware_pl/#spinalhdl","text":"SpinalHDL is a scala-based meta HLD programming language. SpinalHDL will convert Scala into Verilog. The generated Verilog is very simple and matches what we write in Scala. Besides, you can use Scala Functional Programming to express hardware, really powerful! I found the following stuff very convenient: 1. Connection . I need to connect a lot of AxiStream interfaces very frequently. To connect an input port onto an output port, we can do something like the following snippets. io . in >> io . out . 2. Functional Programming . I can do something like this to get the sum of an array: array . foldLeft ( 0 )( _ + _ )","title":"SpinalHDL"},{"location":"notes/hardware_pl/#google-xls","text":"The XLS (Accelerated HW Synthesis) project is a Rust-like DSL for hardware development.","title":"Google XLS"},{"location":"notes/hardware_pl/#build","text":"I used their docker build . , which is extremely lengthy. This is my first using Bazel. Once the build is done, use docker images to check the new docker image ID. To run, docker run -i -t <ID> /bin/bash . After that, follow their quick-guide . The whole project is pre-mature. There are not too many examples, the building process is too long, and even the basic .x -> .v generation needs quite some manual typing. Following its simple_adder quick-start instructions, the following Verilog code is generated: module __simple_add__add ( input wire clk , input wire [ 31 : 0 ] x , input wire [ 31 : 0 ] y , output wire [ 31 : 0 ] out ); // ===== Pipe stage 0: // Registers for pipe stage 0: reg [ 31 : 0 ] p0_x ; reg [ 31 : 0 ] p0_y ; always_ff @ ( posedge clk ) begin p0_x <= x ; p0_y <= y ; end // ===== Pipe stage 1: wire [ 31 : 0 ] p1_add_3_comb ; assign p1_add_3_comb = p0_x + p0_y ; // Registers for pipe stage 1: reg [ 31 : 0 ] p1_add_3 ; always_ff @ ( posedge clk ) begin p1_add_3 <= p1_add_3_comb ; end assign out = p1_add_3 ; endmodule","title":"Build"},{"location":"notes/hardware_pl/#llvm-circt","text":"\u201cCIRCT\u201d stands for \u201cCircuit IR Compilers and Tools\u201d . This is also an early-stage LLVM project.","title":"LLVM CIRCT"},{"location":"notes/kvm-basic/","text":"Just some basics about KVM \u00b6 Update: you can fine more info here https://gdoc.pub/doc/e/2PACX-1vSsskD0A2XgHoZhaYLAkS7lmCOrfxkGXk1WTovWEAyeoELVdBjrE-NzD8h-NvJfKhxMpUg2aXzaD-XG . Resources \u00b6 Intel Virtualisation: How VT-x, KVM and QEMU Work Together Hacking Notes \u00b6 If you are hacking some low-level stuff that is running as a VM, pay close attention if KVM is involved. I started this note because I spent sometime twisting page_fault IDT entry, but it turns out KVM uses async_page_fault . Oh, well. KVM page fault entry ( arch/x86/entry/entry_64.S ) It is idtentry async_page_fault do_async_page_fault has_error_code=1 ..not idtentry page_fault do_page_fault has_error_code=1 More on Virturlization \u00b6 Well. I swear I want to learn more about Virturlization.. Intel SDM, volume 3, Chapter 23 - Chapter 33. \u2013 Yizhou Shan Created: May 20, 2019 Last Updated: Sep 11, 2019","title":"Linux-KVM"},{"location":"notes/kvm-basic/#just-some-basics-about-kvm","text":"Update: you can fine more info here https://gdoc.pub/doc/e/2PACX-1vSsskD0A2XgHoZhaYLAkS7lmCOrfxkGXk1WTovWEAyeoELVdBjrE-NzD8h-NvJfKhxMpUg2aXzaD-XG .","title":"Just some basics about KVM"},{"location":"notes/kvm-basic/#resources","text":"Intel Virtualisation: How VT-x, KVM and QEMU Work Together","title":"Resources"},{"location":"notes/kvm-basic/#hacking-notes","text":"If you are hacking some low-level stuff that is running as a VM, pay close attention if KVM is involved. I started this note because I spent sometime twisting page_fault IDT entry, but it turns out KVM uses async_page_fault . Oh, well. KVM page fault entry ( arch/x86/entry/entry_64.S ) It is idtentry async_page_fault do_async_page_fault has_error_code=1 ..not idtentry page_fault do_page_fault has_error_code=1","title":"Hacking Notes"},{"location":"notes/kvm-basic/#more-on-virturlization","text":"Well. I swear I want to learn more about Virturlization.. Intel SDM, volume 3, Chapter 23 - Chapter 33. \u2013 Yizhou Shan Created: May 20, 2019 Last Updated: Sep 11, 2019","title":"More on Virturlization"},{"location":"notes/linux-boot/","text":"Linux Boot Sequence after GRUB \u00b6 Version History Date Description Dec 23, 2020 new This is adopted from my note here . I was looking into this while I was building the LegoOS\u2019s booting process. I was trying to find out how GRUB2 loads the kernel image and how it prepares all the boot environment. Linux Boot Protocol and Sequence \u00b6 Linux (x86) has a boot protocol between the bootloader and kernel image itself, described here . Essentially, there is a contiguous memory region passing information between these two entities. This big region just like a big C struct : some fields are filled by kernel duing compile time ( arch/x86/boot/tools/build.c and some in code), some fields are filled by GRUB2 during boot time to tell kernel some important addresses, e.g., kernel parameters, ramdisk locations etc. GRUB2 code follows the protocol, and you can partially tell from the grub_cmd_linux() function. Last time I working on this was late 2016, I truly spent a lot investigating how GRUB and linux boot works. I will try to document a bit, if my memory serves: In the Linux kernel, file arch/x86/boot/header.S is the first file got run after GRUB2. This file is a bit complicated but not hard to understand! It has 3 parts. For the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot. It the kernel was loaded by a bootloader like GRUB2, the first part will never execute. The bootload will directly jump to the second part. This is part of the boot protocol. For the second part, it lists all the fields described by the boot protocol. And finally the third part is real-mode instructions that got run after the GRUB2 jumo. The starting function is called start_of_setup , which will do some stack checking, and then jump to C code in arch/x86/boot/main.c . arch/x86/boot/main.c runs on real-mode, it will do some setup and jump to protected-mode (32-bit). It is running after BIOS but before the actual Linux kernel. Thus this piece of code must rely on BIOS to do stuff, which makes it very unique. The major task of the setup code is to prepare the struct boot_params , which has all the boot information, some of them were extracted from the header.S . The struct boot_params will be passed down and used by many kernel subsystems later on. The final jump happens in arch/x86/boot/pmjump.S # # Jump to protected-mode kernel, 0x100000 # which is the compressed/head_$(BITS).o # jmp *% eax Then, we are in arch/x86/boot/compressed/head_64.S . Above pmjump jumps to startup_32 , it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point startup_64 . And finally, we are in 64-bit. The final jump will go to arch/x86/kernel/head_64.S . We are close! Now we are in arch/x86/kernel/head_64.S . We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to arch/x86/kernel/head64.c , the C code! I guess an interesting part is secondary_startup_64 . This code is actually run by non-booting CPUs, or secondary CPUs. After the major boot CPU is up and running (already within start_kernel() ), I believe its the smp_init() that will send IPI wakeup interrupts to all present secondary CPUs. The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in arch/x86/realmode/rm/trampoline.S ! arch/x86/realmode is interesting. It uses piggyback technique. All the real-mode and 32bit code are in arch/x86/realmode/rm/* , a special linker script is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty! Hooray, C world. We are in arch/x86/kernel/head64.c . The starting function is x86_64_start_kernel ! And the end is the start_kernel , the one in init/main.c . In all, there are a lot jumps after GRUB2 loads the kernel image, and it\u2019s really a long road before we can reach start_kernel() . It probably should not be this complex, but the x86 architecture is just making it worse. linux v.s. linux16 \u00b6 An interesting thing is that there are two ways to load an kernel image in grub.cfg , either linux vmlinuz-3.10.0 or linux16 vmlinuz-3.10.0 . They have different effects, but not sure what are those differences. I remember only the linux16 one works for me, but not remembering why. At least on CentOS 7, it\u2019s all linux16 . The linux16 and initrd16 in grub-core/loader/i386/pc/linux.c : GRUB_MOD_INIT ( linux16 ) { cmd_linux = grub_register_command ( \"linux16\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd16\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; } The linux and initrd in grub-core/loader/i386/linux.c : static grub_command_t cmd_linux , cmd_initrd ; GRUB_MOD_INIT ( linux ) { cmd_linux = grub_register_command ( \"linux\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; }","title":"Linux-Boot-After-GRUB"},{"location":"notes/linux-boot/#linux-boot-sequence-after-grub","text":"Version History Date Description Dec 23, 2020 new This is adopted from my note here . I was looking into this while I was building the LegoOS\u2019s booting process. I was trying to find out how GRUB2 loads the kernel image and how it prepares all the boot environment.","title":"Linux Boot Sequence after GRUB"},{"location":"notes/linux-boot/#linux-boot-protocol-and-sequence","text":"Linux (x86) has a boot protocol between the bootloader and kernel image itself, described here . Essentially, there is a contiguous memory region passing information between these two entities. This big region just like a big C struct : some fields are filled by kernel duing compile time ( arch/x86/boot/tools/build.c and some in code), some fields are filled by GRUB2 during boot time to tell kernel some important addresses, e.g., kernel parameters, ramdisk locations etc. GRUB2 code follows the protocol, and you can partially tell from the grub_cmd_linux() function. Last time I working on this was late 2016, I truly spent a lot investigating how GRUB and linux boot works. I will try to document a bit, if my memory serves: In the Linux kernel, file arch/x86/boot/header.S is the first file got run after GRUB2. This file is a bit complicated but not hard to understand! It has 3 parts. For the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot. It the kernel was loaded by a bootloader like GRUB2, the first part will never execute. The bootload will directly jump to the second part. This is part of the boot protocol. For the second part, it lists all the fields described by the boot protocol. And finally the third part is real-mode instructions that got run after the GRUB2 jumo. The starting function is called start_of_setup , which will do some stack checking, and then jump to C code in arch/x86/boot/main.c . arch/x86/boot/main.c runs on real-mode, it will do some setup and jump to protected-mode (32-bit). It is running after BIOS but before the actual Linux kernel. Thus this piece of code must rely on BIOS to do stuff, which makes it very unique. The major task of the setup code is to prepare the struct boot_params , which has all the boot information, some of them were extracted from the header.S . The struct boot_params will be passed down and used by many kernel subsystems later on. The final jump happens in arch/x86/boot/pmjump.S # # Jump to protected-mode kernel, 0x100000 # which is the compressed/head_$(BITS).o # jmp *% eax Then, we are in arch/x86/boot/compressed/head_64.S . Above pmjump jumps to startup_32 , it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point startup_64 . And finally, we are in 64-bit. The final jump will go to arch/x86/kernel/head_64.S . We are close! Now we are in arch/x86/kernel/head_64.S . We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to arch/x86/kernel/head64.c , the C code! I guess an interesting part is secondary_startup_64 . This code is actually run by non-booting CPUs, or secondary CPUs. After the major boot CPU is up and running (already within start_kernel() ), I believe its the smp_init() that will send IPI wakeup interrupts to all present secondary CPUs. The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in arch/x86/realmode/rm/trampoline.S ! arch/x86/realmode is interesting. It uses piggyback technique. All the real-mode and 32bit code are in arch/x86/realmode/rm/* , a special linker script is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty! Hooray, C world. We are in arch/x86/kernel/head64.c . The starting function is x86_64_start_kernel ! And the end is the start_kernel , the one in init/main.c . In all, there are a lot jumps after GRUB2 loads the kernel image, and it\u2019s really a long road before we can reach start_kernel() . It probably should not be this complex, but the x86 architecture is just making it worse.","title":"Linux Boot Protocol and Sequence"},{"location":"notes/linux-boot/#linux-vs-linux16","text":"An interesting thing is that there are two ways to load an kernel image in grub.cfg , either linux vmlinuz-3.10.0 or linux16 vmlinuz-3.10.0 . They have different effects, but not sure what are those differences. I remember only the linux16 one works for me, but not remembering why. At least on CentOS 7, it\u2019s all linux16 . The linux16 and initrd16 in grub-core/loader/i386/pc/linux.c : GRUB_MOD_INIT ( linux16 ) { cmd_linux = grub_register_command ( \"linux16\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd16\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; } The linux and initrd in grub-core/loader/i386/linux.c : static grub_command_t cmd_linux , cmd_initrd ; GRUB_MOD_INIT ( linux ) { cmd_linux = grub_register_command ( \"linux\" , grub_cmd_linux , 0 , N_ ( \"Load Linux.\" )); cmd_initrd = grub_register_command ( \"initrd\" , grub_cmd_initrd , 0 , N_ ( \"Load initrd.\" )); my_mod = mod ; }","title":"linux v.s. linux16"},{"location":"notes/linux-resource/","text":"Linux Kernel Resource \u00b6 Version History Date Description Mar 13, 2020 add lkt, nice Oct 7, 2019 Feels like I need to start thi High-level \u00b6 Linux Kernel Teaching The Linux Storage Stack Diagram Linux kernel map Sched \u00b6 Evolution of the x86 context switch in Linux Misc \u00b6 Why printk() is so complicated (and how to fix it) Slide . Special sections in Linux binaries, 2013 Storage \u00b6 i10, good NVMe code","title":"Linux-Resource"},{"location":"notes/linux-resource/#linux-kernel-resource","text":"Version History Date Description Mar 13, 2020 add lkt, nice Oct 7, 2019 Feels like I need to start thi","title":"Linux Kernel Resource"},{"location":"notes/linux-resource/#high-level","text":"Linux Kernel Teaching The Linux Storage Stack Diagram Linux kernel map","title":"High-level"},{"location":"notes/linux-resource/#sched","text":"Evolution of the x86 context switch in Linux","title":"Sched"},{"location":"notes/linux-resource/#misc","text":"Why printk() is so complicated (and how to fix it) Slide . Special sections in Linux binaries, 2013","title":"Misc"},{"location":"notes/linux-resource/#storage","text":"i10, good NVMe code","title":"Storage"},{"location":"notes/os/","text":"Operating Systems \u00b6 Multics Plan 9 Taos SPIN: V++: - Cache kernel. Nemesis - A more hybrid approach - QoS Crosstalk Singularity, Helios","title":"Operating Systems"},{"location":"notes/os/#operating-systems","text":"Multics Plan 9 Taos SPIN: V++: - Cache kernel. Nemesis - A more hybrid approach - QoS Crosstalk Singularity, Helios","title":"Operating Systems"},{"location":"notes/paper_fpga/","text":"An FPGA Reading List \u00b6 Version History Date Description Aug 26, 2020 Add those 2 ISCA\u201820 papers to Host Virtual Memory Section Nov 30, 2019 Add a lot security papers Oct 22, 2019 Shuffle scheduling section. More focused. Add two more recent fpga-virt papers Oct 5, 2019 More on scheduling. Add NoC. Add Security. Oct 4, 2019 Add more papers extracted from AmophOS Oct 3, 2019 Initial version from Github A list of related papers I came across while doing FPGA-related research. If you\u2019d like to contribute, please comment below or create PR here . Virtualization Scheduling NoC Memory Hierarchy Dynamic Memory Allocation Integrate with Host Virtual Memory Integrate with Host OSs Security Summary Languages, Runtime, and Framework Xilinx HLS Xilinx CAD High-Level Languages and Platforms Integrate with Frameworks Cloud Infrastructure Misc Applications Programmable Network Database Storage Machine Learning Graph Key-Value Store Bio Consensus Video Processing Blockchain Micro-services FPGA Internal General Partial Reconfiguration Logical Optimization and Technology Mapping Place and Route RTL2FPGA Virtualization \u00b6 Scheduling \u00b6 Scheduling is big topic for FPGA. Unlike the traditional CPU scheduling, there are more aspects to consider, e.g., 1) Partial reconfiguration (PR), 2) Dynamic self PR, 3) Preemptive scheduling, 4) Relocation, 5) Floorplanning, and so on. Preemptive Scheduling \u00b6 Preemptive multitasking on FPGAs, 2000 Multitasking on FPGA Coprocessors, 2000 Context saving and restoring for multitasking in reconfigurable systems, 2005 ReconOS Cooperative multithreading in dynamically reconfigurable systems, FPL\u201809 Block, drop or roll(back): Alternative preemption methods for RH multi-tasking, FCCM\u201809 Hardware Context-Switch Methodology for Dynamically Partially Reconfigurable Systems, 2010 On-chip Context Save and Restore of Hardware Tasks on Partially Reconfigurable FPGAs, 2013 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013 Preemptive Hardware Multitasking in ReconOS, 2015 Preemptive Reconfiguration \u00b6 Preemption of the Partial Reconfiguration Process to Enable Real-Time Computing, 2018 Bitstreams \u00b6 Github 7-series bitmap reverse engineering PARBIT: A Tool to Transform Bitfiles to Implement Partial Reconfiguration of Field Programmable Gate Arrays (FPGAs), 2001 BIL: A TOOL-CHAIN FOR BITSTREAM REVERSE-ENGINEERING, 2012 BITMAN: A Tool and API for FPGA Bitstream Manipulations, 2017 Relocation: \u00b6 Context saving and restoring for multitasking in reconfigurable systems, 2005 REPLICA2Pro: Task Relocation by Bitstream Manipulation in Virtex-II/Pro FPGAs, 2006 Relocation and Automatic Floor-planning of FPGA Partial Configuration Bit-Streams, MSR 2008 Internal and External Bitstream Relocation for Partial Dynamic Reconfiguration, 2009 PRR-PRR Dynamic Relocation, 2009 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2003 AutoReloc, 2016 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013 Others \u00b6 hthreads: A hardware/software co-designed multithreaded RTOS kernel, 2005 hthreads: Enabling a Uniform Programming Model Across the Software/Hardware Boundary, FCCM\u201816 Tartan: Evaluating Spatial Computation for Whole Program Execution, ASPLOS\u201806 A virtual hardware operating system for the Xilinx XC6200, 1996 The Swappable Logic Unit: a Paradigm for Virtual Hardware, FCCM\u201897 Run-time management of dynamically reconfigurable designs, 1998 All above ones are early work on FPGA scheduling. Worth a read, but don\u2019t take some of their assumptions. Some have been changed after SO many years. S1. Reconfigurable Hardware Operating Systems: From Design Concepts to Realizations, 2003 S2. Operating Systems for Reconfigurable Embedded Platforms: Online Scheduling of Real-Time Tasks, 2004 Very fruitful discussion. The paper schedules bitstreams inside FPGA, following a Real-Time sched policy (deadline) . Different from CPU sched, FPGA scheduling needs to consider \u201careas\u201d. The chip is a rectangle box, allocating areas needs great care to avoid fragmentation! Context saving and restoring for multitasking in reconfigurable systems, FPL\u201805 Optimizing deschedule perf. This paper discusses ways to save and restore the state information of a hardware task. There are generally three approachs: a) adding indirection. Let app use system API to read/write states. b) yield-type API. c) use PR controller to read back bitstream. This paper used ICAP to read the bitstream back and extract necenssay state information that must be present at next bitstream resume. Scheduling intervals for reconfigurable computing, FCCM\u201808 Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010 Online Scheduling for Multi-core Shared Reconfigurable Fabric, DATE\u201812 Multi-shape Tasks Scheduling for Online Multitasking on FPGAs, 2014 AmophOS, OSDI\u201818 Hardware context switching on FPGAs, 2014 Efficient Hardware Context-Switch for Task Migration between Heterogeneous FPGAs, 2016 NoC \u00b6 Network-on-Chip on FPGA. Interconnection Networks Enable Fine-Grain Dynamic Multi-Tasking on FPGAs, 2002 Like the idea of separating computation from communication. Also a lot discussions about possible NoC designs within FPGA. LEAP Soft connections: Addressing the hardware-design modularity problem, DAC\u201809 Virtual channel concept. Time-insensitive. Leveraging Latency-Insensitivity to Ease Multiple FPGA Design, FPGA\u201812 CONNECT: re-examining conventional wisdom for designing nocs in the context of FPGAs, FPGA\u201812 Your Programmable NIC Should be a Programmable Switch, HotNets\u201818 Memory Hierarchy \u00b6 Papers deal with BRAM, registers, on-board DRAM, and host DRAM. LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\u201811 Main design hierarchy: Use BRAM as L1 cache, use on-board DRAM as L2 cache, and host memory as the backing store. Everthing is abstracted away through their interface (similar to load/store). Programming is pretty much the same as if you are writing for CPU. According to sec 2.2.2, its scratchpad controller, is using simple segment-based mapping scheme. Like AmorphOS\u2019s one. LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\u201814 Follow up work on LEAP Scratchpads, extends the work to have cache coherence between multiple FPGAs. Coherent Scatchpads with MOSI protocol. MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\u201815 CoRAM: An In-Fabric Memory Architecture for FPGA-Based Computing CoRAM provides an interface for managing the on- and off-chip memory resource of an FPGA. It use \u201ccontrol threads\u201d enforce low-level control on data movement. Seriously, the CoRAM is just like Processor L1-L3 caches. CoRAM Prototype and evaluation of the CoRAM memory architecture for FPGA-based computing, FPGA\u201812 Prototype on FPGA. Sharing, Protection, and Compatibility for Reconfigurable Fabric with AMORPHOS, OSDI\u201818 Hull: provides memory protection for on-board DRAM using segment-based address translation. Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817 Dynamic Memory Allocation \u00b6 malloc() and free() for FPGA on-board DRAM. A High-Performance Memory Allocator for Object-Oriented Systems, IEEE\u201896 SysAlloc: A Hardware Manager for Dynamic Memory Allocation in Heterogeneous Systems, FPL\u201815 Hi-DMM: High-Performance Dynamic Memory Management in High-Level Synthesis, IEEE\u201818 Integrate with Host Virtual Memory \u00b6 Papers deal with OS Virtual Memory System (VMS). Note that, all these papers introduce some form of MMU into the FPGA to let FPGA be able to work with host VMS. This added MMU is similar to CPU\u2019s MMU and RDMA NIC\u2019s internal cache . Note that the VMS still runs inside Linux (include pgfault, swapping, TLB shootdown and so on.), except one recent ISCA\u201820 paper. Virtual Memory Window for Application-Specific Reconfigurable Coprocessors, DAC\u201804 Early work that adds a new MMU to FPGA to let FPGA logic access on-chip DRAM . Note, it\u2019s not the system main memory. Thus the translation pgtable is different. Has some insights on prefetching and MMU CAM design. Seamless Hardware Software Integration in Reconfigurable Computing Systems, 2005 Follow up summary on previous DAC\u201804 Virtual Memory Window. A Reconfigurable Hardware Interface for a Modern Computing System, FCCM\u201807 This work adds a new MMU which includes a 16-entry TLB to FPGA. FPGA and CPU shares the same user virtual address space, use the same physical memory. FPGA and CPU share memory at cacheline granularity , FPGA is just another core in this sense. Upon a TLB miss at FPGA MMU, the FPGA sends interrupt to CPU, to let software to handle the TLB miss . Using software-managed TLB miss is not efficient. But they made cache coherence between FPGA and CPU easy. Low-Latency High-Bandwidth HW/SW Communication in a Virtual Memory Environment, FPL\u201808 This work actually add a new MMU to FPGA, which works just like CPU MMU. It\u2019s similar to IOMMU, in some sense. But I think they missed one important aspect: cache coherence between CPU and FPGA. There is not too much information about this in the paper, it seems they do not have cache at FPGA. Anyhow, this is why recently CCIX and OpenCAPI are proposed. Memory Virtualization for Multithreaded Reconfigurable Hardware, FPL\u201811 Part of the ReconOS project They implemented a simple MMU inside FPGA that includes a TLB. On protection violation or page invalid access cases, their MMU just hand over to CPU pgfault routines. How is this different from the FPL\u201808 one? Actually, IMO, they are the same. S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817 This paper also implemented a hardware MMU, but the virtual memory system still run on Linux. Also listed in Cloud Infrastructure part. Lightweight Virtual Memory Support for Many-Core Accelerators in Heterogeneous Embedded SoCs, 2015 Lightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs, IEEE\u201817 Part of the PULP project. Essentially a software-managed IOMMU. The control path is running as a Linux kernel module. The datapath is a lightweight AXI transation translation. Flick: Fast and Lightweight ISA-Crossing Call for Heterogeneous-ISA Environments, ISCA\u201820 This paper adds an MMU/TLB into FPGA-side RISC-V to fetch/translate host pgtable entries. This paper\u2019s goal is to migrate threads between different ISAs, the key is VM. But what\u2019s new? A Case for Hardware-Based Demand Paging, ISCA\u201820 This paper is not FPGA-based, but does augments host MMU with pgfault handling capability. This paper targets file-backed pgfault, more specific, ultra-low-latency SSD backed files. It adds several HW units to let CPU MMU able to handle and resolve such pgfaults (essentially offload VFS->FS->BLK->NVMe Driver functionalties into HW. Some part is done via mmap() beforehand). It\u2019s async free page list, async LRU handling are used by our work as well. Integrate with Host OSs \u00b6 A Virtual Hardware Operating System for the Xilinx XC6200, FPL\u201896 Operating systems for reconfigurable embedded platforms: online scheduling of real-time tasks, IEEE\u201804 hthreads: a hardware/software co-designed multithreaded RTOS kernel, 2005 Reconfigurable computing: architectures and design methods, IEE\u201805 BORPH: An Operating System for FPGA-Based Reconfigurable Computers. PhD Thesis. FUSE: Front-end user framework for O/S abstraction of hardware accelerators, FCCM\u201811 ReconOS \u2013 an Operating System Approach for Reconfigurable Computing, IEEE Micro\u201814 Invoke kernel from FPGA. They built a shell in FPGA and delegation threads in CPU to achieve this. They implemented their own MMU (using pre-established pgtables) to let FPGA logic to access system memory. Ref . Read the \u201cOperating Systems for Reconfigurable Computing\u201d sidebar, nice summary. LEAP Soft connections: Addressing the hardware-design modularity problem, DAC\u201809 Channel concept. Good. LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\u201811 BRAM/on-board DRAM/host DRAM layering. Caching. LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories Add cache-coherence on top of previous work. Also check out my note on Cache Coherence . LEAP FPGA Operating System, FPL\u201814. A Survey on FPGA Virtualization, FPL\u201818 ZUCL 2.0: Virtualised Memory and Communication for ZYNQ UltraScale+ FPGAs, FSP\u201819 Security \u00b6 If I were to recommend, I\u2019d suggest start from: Recent Attacks and Defenses on FPGA-based Systems, 2019 Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey, 2019 FPGA security: Motivations, features, and applications, 2014 The whole list: FPGAhammer : Remote Voltage Fault Attacks on Shared FPGAs , suitable for DFA on AES FPGA-Based Remote Power Side-Channel Attacks Characterization of long wire data leakage in deep submicron FPGAS Protecting against cryptographic Trojans in FPGAS FPGA Side Channel Attacks without Physical Access FPGA security: Motivations, features, and applications FPGA side-channel receivers Security of FPGAs in data centers Secure Function Evaluation Using an FPGA Overlay Architecture Mitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud The Costs of Confidentiality in Virtualized FPGAs Temporal Thermal Covert Channels in Cloud FPGAs Characterizing Power Distribution Attacks in Multi-User FPGA Environments FASE: FPGA Acceleration of Secure Function Evaluation Securing Cryptographic Circuits by Exploiting Implementation Diversity and Partial Reconfiguration on FPGAs Measuring Long Wire Leakage with Ring Oscillators in Cloud FPGAs Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey Leaky Wires: Information Leakage and Covert Communication Between FPGA Long Wires Using the Power Side Channel of FPGAs for Communication An Inside Job: Remote Power Analysis Attacks on FPGAs Leakier Wires: Exploiting FPGA Long Wires for Covert- and Side-channel Attacks Voltage drop-based fault attacks on FPGAs using valid bitstreams Moats and Drawbridges: An Isolation Primitive for Reconfigurable Hardware Based Systems Sensing nanosecond-scale voltage attacks and natural transients in FPGAs Holistic Power Side-Channel Leakage Assessment: Hiding Intermittent Information Leakage with Architectural Support for Blinking Examining the consequences of high-level synthesis optimizations on power side-channel Register transfer level information flow tracking for provably secure hardware design A Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs Recent Attacks and Defenses on FPGA-based Systems PFC: Privacy Preserving FPGA Cloud - A Case Study of MapReduce A Pay-per-Use Licensing Scheme for Hardware IP Cores in Recent SRAM-Based FPGAs FPGAs for trusted cloud computing Summary \u00b6 Summary on current FPGA Virtualization Status. Prior art mainly focus on: 1) How to virtualize on-chip BRAM (e.g., CoRAM, LEAP Scratchpad), 2) How to work with host, specifically, how to use the host DRAM, how to use host virtual memory. 3) How to schedule bitstreams inside a FPGA chip. 4) How to provide certain services to make FPGA programming easier (mostly work with host OS). Languages, Runtime, and Framework \u00b6 Innovations in the toolchain space. Xilinx HLS \u00b6 Design Patterns for Code Reuse in HLS Packet Processing Pipelines, FCCM\u201819 A very good HLS library from Mellanox folks. Templatised Soft Floating-Point for High-Level Synthesis, FCCM\u201819 ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA, FCCM\u201818 HLScope+: Fast and Accurate Performance Estimation for FPGA HLS, ICCAD\u201817 Separation Logic-Assisted Code Transformations for Efficient High-Level Synthesis, FCCM\u201814 An HLS design aids that analyze the original program at compile time and perform automated code transformations. The tool analysis pointer-manipulating programs and automatically splits heap-allocated data structures into disjoint, independent regions. The tool is for C++ heap operations. To put in another way: the tool looks at your BRAM usage, found any false-dependencies, and make multiple independent regions, then your II is improved. MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\u201815 This is an HLS toolchain aid. Follow-up work of the above FCCM\u201814 one. This time they use LEAP scracchpads as the underlying caching block. Xilinx CAD \u00b6 Maverick: A Stand-alone CAD Flow for Partially Reconfigurable FPGA Modules, FCCM\u201819 High-Level Languages and Platforms \u00b6 Just-in-Time Compilation for Verilog, ASPLOS\u201819 Chisel: Constructing Hardware in a Scala Embedded Language, DAC\u201812 Chisel is being actively improved and used by UCB folks. Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\u201818 From JVM to FPGA: Bridging Abstraction Hierarchy via Optimized Deep Pipelining, HotCloud\u201818 HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing, FPGA\u201819 LINQits: Big Data on Little Clients, ISCA\u201813 From Microsoft, used to express SQL-like functions (thus big data) and runs on ZYNQ (thus little client), You wrote C#, LINQits translate it to verilog, and run the whole thing at a ZYNQ (ARM+FPGA) board. Lime: a Java-Compatible and Synthesizable Language for Heterogeneous Architectures, OOPSLA\u201810 Lime is a Java-based programming model and runtime from IBM which aims to provide a single unified language to program heterogeneous architectures, from FPGAs to conventional CPUs A line of work from Standord Generating configurable hardware from parallel patterns, ASPLOS\u201816 Plasticine: A Reconfigurable Architecture For Parallel Patterns, ISCA\u201817 Spatial: A Language and Compiler for Application Accelerators, PLDI\u201818 Spatial generates Chisel code along with C++ code which can be used on a host CPU to control the execution of the accelerator on the target FPGA. This kind of academic papers must have a lot good ideas. But the truth is it will not be reliable because it\u2019s from academic labs. Integrate with Frameworks \u00b6 Map-reduce as a Programming Model for Custom Computing Machines, FCCM\u201808 This paper proposes a model to translate MapReduce code written in C to code that could run on FPGA and GPU. Many details are omitted, and they don\u2019t really have the compiler. Single-host framework, everything is in FPGA and GPU. Axel: A Heterogeneous Cluster with FPGAs and GPUs, FPGA\u201810 A distributed MapReduce Framework, targets clusters with CPU, GPU, and FPGA. Mainly the idea of scheduling FPGA/GPU jobs. Distributed Framework. FPMR: MapReduce Framework on FPGA, FPGA\u201810 A MapReduce framework on a single host\u2019s FPGA. You need to write Verilog/HLS for processing logic to hook with their framework. The framework mainly includes a data transfer controller, a simple schedule that enable certain blocks at certain time. Single-host framework, everything is in FPGA. Melia: A MapReduce Framework on OpenCL-Based FPGAs, IEEE\u201816 Another framework, written in OpenCL, and users can use OpenCL to program as well. Similar to previous work, it\u2019s more about the framework design, not specific algorithms on FPGA. Single-host framework, everything is in FPGA. But they have a discussion on running on multiple FPGAs. Four MapReduce FPGA papers here, I believe there are more. The marriage between MapReduce and FPGA is not something hard to understand. FPGA can be viewed as another core with different capabilities. The thing is, given FPGA\u2019s reprogram-time and limited on-board memory, how to design a good scheduling algorithm and data moving/caching mechanisms. Those papers give some hints on this. UCLA: When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\u201816 UCLA: Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale, SoCC\u201816 A system that hooks FPGA with Spark. There is a line of work that hook FPGA with big data processing framework (Spark), so the implementation of FPGA and the scale-out software can be separated. The Spark can schedule FPGA jobs to different machines, and take care of scale-out, failure handling etc. But, I personally think this line of work is really just an extension to ReconOS/FUSE/BORPH line of work. The main reason is: both these two lines of work try to integrate jobs run on CPU and jobs run on FPGA, so CPU and FPGA have an easier way to talk, or put in another way, CPU and FPGA have a better division of labor. Whether it\u2019s single-machine (like ReconOS, Melia), or distributed (like Blaze, Axel), they are essentially the same. UCLA: Heterogeneous Datacenters: Options and Opportunities, DAC\u201816 Follow up work of Blaze. Nice comparison of big and wimpy cores. Cloud Infrastructure \u00b6 Huawei: FPGA as a Service in the Cloud UCLA: Customizable Computing: From Single Chip to Datacenters, IEEE\u201818 UCLA: Accelerator-Rich Architectures: Opportunities and Progresses, DAC\u201814 Reminds me of OmniX . Disaggregation at a different scale. This paper actually targets single-machine case. But it can reflect a distributed setting. Enabling FPGAs in the Cloud, CF\u201814 Paper raised four important aspects to enable FPGA in cloud: Abstraction, Sharing, Compatibility, and Security. FPGA itself requires a shell (paper calls it service logic) and being partitioned into multiple slots. Things discussed in the paper are straightforward, but worth reading. They did not solve the FPGA sharing issue, which, is solved by AmorphOS. FPGAs in the Cloud: Booting Virtualized Hardware Accelerators with OpenStack, FCCM\u201814 Use OpenStack to manage FPGA resource. The FPGA is partitioned into multiple regions, each region can use PR. The FPGA shell includes: 1) basic MAC, and packet dispatcher, 2) memory controller, and segment-based partition scheme, 3) a soft processor used for runtime PR control. One very important aspect of this project is: they envision input to FPGA comes from Ethernet, which is very true nowadays. And this also makes their project quite similar to Catapult. It\u2019s a very solid paper, though the evaluation is a little bit weak. What could be added: migration, different-sized region. The above CF and FCCM papers are similar in the sense that they are both building SW framework and HW shell to provide a unified cloud management system. They differ in their shell design: CF one take inputs from DMA engine, which is local system DRAM, FCCM one take inputs from Ethernet. The things after DMA or MAC, are essentially similar. It seems all of them are using simple segment-based memory partition for user FPGA logic. What\u2019s the pros and cons of using paging here? S1 DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\u201814 S2 Virtualized FPGA Accelerators for Efficient Cloud Computing, CloudCom\u201815 S3 Designing a Virtual Runtime for FPGA Accelerators in the Cloud, FPL\u201816 S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817 The above four papers came from the same group of folks. S1 developed a framework to use PCIe to do PR, okay. S2 is a follow-up on S1, read S2\u2019s chapter IV hardware architecture, many implementation details like internal FPGA switch, AXI stream interface. But no memory virtualization discussion. S3 is a two page short paper. S4 is the realization of S3. I was particularly interested if S4 has implemented their own virtual memory management. The answer is NO. S4 leveraged on-chip Linux, they just build a customized MMU (in the form of using BRAM to store page tables. This approach is similar to the papers listed in Integrate with Virtual Memory ). Many things discussed in S4 have been proposed multiple times in previous cloud FPGA papers since 2014. MS: A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services, ISCA\u201814 MS: A Cloud-Scale Acceleration Architecture, Micro\u201816 Catapult is unique in its shell, which includes the Lightweight Transport Layer (LTL), and Elastic Router(ER). The cloud management part, which the paper just briefly mentioned, actually should include everything the above CF\u201814 and FCCM\u201814 have. The LTL has congestion control, packet loss detection/resend, ACK/NACK. The ER is a crossbar switch used by FPGA internal modules, which is essential to connect shell and roles. These two Catapult papers are simply a must read. MS: A Configurable Cloud-Scale DNN Processor for Real-Time AI, Micro\u201818 MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\u201818 MS: Direct Universal Access : Making Data Center Resources Available to FPGA, NSDI\u201819 Catapult is just sweet, isn\u2019t it? ASIC Clouds: Specializing the Datacenter, ISCA\u201816 Virtualizating FPGAs in the Cloud, ASPLOS\u201820 , to appear. Misc \u00b6 A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\u201816 Applications \u00b6 Programmable Network \u00b6 MS: ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware, SIGCOMM\u201816 MS: Multi-Path Transport for RDMA in Datacenters, NSDI\u201818 MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\u201818 Mellanox. NICA: An Infrastructure for Inline Acceleration of Network Applications, ATC\u201819 The Case For In-Network Computing On Demand, EuroSys\u201819 Fast, Scalable, and Programmable Packet Scheduler in Hardware, SIGCOMM\u201819 HPCC: high precision congestion control, SIGCOMM\u201819 Offloading Distributed Applications onto SmartNICs using iPipe, SIGCOMM\u201819 Not necessary FPGA, but SmartNICs. The actor programming model seems a good fit. There is another paper from ATC\u201819 that optimizes distributed actor runtime . Database and SQL \u00b6 On-the-fly Composition of FPGA-Based SQL Query Accelerators Using A Partially Reconfigurable Module Library, 2012 Accelerating database systems using FPGAs: A survey, FPL\u201818 Storage \u00b6 Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\u201819 INSIDER: Designing In-Storage Computing System for Emerging High-Performance Drive, ATC\u201819 LightStore: Software-defined Network-attached Key-value Drives, ASPLOS\u201819 FIDR: A Scalable Storage System for Fine-Grain Inline Data Reduction with Efficient Memory Handling, MICRO\u201819 CIDR: A Cost-Effective In-line Data Reduction System for Terabit-per-Second Scale SSD Array, HPCA\u201819 Machine Learning \u00b6 TABLA: A Unified Template-based Framework for Accelerating Statistical Machine Learning, HPCA\u201816 Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks, FPGA\u201815 From High-Level Deep Neural Models to FPGAs, ISCA\u201816 Deep Learning on FPGAs: Past, Present, and Future, arXiv\u201816 Accelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC, FPT\u201816 FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, FPGA\u201817 In-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA\u201817 Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs, FPGA\u201817 A Configurable Cloud-Scale DNN Processor for Real-Time AI, ISCA\u201818 Microsoft Project Brainware. Built on Catapult. A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks, MICRO\u201818 DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, ICCAD\u201818 FA3C : FPGA-Accelerated Deep Reinforcement Learning\uff0c ASPLOS\u201919 Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\u201819 Graph \u00b6 A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing, ISCA\u201815 Energy Efficient Architecture for Graph Analytics Accelerators, ISCA\u201816 Boosting the Performance of FPGA-based Graph Processor using Hybrid Memory Cube: A Case for Breadth First Search, FPGA\u201817 FPGA-Accelerated Transactional Execution of Graph Workloads, FPGA\u201817 An FPGA Framework for Edge-Centric Graph Processing, CF\u201818 Key-Value Store \u00b6 Achieving 10Gbps line-rate key-value stores with FPGAs, HotCloud\u201813 Thin Servers with Smart Pipes: Designing SoC Accelerators for Memcached, ISCA\u201813 An FPGA Memcached Appliance, FPGA\u201813 Scaling out to a Single-Node 80Gbps Memcached Server with 40Terabytes of Memory, HotStorage\u201815 KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC, SOSP\u201817 This link is also useful for better understading Morning Paper Ultra-Low-Latency and Flexible In-Memory Key-Value Store System Design on CPU-FPGA, FPT\u201818 Bio \u00b6 When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\u201816 FPGA Accelerated INDEL Realignment in the Cloud, HPCA\u201819 Consensus \u00b6 Consensus in a Box: Inexpensive Coordination in Hardware, NSDI\u201816 Video Processing \u00b6 Quantifying the Benefits of Dynamic Partial Reconfiguration for Embedded Vision Applications (FPL 2019) Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic Partial Reconfiguration (FPL 2018) FPGA Internal \u00b6 FPGA20: Highlighting Significant Contributions from 20 Years of the International Symposium on Field-Programmable Gate Arrays (1992\u20132011) General \u00b6 FPGA and CPLD architectures: a tutorial, 1996 Reconfigurable computing: a survey of systems and software, 2002 Reconfigurable computing: architectures and design methods FPGA Architecture: Survey and Challenges, 2007 Read the first two paragraphs of each section and then come back to read all of that if needed. RAMP: Research Accelerator For Multiple Processors, 2007 Three Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology, IEEE\u201815 Partial Reconfiguration \u00b6 FPGA Dynamic and Partial Reconfiguration: A Survey of Architectures, Methods, and Applications, CSUR\u201818 Must read. DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\u201814 A high speed open source controller for FPGA partial reconfiguration Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010 Logical Optimization and Technology Mapping \u00b6 FlowMap: An Optimal Technology Mapping Algorithm for Delay Optimization in Lookup-Table Based FPGA Designs, 1994 Combinational Logic Synthesis for LUT Based Field Programmable Gate Arrays, 1996 DAOmap: A Depth-optimal Area Optimization Mapping Algorithm for FPGA Designs, 2004 Place and Route \u00b6 VPR: A New Packing, Placement and Routing Tool for FPGA Research, 1997 VTR 7.0: Next Generation Architecture and CAD System for FPGAs, 2014 RTL2FPGA \u00b6 A Case for FAME: FPGA Architecture Model Execution, 2010 Strober: Fast and Accurate Sample-Based Energy Simulation for Arbitrary RTL, 2016 Evaluation of RISC-V RTL with FPGA-Accelerated Simulation, 2017 FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud, 2018","title":"FPGA-Paper-Reading"},{"location":"notes/paper_fpga/#an-fpga-reading-list","text":"Version History Date Description Aug 26, 2020 Add those 2 ISCA\u201820 papers to Host Virtual Memory Section Nov 30, 2019 Add a lot security papers Oct 22, 2019 Shuffle scheduling section. More focused. Add two more recent fpga-virt papers Oct 5, 2019 More on scheduling. Add NoC. Add Security. Oct 4, 2019 Add more papers extracted from AmophOS Oct 3, 2019 Initial version from Github A list of related papers I came across while doing FPGA-related research. If you\u2019d like to contribute, please comment below or create PR here . Virtualization Scheduling NoC Memory Hierarchy Dynamic Memory Allocation Integrate with Host Virtual Memory Integrate with Host OSs Security Summary Languages, Runtime, and Framework Xilinx HLS Xilinx CAD High-Level Languages and Platforms Integrate with Frameworks Cloud Infrastructure Misc Applications Programmable Network Database Storage Machine Learning Graph Key-Value Store Bio Consensus Video Processing Blockchain Micro-services FPGA Internal General Partial Reconfiguration Logical Optimization and Technology Mapping Place and Route RTL2FPGA","title":"An FPGA Reading List"},{"location":"notes/paper_fpga/#virtualization","text":"","title":"Virtualization"},{"location":"notes/paper_fpga/#scheduling","text":"Scheduling is big topic for FPGA. Unlike the traditional CPU scheduling, there are more aspects to consider, e.g., 1) Partial reconfiguration (PR), 2) Dynamic self PR, 3) Preemptive scheduling, 4) Relocation, 5) Floorplanning, and so on.","title":"Scheduling"},{"location":"notes/paper_fpga/#preemptive-scheduling","text":"Preemptive multitasking on FPGAs, 2000 Multitasking on FPGA Coprocessors, 2000 Context saving and restoring for multitasking in reconfigurable systems, 2005 ReconOS Cooperative multithreading in dynamically reconfigurable systems, FPL\u201809 Block, drop or roll(back): Alternative preemption methods for RH multi-tasking, FCCM\u201809 Hardware Context-Switch Methodology for Dynamically Partially Reconfigurable Systems, 2010 On-chip Context Save and Restore of Hardware Tasks on Partially Reconfigurable FPGAs, 2013 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013 Preemptive Hardware Multitasking in ReconOS, 2015","title":"Preemptive Scheduling"},{"location":"notes/paper_fpga/#preemptive-reconfiguration","text":"Preemption of the Partial Reconfiguration Process to Enable Real-Time Computing, 2018","title":"Preemptive Reconfiguration"},{"location":"notes/paper_fpga/#bitstreams","text":"Github 7-series bitmap reverse engineering PARBIT: A Tool to Transform Bitfiles to Implement Partial Reconfiguration of Field Programmable Gate Arrays (FPGAs), 2001 BIL: A TOOL-CHAIN FOR BITSTREAM REVERSE-ENGINEERING, 2012 BITMAN: A Tool and API for FPGA Bitstream Manipulations, 2017","title":"Bitstreams"},{"location":"notes/paper_fpga/#relocation","text":"Context saving and restoring for multitasking in reconfigurable systems, 2005 REPLICA2Pro: Task Relocation by Bitstream Manipulation in Virtex-II/Pro FPGAs, 2006 Relocation and Automatic Floor-planning of FPGA Partial Configuration Bit-Streams, MSR 2008 Internal and External Bitstream Relocation for Partial Dynamic Reconfiguration, 2009 PRR-PRR Dynamic Relocation, 2009 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2003 AutoReloc, 2016 HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013","title":"Relocation:"},{"location":"notes/paper_fpga/#others","text":"hthreads: A hardware/software co-designed multithreaded RTOS kernel, 2005 hthreads: Enabling a Uniform Programming Model Across the Software/Hardware Boundary, FCCM\u201816 Tartan: Evaluating Spatial Computation for Whole Program Execution, ASPLOS\u201806 A virtual hardware operating system for the Xilinx XC6200, 1996 The Swappable Logic Unit: a Paradigm for Virtual Hardware, FCCM\u201897 Run-time management of dynamically reconfigurable designs, 1998 All above ones are early work on FPGA scheduling. Worth a read, but don\u2019t take some of their assumptions. Some have been changed after SO many years. S1. Reconfigurable Hardware Operating Systems: From Design Concepts to Realizations, 2003 S2. Operating Systems for Reconfigurable Embedded Platforms: Online Scheduling of Real-Time Tasks, 2004 Very fruitful discussion. The paper schedules bitstreams inside FPGA, following a Real-Time sched policy (deadline) . Different from CPU sched, FPGA scheduling needs to consider \u201careas\u201d. The chip is a rectangle box, allocating areas needs great care to avoid fragmentation! Context saving and restoring for multitasking in reconfigurable systems, FPL\u201805 Optimizing deschedule perf. This paper discusses ways to save and restore the state information of a hardware task. There are generally three approachs: a) adding indirection. Let app use system API to read/write states. b) yield-type API. c) use PR controller to read back bitstream. This paper used ICAP to read the bitstream back and extract necenssay state information that must be present at next bitstream resume. Scheduling intervals for reconfigurable computing, FCCM\u201808 Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010 Online Scheduling for Multi-core Shared Reconfigurable Fabric, DATE\u201812 Multi-shape Tasks Scheduling for Online Multitasking on FPGAs, 2014 AmophOS, OSDI\u201818 Hardware context switching on FPGAs, 2014 Efficient Hardware Context-Switch for Task Migration between Heterogeneous FPGAs, 2016","title":"Others"},{"location":"notes/paper_fpga/#noc","text":"Network-on-Chip on FPGA. Interconnection Networks Enable Fine-Grain Dynamic Multi-Tasking on FPGAs, 2002 Like the idea of separating computation from communication. Also a lot discussions about possible NoC designs within FPGA. LEAP Soft connections: Addressing the hardware-design modularity problem, DAC\u201809 Virtual channel concept. Time-insensitive. Leveraging Latency-Insensitivity to Ease Multiple FPGA Design, FPGA\u201812 CONNECT: re-examining conventional wisdom for designing nocs in the context of FPGAs, FPGA\u201812 Your Programmable NIC Should be a Programmable Switch, HotNets\u201818","title":"NoC"},{"location":"notes/paper_fpga/#memory-hierarchy","text":"Papers deal with BRAM, registers, on-board DRAM, and host DRAM. LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\u201811 Main design hierarchy: Use BRAM as L1 cache, use on-board DRAM as L2 cache, and host memory as the backing store. Everthing is abstracted away through their interface (similar to load/store). Programming is pretty much the same as if you are writing for CPU. According to sec 2.2.2, its scratchpad controller, is using simple segment-based mapping scheme. Like AmorphOS\u2019s one. LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\u201814 Follow up work on LEAP Scratchpads, extends the work to have cache coherence between multiple FPGAs. Coherent Scatchpads with MOSI protocol. MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\u201815 CoRAM: An In-Fabric Memory Architecture for FPGA-Based Computing CoRAM provides an interface for managing the on- and off-chip memory resource of an FPGA. It use \u201ccontrol threads\u201d enforce low-level control on data movement. Seriously, the CoRAM is just like Processor L1-L3 caches. CoRAM Prototype and evaluation of the CoRAM memory architecture for FPGA-based computing, FPGA\u201812 Prototype on FPGA. Sharing, Protection, and Compatibility for Reconfigurable Fabric with AMORPHOS, OSDI\u201818 Hull: provides memory protection for on-board DRAM using segment-based address translation. Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817","title":"Memory Hierarchy"},{"location":"notes/paper_fpga/#dynamic-memory-allocation","text":"malloc() and free() for FPGA on-board DRAM. A High-Performance Memory Allocator for Object-Oriented Systems, IEEE\u201896 SysAlloc: A Hardware Manager for Dynamic Memory Allocation in Heterogeneous Systems, FPL\u201815 Hi-DMM: High-Performance Dynamic Memory Management in High-Level Synthesis, IEEE\u201818","title":"Dynamic Memory Allocation"},{"location":"notes/paper_fpga/#integrate-with-host-virtual-memory","text":"Papers deal with OS Virtual Memory System (VMS). Note that, all these papers introduce some form of MMU into the FPGA to let FPGA be able to work with host VMS. This added MMU is similar to CPU\u2019s MMU and RDMA NIC\u2019s internal cache . Note that the VMS still runs inside Linux (include pgfault, swapping, TLB shootdown and so on.), except one recent ISCA\u201820 paper. Virtual Memory Window for Application-Specific Reconfigurable Coprocessors, DAC\u201804 Early work that adds a new MMU to FPGA to let FPGA logic access on-chip DRAM . Note, it\u2019s not the system main memory. Thus the translation pgtable is different. Has some insights on prefetching and MMU CAM design. Seamless Hardware Software Integration in Reconfigurable Computing Systems, 2005 Follow up summary on previous DAC\u201804 Virtual Memory Window. A Reconfigurable Hardware Interface for a Modern Computing System, FCCM\u201807 This work adds a new MMU which includes a 16-entry TLB to FPGA. FPGA and CPU shares the same user virtual address space, use the same physical memory. FPGA and CPU share memory at cacheline granularity , FPGA is just another core in this sense. Upon a TLB miss at FPGA MMU, the FPGA sends interrupt to CPU, to let software to handle the TLB miss . Using software-managed TLB miss is not efficient. But they made cache coherence between FPGA and CPU easy. Low-Latency High-Bandwidth HW/SW Communication in a Virtual Memory Environment, FPL\u201808 This work actually add a new MMU to FPGA, which works just like CPU MMU. It\u2019s similar to IOMMU, in some sense. But I think they missed one important aspect: cache coherence between CPU and FPGA. There is not too much information about this in the paper, it seems they do not have cache at FPGA. Anyhow, this is why recently CCIX and OpenCAPI are proposed. Memory Virtualization for Multithreaded Reconfigurable Hardware, FPL\u201811 Part of the ReconOS project They implemented a simple MMU inside FPGA that includes a TLB. On protection violation or page invalid access cases, their MMU just hand over to CPU pgfault routines. How is this different from the FPL\u201808 one? Actually, IMO, they are the same. S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817 This paper also implemented a hardware MMU, but the virtual memory system still run on Linux. Also listed in Cloud Infrastructure part. Lightweight Virtual Memory Support for Many-Core Accelerators in Heterogeneous Embedded SoCs, 2015 Lightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs, IEEE\u201817 Part of the PULP project. Essentially a software-managed IOMMU. The control path is running as a Linux kernel module. The datapath is a lightweight AXI transation translation. Flick: Fast and Lightweight ISA-Crossing Call for Heterogeneous-ISA Environments, ISCA\u201820 This paper adds an MMU/TLB into FPGA-side RISC-V to fetch/translate host pgtable entries. This paper\u2019s goal is to migrate threads between different ISAs, the key is VM. But what\u2019s new? A Case for Hardware-Based Demand Paging, ISCA\u201820 This paper is not FPGA-based, but does augments host MMU with pgfault handling capability. This paper targets file-backed pgfault, more specific, ultra-low-latency SSD backed files. It adds several HW units to let CPU MMU able to handle and resolve such pgfaults (essentially offload VFS->FS->BLK->NVMe Driver functionalties into HW. Some part is done via mmap() beforehand). It\u2019s async free page list, async LRU handling are used by our work as well.","title":"Integrate with Host Virtual Memory"},{"location":"notes/paper_fpga/#integrate-with-host-oss","text":"A Virtual Hardware Operating System for the Xilinx XC6200, FPL\u201896 Operating systems for reconfigurable embedded platforms: online scheduling of real-time tasks, IEEE\u201804 hthreads: a hardware/software co-designed multithreaded RTOS kernel, 2005 Reconfigurable computing: architectures and design methods, IEE\u201805 BORPH: An Operating System for FPGA-Based Reconfigurable Computers. PhD Thesis. FUSE: Front-end user framework for O/S abstraction of hardware accelerators, FCCM\u201811 ReconOS \u2013 an Operating System Approach for Reconfigurable Computing, IEEE Micro\u201814 Invoke kernel from FPGA. They built a shell in FPGA and delegation threads in CPU to achieve this. They implemented their own MMU (using pre-established pgtables) to let FPGA logic to access system memory. Ref . Read the \u201cOperating Systems for Reconfigurable Computing\u201d sidebar, nice summary. LEAP Soft connections: Addressing the hardware-design modularity problem, DAC\u201809 Channel concept. Good. LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\u201811 BRAM/on-board DRAM/host DRAM layering. Caching. LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories Add cache-coherence on top of previous work. Also check out my note on Cache Coherence . LEAP FPGA Operating System, FPL\u201814. A Survey on FPGA Virtualization, FPL\u201818 ZUCL 2.0: Virtualised Memory and Communication for ZYNQ UltraScale+ FPGAs, FSP\u201819","title":"Integrate with Host OSs"},{"location":"notes/paper_fpga/#security","text":"If I were to recommend, I\u2019d suggest start from: Recent Attacks and Defenses on FPGA-based Systems, 2019 Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey, 2019 FPGA security: Motivations, features, and applications, 2014 The whole list: FPGAhammer : Remote Voltage Fault Attacks on Shared FPGAs , suitable for DFA on AES FPGA-Based Remote Power Side-Channel Attacks Characterization of long wire data leakage in deep submicron FPGAS Protecting against cryptographic Trojans in FPGAS FPGA Side Channel Attacks without Physical Access FPGA security: Motivations, features, and applications FPGA side-channel receivers Security of FPGAs in data centers Secure Function Evaluation Using an FPGA Overlay Architecture Mitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud The Costs of Confidentiality in Virtualized FPGAs Temporal Thermal Covert Channels in Cloud FPGAs Characterizing Power Distribution Attacks in Multi-User FPGA Environments FASE: FPGA Acceleration of Secure Function Evaluation Securing Cryptographic Circuits by Exploiting Implementation Diversity and Partial Reconfiguration on FPGAs Measuring Long Wire Leakage with Ring Oscillators in Cloud FPGAs Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey Leaky Wires: Information Leakage and Covert Communication Between FPGA Long Wires Using the Power Side Channel of FPGAs for Communication An Inside Job: Remote Power Analysis Attacks on FPGAs Leakier Wires: Exploiting FPGA Long Wires for Covert- and Side-channel Attacks Voltage drop-based fault attacks on FPGAs using valid bitstreams Moats and Drawbridges: An Isolation Primitive for Reconfigurable Hardware Based Systems Sensing nanosecond-scale voltage attacks and natural transients in FPGAs Holistic Power Side-Channel Leakage Assessment: Hiding Intermittent Information Leakage with Architectural Support for Blinking Examining the consequences of high-level synthesis optimizations on power side-channel Register transfer level information flow tracking for provably secure hardware design A Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs Recent Attacks and Defenses on FPGA-based Systems PFC: Privacy Preserving FPGA Cloud - A Case Study of MapReduce A Pay-per-Use Licensing Scheme for Hardware IP Cores in Recent SRAM-Based FPGAs FPGAs for trusted cloud computing","title":"Security"},{"location":"notes/paper_fpga/#summary","text":"Summary on current FPGA Virtualization Status. Prior art mainly focus on: 1) How to virtualize on-chip BRAM (e.g., CoRAM, LEAP Scratchpad), 2) How to work with host, specifically, how to use the host DRAM, how to use host virtual memory. 3) How to schedule bitstreams inside a FPGA chip. 4) How to provide certain services to make FPGA programming easier (mostly work with host OS).","title":"Summary"},{"location":"notes/paper_fpga/#languages-runtime-and-framework","text":"Innovations in the toolchain space.","title":"Languages, Runtime, and Framework"},{"location":"notes/paper_fpga/#xilinx-hls","text":"Design Patterns for Code Reuse in HLS Packet Processing Pipelines, FCCM\u201819 A very good HLS library from Mellanox folks. Templatised Soft Floating-Point for High-Level Synthesis, FCCM\u201819 ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA, FCCM\u201818 HLScope+: Fast and Accurate Performance Estimation for FPGA HLS, ICCAD\u201817 Separation Logic-Assisted Code Transformations for Efficient High-Level Synthesis, FCCM\u201814 An HLS design aids that analyze the original program at compile time and perform automated code transformations. The tool analysis pointer-manipulating programs and automatically splits heap-allocated data structures into disjoint, independent regions. The tool is for C++ heap operations. To put in another way: the tool looks at your BRAM usage, found any false-dependencies, and make multiple independent regions, then your II is improved. MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\u201815 This is an HLS toolchain aid. Follow-up work of the above FCCM\u201814 one. This time they use LEAP scracchpads as the underlying caching block.","title":"Xilinx HLS"},{"location":"notes/paper_fpga/#xilinx-cad","text":"Maverick: A Stand-alone CAD Flow for Partially Reconfigurable FPGA Modules, FCCM\u201819","title":"Xilinx CAD"},{"location":"notes/paper_fpga/#high-level-languages-and-platforms","text":"Just-in-Time Compilation for Verilog, ASPLOS\u201819 Chisel: Constructing Hardware in a Scala Embedded Language, DAC\u201812 Chisel is being actively improved and used by UCB folks. Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\u201818 From JVM to FPGA: Bridging Abstraction Hierarchy via Optimized Deep Pipelining, HotCloud\u201818 HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing, FPGA\u201819 LINQits: Big Data on Little Clients, ISCA\u201813 From Microsoft, used to express SQL-like functions (thus big data) and runs on ZYNQ (thus little client), You wrote C#, LINQits translate it to verilog, and run the whole thing at a ZYNQ (ARM+FPGA) board. Lime: a Java-Compatible and Synthesizable Language for Heterogeneous Architectures, OOPSLA\u201810 Lime is a Java-based programming model and runtime from IBM which aims to provide a single unified language to program heterogeneous architectures, from FPGAs to conventional CPUs A line of work from Standord Generating configurable hardware from parallel patterns, ASPLOS\u201816 Plasticine: A Reconfigurable Architecture For Parallel Patterns, ISCA\u201817 Spatial: A Language and Compiler for Application Accelerators, PLDI\u201818 Spatial generates Chisel code along with C++ code which can be used on a host CPU to control the execution of the accelerator on the target FPGA. This kind of academic papers must have a lot good ideas. But the truth is it will not be reliable because it\u2019s from academic labs.","title":"High-Level Languages and Platforms"},{"location":"notes/paper_fpga/#integrate-with-frameworks","text":"Map-reduce as a Programming Model for Custom Computing Machines, FCCM\u201808 This paper proposes a model to translate MapReduce code written in C to code that could run on FPGA and GPU. Many details are omitted, and they don\u2019t really have the compiler. Single-host framework, everything is in FPGA and GPU. Axel: A Heterogeneous Cluster with FPGAs and GPUs, FPGA\u201810 A distributed MapReduce Framework, targets clusters with CPU, GPU, and FPGA. Mainly the idea of scheduling FPGA/GPU jobs. Distributed Framework. FPMR: MapReduce Framework on FPGA, FPGA\u201810 A MapReduce framework on a single host\u2019s FPGA. You need to write Verilog/HLS for processing logic to hook with their framework. The framework mainly includes a data transfer controller, a simple schedule that enable certain blocks at certain time. Single-host framework, everything is in FPGA. Melia: A MapReduce Framework on OpenCL-Based FPGAs, IEEE\u201816 Another framework, written in OpenCL, and users can use OpenCL to program as well. Similar to previous work, it\u2019s more about the framework design, not specific algorithms on FPGA. Single-host framework, everything is in FPGA. But they have a discussion on running on multiple FPGAs. Four MapReduce FPGA papers here, I believe there are more. The marriage between MapReduce and FPGA is not something hard to understand. FPGA can be viewed as another core with different capabilities. The thing is, given FPGA\u2019s reprogram-time and limited on-board memory, how to design a good scheduling algorithm and data moving/caching mechanisms. Those papers give some hints on this. UCLA: When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\u201816 UCLA: Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale, SoCC\u201816 A system that hooks FPGA with Spark. There is a line of work that hook FPGA with big data processing framework (Spark), so the implementation of FPGA and the scale-out software can be separated. The Spark can schedule FPGA jobs to different machines, and take care of scale-out, failure handling etc. But, I personally think this line of work is really just an extension to ReconOS/FUSE/BORPH line of work. The main reason is: both these two lines of work try to integrate jobs run on CPU and jobs run on FPGA, so CPU and FPGA have an easier way to talk, or put in another way, CPU and FPGA have a better division of labor. Whether it\u2019s single-machine (like ReconOS, Melia), or distributed (like Blaze, Axel), they are essentially the same. UCLA: Heterogeneous Datacenters: Options and Opportunities, DAC\u201816 Follow up work of Blaze. Nice comparison of big and wimpy cores.","title":"Integrate with Frameworks"},{"location":"notes/paper_fpga/#cloud-infrastructure","text":"Huawei: FPGA as a Service in the Cloud UCLA: Customizable Computing: From Single Chip to Datacenters, IEEE\u201818 UCLA: Accelerator-Rich Architectures: Opportunities and Progresses, DAC\u201814 Reminds me of OmniX . Disaggregation at a different scale. This paper actually targets single-machine case. But it can reflect a distributed setting. Enabling FPGAs in the Cloud, CF\u201814 Paper raised four important aspects to enable FPGA in cloud: Abstraction, Sharing, Compatibility, and Security. FPGA itself requires a shell (paper calls it service logic) and being partitioned into multiple slots. Things discussed in the paper are straightforward, but worth reading. They did not solve the FPGA sharing issue, which, is solved by AmorphOS. FPGAs in the Cloud: Booting Virtualized Hardware Accelerators with OpenStack, FCCM\u201814 Use OpenStack to manage FPGA resource. The FPGA is partitioned into multiple regions, each region can use PR. The FPGA shell includes: 1) basic MAC, and packet dispatcher, 2) memory controller, and segment-based partition scheme, 3) a soft processor used for runtime PR control. One very important aspect of this project is: they envision input to FPGA comes from Ethernet, which is very true nowadays. And this also makes their project quite similar to Catapult. It\u2019s a very solid paper, though the evaluation is a little bit weak. What could be added: migration, different-sized region. The above CF and FCCM papers are similar in the sense that they are both building SW framework and HW shell to provide a unified cloud management system. They differ in their shell design: CF one take inputs from DMA engine, which is local system DRAM, FCCM one take inputs from Ethernet. The things after DMA or MAC, are essentially similar. It seems all of them are using simple segment-based memory partition for user FPGA logic. What\u2019s the pros and cons of using paging here? S1 DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\u201814 S2 Virtualized FPGA Accelerators for Efficient Cloud Computing, CloudCom\u201815 S3 Designing a Virtual Runtime for FPGA Accelerators in the Cloud, FPL\u201816 S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\u201817 The above four papers came from the same group of folks. S1 developed a framework to use PCIe to do PR, okay. S2 is a follow-up on S1, read S2\u2019s chapter IV hardware architecture, many implementation details like internal FPGA switch, AXI stream interface. But no memory virtualization discussion. S3 is a two page short paper. S4 is the realization of S3. I was particularly interested if S4 has implemented their own virtual memory management. The answer is NO. S4 leveraged on-chip Linux, they just build a customized MMU (in the form of using BRAM to store page tables. This approach is similar to the papers listed in Integrate with Virtual Memory ). Many things discussed in S4 have been proposed multiple times in previous cloud FPGA papers since 2014. MS: A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services, ISCA\u201814 MS: A Cloud-Scale Acceleration Architecture, Micro\u201816 Catapult is unique in its shell, which includes the Lightweight Transport Layer (LTL), and Elastic Router(ER). The cloud management part, which the paper just briefly mentioned, actually should include everything the above CF\u201814 and FCCM\u201814 have. The LTL has congestion control, packet loss detection/resend, ACK/NACK. The ER is a crossbar switch used by FPGA internal modules, which is essential to connect shell and roles. These two Catapult papers are simply a must read. MS: A Configurable Cloud-Scale DNN Processor for Real-Time AI, Micro\u201818 MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\u201818 MS: Direct Universal Access : Making Data Center Resources Available to FPGA, NSDI\u201819 Catapult is just sweet, isn\u2019t it? ASIC Clouds: Specializing the Datacenter, ISCA\u201816 Virtualizating FPGAs in the Cloud, ASPLOS\u201820 , to appear.","title":"Cloud Infrastructure"},{"location":"notes/paper_fpga/#misc","text":"A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\u201816","title":"Misc"},{"location":"notes/paper_fpga/#applications","text":"","title":"Applications"},{"location":"notes/paper_fpga/#programmable-network","text":"MS: ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware, SIGCOMM\u201816 MS: Multi-Path Transport for RDMA in Datacenters, NSDI\u201818 MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\u201818 Mellanox. NICA: An Infrastructure for Inline Acceleration of Network Applications, ATC\u201819 The Case For In-Network Computing On Demand, EuroSys\u201819 Fast, Scalable, and Programmable Packet Scheduler in Hardware, SIGCOMM\u201819 HPCC: high precision congestion control, SIGCOMM\u201819 Offloading Distributed Applications onto SmartNICs using iPipe, SIGCOMM\u201819 Not necessary FPGA, but SmartNICs. The actor programming model seems a good fit. There is another paper from ATC\u201819 that optimizes distributed actor runtime .","title":"Programmable Network"},{"location":"notes/paper_fpga/#database-and-sql","text":"On-the-fly Composition of FPGA-Based SQL Query Accelerators Using A Partially Reconfigurable Module Library, 2012 Accelerating database systems using FPGAs: A survey, FPL\u201818","title":"Database and SQL"},{"location":"notes/paper_fpga/#storage","text":"Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\u201819 INSIDER: Designing In-Storage Computing System for Emerging High-Performance Drive, ATC\u201819 LightStore: Software-defined Network-attached Key-value Drives, ASPLOS\u201819 FIDR: A Scalable Storage System for Fine-Grain Inline Data Reduction with Efficient Memory Handling, MICRO\u201819 CIDR: A Cost-Effective In-line Data Reduction System for Terabit-per-Second Scale SSD Array, HPCA\u201819","title":"Storage"},{"location":"notes/paper_fpga/#machine-learning","text":"TABLA: A Unified Template-based Framework for Accelerating Statistical Machine Learning, HPCA\u201816 Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks, FPGA\u201815 From High-Level Deep Neural Models to FPGAs, ISCA\u201816 Deep Learning on FPGAs: Past, Present, and Future, arXiv\u201816 Accelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC, FPT\u201816 FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, FPGA\u201817 In-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA\u201817 Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs, FPGA\u201817 A Configurable Cloud-Scale DNN Processor for Real-Time AI, ISCA\u201818 Microsoft Project Brainware. Built on Catapult. A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks, MICRO\u201818 DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, ICCAD\u201818 FA3C : FPGA-Accelerated Deep Reinforcement Learning\uff0c ASPLOS\u201919 Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\u201819","title":"Machine Learning"},{"location":"notes/paper_fpga/#graph","text":"A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing, ISCA\u201815 Energy Efficient Architecture for Graph Analytics Accelerators, ISCA\u201816 Boosting the Performance of FPGA-based Graph Processor using Hybrid Memory Cube: A Case for Breadth First Search, FPGA\u201817 FPGA-Accelerated Transactional Execution of Graph Workloads, FPGA\u201817 An FPGA Framework for Edge-Centric Graph Processing, CF\u201818","title":"Graph"},{"location":"notes/paper_fpga/#key-value-store","text":"Achieving 10Gbps line-rate key-value stores with FPGAs, HotCloud\u201813 Thin Servers with Smart Pipes: Designing SoC Accelerators for Memcached, ISCA\u201813 An FPGA Memcached Appliance, FPGA\u201813 Scaling out to a Single-Node 80Gbps Memcached Server with 40Terabytes of Memory, HotStorage\u201815 KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC, SOSP\u201817 This link is also useful for better understading Morning Paper Ultra-Low-Latency and Flexible In-Memory Key-Value Store System Design on CPU-FPGA, FPT\u201818","title":"Key-Value Store"},{"location":"notes/paper_fpga/#bio","text":"When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\u201816 FPGA Accelerated INDEL Realignment in the Cloud, HPCA\u201819","title":"Bio"},{"location":"notes/paper_fpga/#consensus","text":"Consensus in a Box: Inexpensive Coordination in Hardware, NSDI\u201816","title":"Consensus"},{"location":"notes/paper_fpga/#video-processing","text":"Quantifying the Benefits of Dynamic Partial Reconfiguration for Embedded Vision Applications (FPL 2019) Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic Partial Reconfiguration (FPL 2018)","title":"Video Processing"},{"location":"notes/paper_fpga/#fpga-internal","text":"FPGA20: Highlighting Significant Contributions from 20 Years of the International Symposium on Field-Programmable Gate Arrays (1992\u20132011)","title":"FPGA Internal"},{"location":"notes/paper_fpga/#general","text":"FPGA and CPLD architectures: a tutorial, 1996 Reconfigurable computing: a survey of systems and software, 2002 Reconfigurable computing: architectures and design methods FPGA Architecture: Survey and Challenges, 2007 Read the first two paragraphs of each section and then come back to read all of that if needed. RAMP: Research Accelerator For Multiple Processors, 2007 Three Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology, IEEE\u201815","title":"General"},{"location":"notes/paper_fpga/#partial-reconfiguration","text":"FPGA Dynamic and Partial Reconfiguration: A Survey of Architectures, Methods, and Applications, CSUR\u201818 Must read. DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\u201814 A high speed open source controller for FPGA partial reconfiguration Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010","title":"Partial Reconfiguration"},{"location":"notes/paper_fpga/#logical-optimization-and-technology-mapping","text":"FlowMap: An Optimal Technology Mapping Algorithm for Delay Optimization in Lookup-Table Based FPGA Designs, 1994 Combinational Logic Synthesis for LUT Based Field Programmable Gate Arrays, 1996 DAOmap: A Depth-optimal Area Optimization Mapping Algorithm for FPGA Designs, 2004","title":"Logical Optimization and Technology Mapping"},{"location":"notes/paper_fpga/#place-and-route","text":"VPR: A New Packing, Placement and Routing Tool for FPGA Research, 1997 VTR 7.0: Next Generation Architecture and CAD System for FPGAs, 2014","title":"Place and Route"},{"location":"notes/paper_fpga/#rtl2fpga","text":"A Case for FAME: FPGA Architecture Model Execution, 2010 Strober: Fast and Accurate Sample-Based Energy Simulation for Arbitrary RTL, 2016 Evaluation of RISC-V RTL with FPGA-Accelerated Simulation, 2017 FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud, 2018","title":"RTL2FPGA"},{"location":"notes/paper_perf_shadows/","text":"Hiding In The Shadows \u00b6 Version History Date Description Jul 11, 2019 Initial draft There are shadows under the sun. There are shadows in your life. There are shadows in your computer. This note is about latency tolerance techniques . This note is about how to get the most out of the otherwise-wasted resource. Nanoseconds \u00b6 Architecture solutions to attack nanosecond-level performance shadows that are mostly created by lower level data and instruction cache misses. OoO and SMT are the base to hide these latencies, but they fall short when ROB is full (or some other reasons). When that happens, these academic ideas come in rescue. Runahead \u00b6 Quote In runahead, once the processor stalls, it uses the instruction window to continue to fetch and execute operations. The goal of runahead is to generate new cache misses, thereby turning subsequent demand requests into cache hits instead of cache misses.[5] Papers Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss, ICS\u201997 Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors, HPCA\u201903 Efficient Runahead Execution: Power-Efficient Memory Latency Tolerance, IEEE Micro\u201906 Good timeline graphs show the benefit of Runahead. Runahead Threads to Improve SMT Performance, HPCA\u201908 QoS control policy. Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads, MICRO\u201916 Nice idea to tackle the issue that runahead does not get enough time to run. Also has the notion of ideal runahead coverage. Comments We should separate mechanism and policy. Runahead is the mechanism. It includes: Enter runahead Execution in runahead context (most important thing is to maintain those INV bits and pseudo-retires) Exit runahead Prefetch is one of the policy, a major one. It\u2019s the side effect of running instructions in the execution phase of runahead mode. QoS control is another policy. This means adding specific rules to the execution phase. More specifically: limit the core resource usage of the runahead thread, thus reduce the impact on the co-running HW thread. Helper Threads (or Precomputation) \u00b6 Quote A helper thread is a stripped down version of the main thread that only includes the necessary instructions to generate memory accesses, including control flow instructions [10]. Quote Precomputation uses idle thread contexts in a multithreaded architecture to improve performance of single-threaded applications. It attacks program stalls from data cache misses by pre-computing future memory accesses in available thread contexts, and prefetching these data.[1] Quote Such pre-execution threads are purely speculative, and their instructions are never committed into the main computation. Instead, the pre-execution threads run code designed to trigger cache misses. As long as the pre-execution threads execute far enough in front of the main thread, they effectively hide the latency of the cache misses so that the main thread experiences signicantly fewer memory stalls.[5] Papers Speculative Precomputation: Long-range Prefetching of Delinquent Loads, ISCA\u201801 Dynamic Speculative Precomputation, Micro\u201801 Take a step further by using HW to construct the offloaded code slice automatically. Execution-based Prediction Using Speculative Slices, ISCA\u201801 Tolerating Memory Latency through Software-Controlled Pre-Execution in Simultaneous Multithreading Processors, ISCA\u201801 What\u2019s up with ISCA\u201801? This paper proposed to use software to control when to start running precomputation and when to exit. It uses compiler\u2019s help to generate those code slices, and insert special start/end instructions. On the contrast, hardware-controller precomputation relies on hints such as cache misses. Design and Evaluation of Compiler Algorithms for PreExecution, ASPLOS\u201802 5.1 A Study of Source-Level Compiler Algorithms for Automatic Construction of Pre-Execution Code, TOCS\u201804 Dynamic Helper Threaded Prefetching on the Sun UltraSPARC\u00ae CMP Processor, Micro\u201805 The function table at helper thread seems nice and useful. Accelerating and Adapting Precomputation Threads for Effcient Prefetching, HPCA\u201807 Dynamically construct precomputation code, called p-slices. They can adapt the same program differently depending on the program\u2019s data input and the underlying hardware architecture. Inter-core Prefetching for Multicore Processors Using Migrating Helper Threads , ASPLOS\u201811 Pure software solution. I like the idea. But I don\u2019t think it will work for realistic applications. Learned setcontext(), getcontext(), and swapcontext() . Bootstrapping: Using SMT Hardware to Improve Single-Thread Performance, ASPLOS\u201819 Freeway: Maximizing MLP for slice-out-of-order execution, HPCA\u201819 Strictly speaking this is not in this catogory. But it is this paper that lead me to Runahead and Helper thread topic. I was doing something similar so those techniques caught my eye. Comments The catch about precomputation is that it must create lightweight threads that can actually proceed faster than the main thread, so that they stay out in front. Other catch is: you also need to create the code slice that will run on another core context. First of all, how is this code slice different from the original code? The extracted code will be simplified in the sense that it will only access memory without doing other computations. The second question is how this code slice is extracted and then constructed? There are many ways. You can handwrite, or use a static compiler to pre-generate them (by using techniques in above papers), or use hardware to dynamically generate them during runtime, or use software to dynamically generate them during runtime. There are ways to it, but I don\u2019t think this is the core of precomputation. Also, same thing here, we should separate mechanism and policies. Helper thread (or precomputation) is mainly used as a vehicle for speculatively generating data addresses and prefetching. Thread-Level Speculation \u00b6 Fill me in. Locks \u00b6 Applying the insight of \u201cget the most out of the otherwise-wasted resource\u201d to the lock area. I will wait for Sanidhya\u2019s SOSP\u201819 paper. :-) Misc \u00b6 Stretch: Balancing QoS and throughput for colocated server workloads on SMT cores (Best Paper), HPCA\u201819 Keyword: ROB , Co-location QoS . This paper tackles the perf interference when running co-running two SMT threads on a single physical core, which is the common case in datacenters. However co-running latency-sensitive jobs and batch jobs will have huge impact on the perf of both. This paper found: \u201cLatency-sensitive workloads show little benefit from large ROB capacities in modern server processors .. because frequent cache misses and data-dependent computation limit both instruction and memory-level parallelisms (ILP and MLP). In contrast, many batch workloads benefit from a large ROB that helps unlock higher ILP and MLP.\u201d So they propose to have a ROB partition scheme rather than static equal partition. Of course they also did some very extensive studies before deciding to scale ROB. They first found shared ROB has the biggest impact on perf interference than any other resources such as branch predictor, cache, and so on. They further found that latency-sensitive workload can tolerate some perf slack, which means they will not violate their QoS even with a smaller ROB. Anyway, I think this is a very nice paper. Good reasoning, simple solution, but works effectively. Put it all together \u00b6 Both runahead and helper thread were proposed to do prefetch. But they have a key difference. Runahead is invoked in the same core , and is invoked when ROB is full (not always though). Helper thread is invoked at another core . Besides, runahead can just fetch the instructions and run, no need to cook another code slice. But for helper thread, it needs to extract a code slice that will run on another core. I think the most important thing is to realize their insight. In the most straightforward and plain way: they are trying to get the most out of the otherwise-wasted resource. For example, in runahead, they realize that with some help, the CPU is still able to generate cache misses even if the instruction table is full. For precomputation, obviously it is using the other idle cores. The simple insight itself is not interesting enough, usually where it\u2019s applied make things quite interesting. Microseconds \u00b6 Fill me in Milliseconds \u00b6 Sleep. And wake me up when september ends. And this seems to be enough. ;-) This is true for OS to handle slow HDD and slow network.","title":"Performance-Shadows"},{"location":"notes/paper_perf_shadows/#hiding-in-the-shadows","text":"Version History Date Description Jul 11, 2019 Initial draft There are shadows under the sun. There are shadows in your life. There are shadows in your computer. This note is about latency tolerance techniques . This note is about how to get the most out of the otherwise-wasted resource.","title":"Hiding In The Shadows"},{"location":"notes/paper_perf_shadows/#nanoseconds","text":"Architecture solutions to attack nanosecond-level performance shadows that are mostly created by lower level data and instruction cache misses. OoO and SMT are the base to hide these latencies, but they fall short when ROB is full (or some other reasons). When that happens, these academic ideas come in rescue.","title":"Nanoseconds"},{"location":"notes/paper_perf_shadows/#runahead","text":"Quote In runahead, once the processor stalls, it uses the instruction window to continue to fetch and execute operations. The goal of runahead is to generate new cache misses, thereby turning subsequent demand requests into cache hits instead of cache misses.[5] Papers Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss, ICS\u201997 Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors, HPCA\u201903 Efficient Runahead Execution: Power-Efficient Memory Latency Tolerance, IEEE Micro\u201906 Good timeline graphs show the benefit of Runahead. Runahead Threads to Improve SMT Performance, HPCA\u201908 QoS control policy. Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads, MICRO\u201916 Nice idea to tackle the issue that runahead does not get enough time to run. Also has the notion of ideal runahead coverage. Comments We should separate mechanism and policy. Runahead is the mechanism. It includes: Enter runahead Execution in runahead context (most important thing is to maintain those INV bits and pseudo-retires) Exit runahead Prefetch is one of the policy, a major one. It\u2019s the side effect of running instructions in the execution phase of runahead mode. QoS control is another policy. This means adding specific rules to the execution phase. More specifically: limit the core resource usage of the runahead thread, thus reduce the impact on the co-running HW thread.","title":"Runahead"},{"location":"notes/paper_perf_shadows/#helper-threads-or-precomputation","text":"Quote A helper thread is a stripped down version of the main thread that only includes the necessary instructions to generate memory accesses, including control flow instructions [10]. Quote Precomputation uses idle thread contexts in a multithreaded architecture to improve performance of single-threaded applications. It attacks program stalls from data cache misses by pre-computing future memory accesses in available thread contexts, and prefetching these data.[1] Quote Such pre-execution threads are purely speculative, and their instructions are never committed into the main computation. Instead, the pre-execution threads run code designed to trigger cache misses. As long as the pre-execution threads execute far enough in front of the main thread, they effectively hide the latency of the cache misses so that the main thread experiences signicantly fewer memory stalls.[5] Papers Speculative Precomputation: Long-range Prefetching of Delinquent Loads, ISCA\u201801 Dynamic Speculative Precomputation, Micro\u201801 Take a step further by using HW to construct the offloaded code slice automatically. Execution-based Prediction Using Speculative Slices, ISCA\u201801 Tolerating Memory Latency through Software-Controlled Pre-Execution in Simultaneous Multithreading Processors, ISCA\u201801 What\u2019s up with ISCA\u201801? This paper proposed to use software to control when to start running precomputation and when to exit. It uses compiler\u2019s help to generate those code slices, and insert special start/end instructions. On the contrast, hardware-controller precomputation relies on hints such as cache misses. Design and Evaluation of Compiler Algorithms for PreExecution, ASPLOS\u201802 5.1 A Study of Source-Level Compiler Algorithms for Automatic Construction of Pre-Execution Code, TOCS\u201804 Dynamic Helper Threaded Prefetching on the Sun UltraSPARC\u00ae CMP Processor, Micro\u201805 The function table at helper thread seems nice and useful. Accelerating and Adapting Precomputation Threads for Effcient Prefetching, HPCA\u201807 Dynamically construct precomputation code, called p-slices. They can adapt the same program differently depending on the program\u2019s data input and the underlying hardware architecture. Inter-core Prefetching for Multicore Processors Using Migrating Helper Threads , ASPLOS\u201811 Pure software solution. I like the idea. But I don\u2019t think it will work for realistic applications. Learned setcontext(), getcontext(), and swapcontext() . Bootstrapping: Using SMT Hardware to Improve Single-Thread Performance, ASPLOS\u201819 Freeway: Maximizing MLP for slice-out-of-order execution, HPCA\u201819 Strictly speaking this is not in this catogory. But it is this paper that lead me to Runahead and Helper thread topic. I was doing something similar so those techniques caught my eye. Comments The catch about precomputation is that it must create lightweight threads that can actually proceed faster than the main thread, so that they stay out in front. Other catch is: you also need to create the code slice that will run on another core context. First of all, how is this code slice different from the original code? The extracted code will be simplified in the sense that it will only access memory without doing other computations. The second question is how this code slice is extracted and then constructed? There are many ways. You can handwrite, or use a static compiler to pre-generate them (by using techniques in above papers), or use hardware to dynamically generate them during runtime, or use software to dynamically generate them during runtime. There are ways to it, but I don\u2019t think this is the core of precomputation. Also, same thing here, we should separate mechanism and policies. Helper thread (or precomputation) is mainly used as a vehicle for speculatively generating data addresses and prefetching.","title":"Helper Threads (or Precomputation)"},{"location":"notes/paper_perf_shadows/#thread-level-speculation","text":"Fill me in.","title":"Thread-Level Speculation"},{"location":"notes/paper_perf_shadows/#locks","text":"Applying the insight of \u201cget the most out of the otherwise-wasted resource\u201d to the lock area. I will wait for Sanidhya\u2019s SOSP\u201819 paper. :-)","title":"Locks"},{"location":"notes/paper_perf_shadows/#misc","text":"Stretch: Balancing QoS and throughput for colocated server workloads on SMT cores (Best Paper), HPCA\u201819 Keyword: ROB , Co-location QoS . This paper tackles the perf interference when running co-running two SMT threads on a single physical core, which is the common case in datacenters. However co-running latency-sensitive jobs and batch jobs will have huge impact on the perf of both. This paper found: \u201cLatency-sensitive workloads show little benefit from large ROB capacities in modern server processors .. because frequent cache misses and data-dependent computation limit both instruction and memory-level parallelisms (ILP and MLP). In contrast, many batch workloads benefit from a large ROB that helps unlock higher ILP and MLP.\u201d So they propose to have a ROB partition scheme rather than static equal partition. Of course they also did some very extensive studies before deciding to scale ROB. They first found shared ROB has the biggest impact on perf interference than any other resources such as branch predictor, cache, and so on. They further found that latency-sensitive workload can tolerate some perf slack, which means they will not violate their QoS even with a smaller ROB. Anyway, I think this is a very nice paper. Good reasoning, simple solution, but works effectively.","title":"Misc"},{"location":"notes/paper_perf_shadows/#put-it-all-together","text":"Both runahead and helper thread were proposed to do prefetch. But they have a key difference. Runahead is invoked in the same core , and is invoked when ROB is full (not always though). Helper thread is invoked at another core . Besides, runahead can just fetch the instructions and run, no need to cook another code slice. But for helper thread, it needs to extract a code slice that will run on another core. I think the most important thing is to realize their insight. In the most straightforward and plain way: they are trying to get the most out of the otherwise-wasted resource. For example, in runahead, they realize that with some help, the CPU is still able to generate cache misses even if the instruction table is full. For precomputation, obviously it is using the other idle cores. The simple insight itself is not interesting enough, usually where it\u2019s applied make things quite interesting.","title":"Put it all together"},{"location":"notes/paper_perf_shadows/#microseconds","text":"Fill me in","title":"Microseconds"},{"location":"notes/paper_perf_shadows/#milliseconds","text":"Sleep. And wake me up when september ends. And this seems to be enough. ;-) This is true for OS to handle slow HDD and slow network.","title":"Milliseconds"},{"location":"notes/proc/","text":"Special Files \u00b6 /sys/devices/system/<name> Creation: subsys_system_register() , @ drivers/base/bus.c Note that this subdirectory is a legacy. Newer stuffer are added into other folders inside /sys . /sys/devices/system/cpu/* , @ drivers/base/cpu.c Root Object is cpu_root_attrs . The online file belongs to another sub-object And this register_cpu() function is used to setup the directories for each cpu. Many applications use /sys/devices/system/cpu/online to get the number of available CPUs. And it\u2019s hard to change this behavior because it\u2019s usually encoded inside glibc. Thus, if you want to \u201chide\u201d certain CPUs from applications for some reason, you can write a kernel module that use set_cpu_active(cpu, false) , and then use the following small patch. (Note that using set_cpu_online(cpu, false) will confuse CPU idle routine and panic.) diff --git a/drivers/base/cpu.c b/drivers/base/cpu.c --- a/drivers/base/cpu.c +++ b/drivers/base/cpu.c @@ -220,7 +220,8 @@ static ssize_t show_cpus_attr(struct device *dev, /* Keep in sync with cpu_subsys_attrs */ static struct cpu_attr cpu_attrs[] = { - _CPU_ATTR(online, &__cpu_online_mask), + _CPU_ATTR(online, &__cpu_active_mask), _CPU_ATTR(possible, &__cpu_possible_mask), _CPU_ATTR(present, &__cpu_present_mask), }; /proc/pressure https://lwn.net/Articles/759658/ \u2013 Yizhou Shan Created: Jul 26, 2019 Last Updated: Aug 03, 2019","title":"Linux-Special-Files"},{"location":"notes/proc/#special-files","text":"/sys/devices/system/<name> Creation: subsys_system_register() , @ drivers/base/bus.c Note that this subdirectory is a legacy. Newer stuffer are added into other folders inside /sys . /sys/devices/system/cpu/* , @ drivers/base/cpu.c Root Object is cpu_root_attrs . The online file belongs to another sub-object And this register_cpu() function is used to setup the directories for each cpu. Many applications use /sys/devices/system/cpu/online to get the number of available CPUs. And it\u2019s hard to change this behavior because it\u2019s usually encoded inside glibc. Thus, if you want to \u201chide\u201d certain CPUs from applications for some reason, you can write a kernel module that use set_cpu_active(cpu, false) , and then use the following small patch. (Note that using set_cpu_online(cpu, false) will confuse CPU idle routine and panic.) diff --git a/drivers/base/cpu.c b/drivers/base/cpu.c --- a/drivers/base/cpu.c +++ b/drivers/base/cpu.c @@ -220,7 +220,8 @@ static ssize_t show_cpus_attr(struct device *dev, /* Keep in sync with cpu_subsys_attrs */ static struct cpu_attr cpu_attrs[] = { - _CPU_ATTR(online, &__cpu_online_mask), + _CPU_ATTR(online, &__cpu_active_mask), _CPU_ATTR(possible, &__cpu_possible_mask), _CPU_ATTR(present, &__cpu_present_mask), }; /proc/pressure https://lwn.net/Articles/759658/ \u2013 Yizhou Shan Created: Jul 26, 2019 Last Updated: Aug 03, 2019","title":"Special Files"},{"location":"notes/program_advice/","text":"Programming and Writing Advice \u00b6 Version History Date Description Mar 28, 2020 Started. FreeBSD \u00b6 Quote Source . Our ideology can be described by the following guidelines: Do not add new functionality unless an implementor cannot complete a real application without it. It is as important to decide what a system is not as to decide what it is. Do not serve all the world\u2019s needs; rather, make the system extensible so that additional needs can be met in an upwardly compatible fashion. The only thing worse than generalizing from one example is generalizing from no examples at all. If a problem is not completely understood, it is probably best to provide no solution at all. If you can get 90 percent of the desired effect for 10 percent of the work, use the simpler solution. Isolate complexity as much as possible. Provide mechanism, rather than policy. In particular, place user interface policy in the client\u2019s hands. From Scheifler & Gettys: \u201cX Window System\u201d Prof. John Ousterhout\u2019s Favorite Sayings \u00b6 Quote Source : The greatest performance improvement of all is when a system goes from not-working to working Use your intuition to ask questions, not to answer them The most important component of evolution is death Facts precede concepts If you don\u2019t know what the problem was, you haven\u2019t fixed it If it hasn\u2019t been used, it doesn\u2019t work The only thing worse than a problem that happens all the time is a problem that doesn\u2019t happen all the time The three most powerful words for building credibility are \u201cI don\u2019t know\u201d Coherent systems are inherently unstable Butler W. Lampson\u2019s Hints for Computer System Design \u00b6 https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf Performance Evaluation \u00b6 Systems Benchmarking Crimes Tim Harris: designing experiments for understanding performance Very practical and useful suggestions. Writing Tips \u00b6 Plain Writing Act of 2010, Federal Plain Language Guidelines. . Dash Writing Tips","title":"Guideline & Advice"},{"location":"notes/program_advice/#programming-and-writing-advice","text":"Version History Date Description Mar 28, 2020 Started.","title":"Programming and Writing Advice"},{"location":"notes/program_advice/#freebsd","text":"Quote Source . Our ideology can be described by the following guidelines: Do not add new functionality unless an implementor cannot complete a real application without it. It is as important to decide what a system is not as to decide what it is. Do not serve all the world\u2019s needs; rather, make the system extensible so that additional needs can be met in an upwardly compatible fashion. The only thing worse than generalizing from one example is generalizing from no examples at all. If a problem is not completely understood, it is probably best to provide no solution at all. If you can get 90 percent of the desired effect for 10 percent of the work, use the simpler solution. Isolate complexity as much as possible. Provide mechanism, rather than policy. In particular, place user interface policy in the client\u2019s hands. From Scheifler & Gettys: \u201cX Window System\u201d","title":"FreeBSD"},{"location":"notes/program_advice/#prof-john-ousterhouts-favorite-sayings","text":"Quote Source : The greatest performance improvement of all is when a system goes from not-working to working Use your intuition to ask questions, not to answer them The most important component of evolution is death Facts precede concepts If you don\u2019t know what the problem was, you haven\u2019t fixed it If it hasn\u2019t been used, it doesn\u2019t work The only thing worse than a problem that happens all the time is a problem that doesn\u2019t happen all the time The three most powerful words for building credibility are \u201cI don\u2019t know\u201d Coherent systems are inherently unstable","title":"Prof. John Ousterhout's Favorite Sayings"},{"location":"notes/program_advice/#butler-w-lampsons-hints-for-computer-system-design","text":"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf","title":"Butler W. Lampson's Hints for Computer System Design"},{"location":"notes/program_advice/#performance-evaluation","text":"Systems Benchmarking Crimes Tim Harris: designing experiments for understanding performance Very practical and useful suggestions.","title":"Performance Evaluation"},{"location":"notes/program_advice/#writing-tips","text":"Plain Writing Act of 2010, Federal Plain Language Guidelines. . Dash Writing Tips","title":"Writing Tips"},{"location":"notes/rmap/","text":"Linux Reverse Map \u00b6 Read those carefully, you will understand: PDF: Object-based Reverse Mapping LWN: Virtual Memory II: the return of objrmap LWN: The object-based reverse-mapping VM I used to implement the basic PTE-chain based rmap for LegoOS . I can see the downsides of it. I tried to understand the linux rmap before, somehow gave up because I couldn\u2019t fully understand one thing: for a page that is shared among multiple processes\u2019 VMAs, the source code suggests it will always have same offset from the beginning of all VMA (i.e., vm_start ). But does it actually works like this for ALL cases? I just think it\u2019s possible that a page is mapped by an VMA which has a slightly different starting address. I still have doubt about it. But after accepting this assumption, it\u2019s just easy to understand. I will check later on. The code suggests: The offset of a page is saved in page->index . For anonmouys pages, the page->index is saved by page_set_anon_rmap() . When doing rmap walk over multiple VMAs: For anon : unsigned long address = vma_address(page, vma); For file : unsigned long address = vma_address(page, vma); And vma_address() is basically page->index static inline unsigned long __vma_address ( struct page * page , struct vm_area_struct * vma ) { pgoff_t pgoff = page_to_pgoff ( page ); return vma -> vm_start + (( pgoff - vma -> vm_pgoff ) << PAGE_SHIFT ); } Compared to basic PTE-chain based solution, object-based rmap: The real benefit During page fault, we only need to set page->mapping to point to struct anon_vma , rather than allocating a new structure and insert. The downside During rmap walk, we need extra computation to walk each VMA\u2019s page table to make sure that the page is actually mapped within this specific VMA. Adding struct anon_vma is really similar to the idea of reusing address_space , i.e., having a data structure trampoline. Some more boring details: All pages within a single VMA share just one anon_vma . vma->anon_vma indicates if a VMA has attached or note. Related function is anon_vma_prepare() within do_anonymous_fault() 1 . \u2013 Yizhou Shan Created: Jun 16, 2019 Last Updated: Jun 17, 2019","title":"Linux-Reverse-Map(rmap)"},{"location":"notes/rmap/#linux-reverse-map","text":"Read those carefully, you will understand: PDF: Object-based Reverse Mapping LWN: Virtual Memory II: the return of objrmap LWN: The object-based reverse-mapping VM I used to implement the basic PTE-chain based rmap for LegoOS . I can see the downsides of it. I tried to understand the linux rmap before, somehow gave up because I couldn\u2019t fully understand one thing: for a page that is shared among multiple processes\u2019 VMAs, the source code suggests it will always have same offset from the beginning of all VMA (i.e., vm_start ). But does it actually works like this for ALL cases? I just think it\u2019s possible that a page is mapped by an VMA which has a slightly different starting address. I still have doubt about it. But after accepting this assumption, it\u2019s just easy to understand. I will check later on. The code suggests: The offset of a page is saved in page->index . For anonmouys pages, the page->index is saved by page_set_anon_rmap() . When doing rmap walk over multiple VMAs: For anon : unsigned long address = vma_address(page, vma); For file : unsigned long address = vma_address(page, vma); And vma_address() is basically page->index static inline unsigned long __vma_address ( struct page * page , struct vm_area_struct * vma ) { pgoff_t pgoff = page_to_pgoff ( page ); return vma -> vm_start + (( pgoff - vma -> vm_pgoff ) << PAGE_SHIFT ); } Compared to basic PTE-chain based solution, object-based rmap: The real benefit During page fault, we only need to set page->mapping to point to struct anon_vma , rather than allocating a new structure and insert. The downside During rmap walk, we need extra computation to walk each VMA\u2019s page table to make sure that the page is actually mapped within this specific VMA. Adding struct anon_vma is really similar to the idea of reusing address_space , i.e., having a data structure trampoline. Some more boring details: All pages within a single VMA share just one anon_vma . vma->anon_vma indicates if a VMA has attached or note. Related function is anon_vma_prepare() within do_anonymous_fault() 1 . \u2013 Yizhou Shan Created: Jun 16, 2019 Last Updated: Jun 17, 2019","title":"Linux Reverse Map"},{"location":"notes/trace/","text":"Linux Tracing \u00b6 Version History Date Description Sep 6, 2020 Add more eBPF Jun 10, 2019 Initial version Notes about on various tracers and profilers from the Linux kernel. Also links to some previous notes about the profilers/tracers in LegoOS: notes , and profile points . Under Linux development envionment, we have: ftrace kprobe uprobe perf_event tracepoints eBPF For all these tools, I tend to think this way: Tracing needs two parts, 1) Mechanims to get data and do callback. This means we need a way to let our tracing/profiling code got invoked on a running system. This can be static or dynamic. Static means we added our tracing code to source code, like tracepoints. Dynamic means we added our tracing code when system is running, like ftrace and kprobe. 2) Do our stuff within callback. All of them provide some sort of handling. But eBPF is the most extensive one. For example, ftrace , kprobe , and perf_event include the callback facilities, although they are not just limited to this. ftrace has the call mount way to do callback on every single function invocation. kprobe dynamically patch instructions and to do callback within exception handlers. perf_event can let CPU fire NMI interrupt. Those are all mechanisms to catch perf data. In all, ftrace, kprobe, uprobe, perf_event, tracepoints all have mechanisms to get data and do callback. ftrace is not programmable by normal users, it only prints the info. kprobe allows us to attach customized pre-/post-handlers. perf_event is not programmable, it only reports numbers. Unlike all above subsystems, eBPF itself cannot intercept any programs, but it can be attached to any of the above probes and run customized programs. That\u2019s why eBPF looks so versatile! The BPF Performance Tools book section 2 also takes a deep dive into this topic, and it links all subsystems together with a bit of history as well. Also see the blog from Julia: Linux tracing systems & how they fit together . ftrace \u00b6 Mechanism For each un-inlined function, gcc inserts a call mcount , or a call fentry instruction at the very beginning. This means whenever a function is called, the mcount() or the fentry() callback will be invoked. Having these call instructions introduce a lot overheads. So by default kernel replace call with nop . Only after we echo something > setup_filter_functions will the ftrace code replace nop with call . Do note, Linux uses the linker magic again here, check Steven\u2019s slides. You can do a objdump vmlinux -d , and able to see the following instructions for almost all functions: callq ffffffff81a01560 <__fentry__> . x86 related code: arch/x86/kernel/ftrace_64.S , arch/x86/kernel/ftrace.c Questions: it seems we can know when a function got called by using fentry, but how can we know the function has returned? The trick is: the returning address is pushed to stack when a function got called. So ftrace, again, can replace that return address, so it can catch the exit time, and calculate the latency of a function. Neat!! Resources ftrace internal from Steven Usage Files under /sys/kernel/debug/tracing/* perf help ftrace kprobe \u00b6 Mechanism Kprobe replaces the original assembly instruction with an int3 trap instruction. So when we ran into the PC of the original instruction, an int3 CPU exception will happen. Within do_in3() , kernel will callback to core kprobe layer to do pre-handler . After singlestep, CPU have debug exception. Kernel walks into do_debug() , where kprobe run post-handler . Kprobe is powerful, because it\u2019s able to trace almost everything at instruction level. Kprobe can NOT touch things inside entry.S . It needs a valid pt_regs to operate. Resources An introduction to kprobes (LWN) eBPF \u00b6 Mechanism I think one of the most important things is to understand what\u2019s the relationship between eBPF and the others. Part I: Hook . eBPF attach its program to kprobe/uprobe/ftrace/perf_event. You can think eBPF of a generic callback layer for kprobe/uprobe/ftrace/perf_event. It\u2019s essentially the second part of tracing we mentioned above. (see include/uapi/linux/bpf.h , you can find BPF_PROG_TYPE_KPROBE , BPF_PROG_TYPE_PERF_EVENT ) Part II: Run . eBPF runs user supplied programs when the above hooks are invoked. eBPF is event-driven. Usually as a user, we do not need to write and load eBPF programs directly. That process is quite intense, you need to compile programs into eBPF bytecode, and then use eBPF SYSCALL to load into kernel. Quite a lot higher-level frameworks have been introduced. For example, bcc a layer on top of raw eBPF and smooth the process. bpftrace is even a layer higher than bcc, where users can write scripts to control eBPF. There are more frameworks on this space. Once you understand how it works below, it is not hard to understand and use high-level frameworks. Resources Brendan D. Gregg Blog Github: Awesome-eBPF Cilium: BPF and XDP Reference Guide Blog: An eBPF overview bcc bpftrace perf \u00b6 perf tool is simply amazing. It not only use CPU PMU, but also integrated with ftrace/kprobe/eBPF. perf is a tool to present data, but also a tool to collect data. Good references http://www.brendangregg.com/perf.html https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/ https://opensource.com/article/18/7/fun-perf-and-python Trace in real time: Print the number of page faults happen in every one second: perf stat -e \"page-faults\" -I 1000 -a -- sleep 10 Print the numberf of mmap syscall happen in every one second: perf stat -e \"syscalls:sys_enter_mmap\" -I 1000 -a -- sleep 10 Dynamically trace kernel functions: perf probe --add do_anonymous_page perf stat -I 5000 -e \"page-faults,probe:do_anonymous_page\" -- sleep 10 perf probe --del = probe:do_anonymous_page","title":"Linux-Trace-and-Profile"},{"location":"notes/trace/#linux-tracing","text":"Version History Date Description Sep 6, 2020 Add more eBPF Jun 10, 2019 Initial version Notes about on various tracers and profilers from the Linux kernel. Also links to some previous notes about the profilers/tracers in LegoOS: notes , and profile points . Under Linux development envionment, we have: ftrace kprobe uprobe perf_event tracepoints eBPF For all these tools, I tend to think this way: Tracing needs two parts, 1) Mechanims to get data and do callback. This means we need a way to let our tracing/profiling code got invoked on a running system. This can be static or dynamic. Static means we added our tracing code to source code, like tracepoints. Dynamic means we added our tracing code when system is running, like ftrace and kprobe. 2) Do our stuff within callback. All of them provide some sort of handling. But eBPF is the most extensive one. For example, ftrace , kprobe , and perf_event include the callback facilities, although they are not just limited to this. ftrace has the call mount way to do callback on every single function invocation. kprobe dynamically patch instructions and to do callback within exception handlers. perf_event can let CPU fire NMI interrupt. Those are all mechanisms to catch perf data. In all, ftrace, kprobe, uprobe, perf_event, tracepoints all have mechanisms to get data and do callback. ftrace is not programmable by normal users, it only prints the info. kprobe allows us to attach customized pre-/post-handlers. perf_event is not programmable, it only reports numbers. Unlike all above subsystems, eBPF itself cannot intercept any programs, but it can be attached to any of the above probes and run customized programs. That\u2019s why eBPF looks so versatile! The BPF Performance Tools book section 2 also takes a deep dive into this topic, and it links all subsystems together with a bit of history as well. Also see the blog from Julia: Linux tracing systems & how they fit together .","title":"Linux Tracing"},{"location":"notes/trace/#ftrace","text":"Mechanism For each un-inlined function, gcc inserts a call mcount , or a call fentry instruction at the very beginning. This means whenever a function is called, the mcount() or the fentry() callback will be invoked. Having these call instructions introduce a lot overheads. So by default kernel replace call with nop . Only after we echo something > setup_filter_functions will the ftrace code replace nop with call . Do note, Linux uses the linker magic again here, check Steven\u2019s slides. You can do a objdump vmlinux -d , and able to see the following instructions for almost all functions: callq ffffffff81a01560 <__fentry__> . x86 related code: arch/x86/kernel/ftrace_64.S , arch/x86/kernel/ftrace.c Questions: it seems we can know when a function got called by using fentry, but how can we know the function has returned? The trick is: the returning address is pushed to stack when a function got called. So ftrace, again, can replace that return address, so it can catch the exit time, and calculate the latency of a function. Neat!! Resources ftrace internal from Steven Usage Files under /sys/kernel/debug/tracing/* perf help ftrace","title":"ftrace"},{"location":"notes/trace/#kprobe","text":"Mechanism Kprobe replaces the original assembly instruction with an int3 trap instruction. So when we ran into the PC of the original instruction, an int3 CPU exception will happen. Within do_in3() , kernel will callback to core kprobe layer to do pre-handler . After singlestep, CPU have debug exception. Kernel walks into do_debug() , where kprobe run post-handler . Kprobe is powerful, because it\u2019s able to trace almost everything at instruction level. Kprobe can NOT touch things inside entry.S . It needs a valid pt_regs to operate. Resources An introduction to kprobes (LWN)","title":"kprobe"},{"location":"notes/trace/#ebpf","text":"Mechanism I think one of the most important things is to understand what\u2019s the relationship between eBPF and the others. Part I: Hook . eBPF attach its program to kprobe/uprobe/ftrace/perf_event. You can think eBPF of a generic callback layer for kprobe/uprobe/ftrace/perf_event. It\u2019s essentially the second part of tracing we mentioned above. (see include/uapi/linux/bpf.h , you can find BPF_PROG_TYPE_KPROBE , BPF_PROG_TYPE_PERF_EVENT ) Part II: Run . eBPF runs user supplied programs when the above hooks are invoked. eBPF is event-driven. Usually as a user, we do not need to write and load eBPF programs directly. That process is quite intense, you need to compile programs into eBPF bytecode, and then use eBPF SYSCALL to load into kernel. Quite a lot higher-level frameworks have been introduced. For example, bcc a layer on top of raw eBPF and smooth the process. bpftrace is even a layer higher than bcc, where users can write scripts to control eBPF. There are more frameworks on this space. Once you understand how it works below, it is not hard to understand and use high-level frameworks. Resources Brendan D. Gregg Blog Github: Awesome-eBPF Cilium: BPF and XDP Reference Guide Blog: An eBPF overview bcc bpftrace","title":"eBPF"},{"location":"notes/trace/#perf","text":"perf tool is simply amazing. It not only use CPU PMU, but also integrated with ftrace/kprobe/eBPF. perf is a tool to present data, but also a tool to collect data. Good references http://www.brendangregg.com/perf.html https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/ https://opensource.com/article/18/7/fun-perf-and-python Trace in real time: Print the number of page faults happen in every one second: perf stat -e \"page-faults\" -I 1000 -a -- sleep 10 Print the numberf of mmap syscall happen in every one second: perf stat -e \"syscalls:sys_enter_mmap\" -I 1000 -a -- sleep 10 Dynamically trace kernel functions: perf probe --add do_anonymous_page perf stat -I 5000 -e \"page-faults,probe:do_anonymous_page\" -- sleep 10 perf probe --del = probe:do_anonymous_page","title":"perf"},{"location":"notes/userfaultfd/","text":"Userfaultfd \u00b6 (Notes based on linux 5.2-rc3) Code Layout Major file: fs/userfaultfd.c , which has all the functions and callbacks. Callers spread across: mm/memory.c , mm/mremap.c , mm/mmap.c , and some others. The userfaultfd code is not that hard to understand if you already know how waitqueue etc work. It\u2019s built center around the file_ops , and couple callbacks for mm. handle_userfault() , called by mm/memory.c : Userfaultfd callback only happens for anonymous pgfault Userfaultfd skip all the LRU, rmap, cgroup Userfaultfd does not use the shared global zero page userfaultfd_unmap_prep(), userfaultfd_unmap_complete() , called by mm/mmap.c , and mm/mremap.c : Userfaultfd got notified if there are remap and unmap Userfaultfd deliver events via userfaultfd_event_wait_completion() I found code in mmap.c and mremap.c is NOT skipping rmap/lru code. Since userfaultfd related pages don\u2019t have these setup during pgfault, I think those rmap/lru cleanup code will notice this and handle it well. In conclusion, userfault skip the expansive rmap/lru setup/teardown. Why userfaultfd? At first developed to enhance VM migration: after migration, the destination QEMU can handle pgfault and bring pages from remote via network. Some databases also use it to have customized feature: http://tech.adroll.com/blog/data/2016/11/29/traildb-mmap-s3.html My thought? The ideal use case is very similar to we did in Hotpot: get the faulting user address, and fetch it from remote. Due to kernel limitations and security constraints, the userfaultfd has to go through many layers and multiple kernel/user crossing. It would be interesting to use user eBPF code to handle pgfault. \u2013 Yizhou Shan Created: Jun 4, 2019 Last Updated: Jun 4, 2019","title":"Linux-Userfaultfd"},{"location":"notes/userfaultfd/#userfaultfd","text":"(Notes based on linux 5.2-rc3) Code Layout Major file: fs/userfaultfd.c , which has all the functions and callbacks. Callers spread across: mm/memory.c , mm/mremap.c , mm/mmap.c , and some others. The userfaultfd code is not that hard to understand if you already know how waitqueue etc work. It\u2019s built center around the file_ops , and couple callbacks for mm. handle_userfault() , called by mm/memory.c : Userfaultfd callback only happens for anonymous pgfault Userfaultfd skip all the LRU, rmap, cgroup Userfaultfd does not use the shared global zero page userfaultfd_unmap_prep(), userfaultfd_unmap_complete() , called by mm/mmap.c , and mm/mremap.c : Userfaultfd got notified if there are remap and unmap Userfaultfd deliver events via userfaultfd_event_wait_completion() I found code in mmap.c and mremap.c is NOT skipping rmap/lru code. Since userfaultfd related pages don\u2019t have these setup during pgfault, I think those rmap/lru cleanup code will notice this and handle it well. In conclusion, userfault skip the expansive rmap/lru setup/teardown. Why userfaultfd? At first developed to enhance VM migration: after migration, the destination QEMU can handle pgfault and bring pages from remote via network. Some databases also use it to have customized feature: http://tech.adroll.com/blog/data/2016/11/29/traildb-mmap-s3.html My thought? The ideal use case is very similar to we did in Hotpot: get the faulting user address, and fetch it from remote. Due to kernel limitations and security constraints, the userfaultfd has to go through many layers and multiple kernel/user crossing. It would be interesting to use user eBPF code to handle pgfault. \u2013 Yizhou Shan Created: Jun 4, 2019 Last Updated: Jun 4, 2019","title":"Userfaultfd"},{"location":"notes/xperf/","text":"x86 Ring Switch Overhead (Page Fault version) \u00b6 Version History Date Description Feb 2, 2020 Move github content to here Aug 7, 2019 Initial draft This page describes the mechanisms to measure the pure x86 ring switch overhead, i.e., from ring 3 to ring 0 and back. It is not straightforward to measure this in Linux kernel. Because when a user program traps from user space to kernel space, kernel will first run some assembly instructions to save the registers and load some new ones for kernel usage (i.e., syscall , common IDT , and some directly registered ). And only then, the kernel will run the C code. Thus if we place the measurement code in the first C function that will run (e.g., do_syscall_64 ), it will be much larger than the actual ring switch overhead. My proposed solutions hacks the entry_64.S and tries to save a timestamp as soon as possible. The first version centers around page fault handler, whose trapping mechanism is different from syscalls. However, I think it could be easily ported. The code is here . Takeaways: It ain\u2019t cheap! It usually take ~400 cycles to trap from user to kernel space. User-to-kernel crossing is more expansive than kernel-to-user crossing! Virtilization adds more overhead The following content is adopted from the Github repo. Numbers \u00b6 The numbers reported by this repo are slightly larger than the real crossing overhead because some instructions are needed in between to do bookkeeping. Check below for details. Some preliminary numbers measured on top of Intel Xeon E5-v3 2.4GHz Platform User to Kernel (Cycles) Kernel to User (Cycles) VM ~600 ~370 Bare-metal ~440 ~270 Mechanisms \u00b6 Files changed \u00b6 The whole patch is xperf.patch arch/x86/entry/entry_64.S arch/x86/mm/fault.c : save u2k_k to user stack xperf/xperf.c : userspace test code User to kernel (u2k) \u00b6 At a high-level, the flow is: User save TSC into stack User pgfault Cross to kernel, get TSC, and save to user stack But devil is in the details, especially this low-level assembly code. There are several difficulties: Once in kernel, we need to save TSC without corrupting any other registers and memory content. Any corruption leads to panic etc. The challenge is to find somewhere to save stuff. Options are: kernel stack, user stack, per-cpu. Using user stack is dangerous, because we can\u2019t use safe probe in this assembly (i.e., copy_from/to_user()). Using kernel stack is not flexible because we need to manually find a spot above pt_regs, and this subject to number of call invoked. We need to ensure the measuring only applied to measure program, but not all user program. We let user save a MAGIC on user stack. The approach: entry_64.S : Save rax/rdx into kernel stack, because they are known to be good if the exceptions came from user space. entry_64.S : Save TSC into a per-cpu area. With swapgs surrounded. entry_64.S : Restore rax/rdx fault.c : use copy_to_user to save u2k_k in user stack. Enable/Disable: entry_64.S : Change xperf_idtentry back to idtentry for both page_fault and async_page_fault . Note: u2k hack is safe because we don\u2019t probe user virtual address directly in assembly. Userspace accessing is done via copy_from_user() . Kernel to user (k2u) \u00b6 At a high-level, the flow is: Kernel save TSC into user stack Kernel IRET Cross to user, get TSC, and calculate latency This is relatively simpiler than measuring u2k because we can safely use kernel stack. The approach: Save scratch %rax, %rdx, %rcx into kernel stack Check if MAGIC match rdtsc save to user stack restore scratch registers Enable/Disable: entry_64.S: There is a xperf_return_kernel_tsc code block. Note: k2u hack is NOT SAFE because we probe user virtual address directly in assembly, i.e., movq %rax, (%rcx) in our hack. During my experiments, sometimes it will crash, but not always. xperf/xperf.c \u00b6 This user program will report both u2k and k2u crossing numbers. After compilation, use objdump xperf.o -d to check assembly, mfence rdtsc <- u2k_u shl $ 0x20 , % rdx or % rdx , % rax mov % rax ,( % rdi ) <- save to user stack movl $ 0x12345678 ,( % rsi ) <- pgfault rdtsc <- k2u_u mfence The user stack layout upon pgfault is: | .. | | 8 B magic | ( filled by user ) + 24 | 8 B u2k_u | ( filled by user ) + 16 | 8 B u2k_k | ( filled by kernel ) + 8 | 8 B k2u_k | ( filled by kernel ) <-- % rsp TSC Measurement \u00b6 TSC will be reodered if no actions are taken. We use mfence to mimize runtime errors. Ideally, we want a test sequence like this: /* * User to Kernel * * mfence * rdtsc <- u2k_u * (user) * ------- pgfault -------- * (kernel) * rdtsc <- u2k_k * mfence */ /* * Kernel to User * * mfence * rdtsc <- k2u_k * (kernel) * ------- IRET -------- * (user) * rdtsc <- k2u_k * mfence */ But we need some instructions in between to do essential setup. So the real instruction flow is: U2K (User) mfence rdtsc <- u2k_u shl $0x20,%rdx or %rdx,%rax mov %rax,(%rdi) movl $0x12345678,(%rsi) -------------------------------- Crossing (Kernel) testb $3, CS-ORIG_RAX(%rsp) jz 1f movq %rax, -8(%rsp) movq %rdx, -16(%rsp) rdtsc <- u2k_k mfence K2U (Kernel) mfence rdtsc <- k2u_k shl $32, %rdx or %rdx, %rax movq %rax, (%rcx) popq %rcx popq %rdx popq %rax INTERRUPT_RETURN -------------------------------- Crossing (User) rdtsc <- k2u_u mfence Misc \u00b6 For VM scenario, the page fault entry point is async_page_fault , not the page_fault . HOWTO Run \u00b6 FAT NOTE: Enabling k2u code might bring crash It\u2019s not safe to disable KPTI Switch back to normal kernel after testing Make sure if you have a way to reboot your machine! Steps: Copy your current kernel\u2019s .config into this repo make oldconfig Disable CONFIG_PAGE_TABLE_ISOLATION Compile kernel and install. Reboot into new kernel Disable hugepage echo never > /sys/kernel/mm/transparent_hugepage/enabled Run xperf/xperf.c , you will get a report.","title":"x86-Ring-Switch-Overhead"},{"location":"notes/xperf/#x86-ring-switch-overhead-page-fault-version","text":"Version History Date Description Feb 2, 2020 Move github content to here Aug 7, 2019 Initial draft This page describes the mechanisms to measure the pure x86 ring switch overhead, i.e., from ring 3 to ring 0 and back. It is not straightforward to measure this in Linux kernel. Because when a user program traps from user space to kernel space, kernel will first run some assembly instructions to save the registers and load some new ones for kernel usage (i.e., syscall , common IDT , and some directly registered ). And only then, the kernel will run the C code. Thus if we place the measurement code in the first C function that will run (e.g., do_syscall_64 ), it will be much larger than the actual ring switch overhead. My proposed solutions hacks the entry_64.S and tries to save a timestamp as soon as possible. The first version centers around page fault handler, whose trapping mechanism is different from syscalls. However, I think it could be easily ported. The code is here . Takeaways: It ain\u2019t cheap! It usually take ~400 cycles to trap from user to kernel space. User-to-kernel crossing is more expansive than kernel-to-user crossing! Virtilization adds more overhead The following content is adopted from the Github repo.","title":"x86 Ring Switch Overhead (Page Fault version)"},{"location":"notes/xperf/#numbers","text":"The numbers reported by this repo are slightly larger than the real crossing overhead because some instructions are needed in between to do bookkeeping. Check below for details. Some preliminary numbers measured on top of Intel Xeon E5-v3 2.4GHz Platform User to Kernel (Cycles) Kernel to User (Cycles) VM ~600 ~370 Bare-metal ~440 ~270","title":"Numbers"},{"location":"notes/xperf/#mechanisms","text":"","title":"Mechanisms"},{"location":"notes/xperf/#files-changed","text":"The whole patch is xperf.patch arch/x86/entry/entry_64.S arch/x86/mm/fault.c : save u2k_k to user stack xperf/xperf.c : userspace test code","title":"Files changed"},{"location":"notes/xperf/#user-to-kernel-u2k","text":"At a high-level, the flow is: User save TSC into stack User pgfault Cross to kernel, get TSC, and save to user stack But devil is in the details, especially this low-level assembly code. There are several difficulties: Once in kernel, we need to save TSC without corrupting any other registers and memory content. Any corruption leads to panic etc. The challenge is to find somewhere to save stuff. Options are: kernel stack, user stack, per-cpu. Using user stack is dangerous, because we can\u2019t use safe probe in this assembly (i.e., copy_from/to_user()). Using kernel stack is not flexible because we need to manually find a spot above pt_regs, and this subject to number of call invoked. We need to ensure the measuring only applied to measure program, but not all user program. We let user save a MAGIC on user stack. The approach: entry_64.S : Save rax/rdx into kernel stack, because they are known to be good if the exceptions came from user space. entry_64.S : Save TSC into a per-cpu area. With swapgs surrounded. entry_64.S : Restore rax/rdx fault.c : use copy_to_user to save u2k_k in user stack. Enable/Disable: entry_64.S : Change xperf_idtentry back to idtentry for both page_fault and async_page_fault . Note: u2k hack is safe because we don\u2019t probe user virtual address directly in assembly. Userspace accessing is done via copy_from_user() .","title":"User to kernel (u2k)"},{"location":"notes/xperf/#kernel-to-user-k2u","text":"At a high-level, the flow is: Kernel save TSC into user stack Kernel IRET Cross to user, get TSC, and calculate latency This is relatively simpiler than measuring u2k because we can safely use kernel stack. The approach: Save scratch %rax, %rdx, %rcx into kernel stack Check if MAGIC match rdtsc save to user stack restore scratch registers Enable/Disable: entry_64.S: There is a xperf_return_kernel_tsc code block. Note: k2u hack is NOT SAFE because we probe user virtual address directly in assembly, i.e., movq %rax, (%rcx) in our hack. During my experiments, sometimes it will crash, but not always.","title":"Kernel to user (k2u)"},{"location":"notes/xperf/#xperfxperfc","text":"This user program will report both u2k and k2u crossing numbers. After compilation, use objdump xperf.o -d to check assembly, mfence rdtsc <- u2k_u shl $ 0x20 , % rdx or % rdx , % rax mov % rax ,( % rdi ) <- save to user stack movl $ 0x12345678 ,( % rsi ) <- pgfault rdtsc <- k2u_u mfence The user stack layout upon pgfault is: | .. | | 8 B magic | ( filled by user ) + 24 | 8 B u2k_u | ( filled by user ) + 16 | 8 B u2k_k | ( filled by kernel ) + 8 | 8 B k2u_k | ( filled by kernel ) <-- % rsp","title":"xperf/xperf.c"},{"location":"notes/xperf/#tsc-measurement","text":"TSC will be reodered if no actions are taken. We use mfence to mimize runtime errors. Ideally, we want a test sequence like this: /* * User to Kernel * * mfence * rdtsc <- u2k_u * (user) * ------- pgfault -------- * (kernel) * rdtsc <- u2k_k * mfence */ /* * Kernel to User * * mfence * rdtsc <- k2u_k * (kernel) * ------- IRET -------- * (user) * rdtsc <- k2u_k * mfence */ But we need some instructions in between to do essential setup. So the real instruction flow is: U2K (User) mfence rdtsc <- u2k_u shl $0x20,%rdx or %rdx,%rax mov %rax,(%rdi) movl $0x12345678,(%rsi) -------------------------------- Crossing (Kernel) testb $3, CS-ORIG_RAX(%rsp) jz 1f movq %rax, -8(%rsp) movq %rdx, -16(%rsp) rdtsc <- u2k_k mfence K2U (Kernel) mfence rdtsc <- k2u_k shl $32, %rdx or %rdx, %rax movq %rax, (%rcx) popq %rcx popq %rdx popq %rax INTERRUPT_RETURN -------------------------------- Crossing (User) rdtsc <- k2u_u mfence","title":"TSC Measurement"},{"location":"notes/xperf/#misc","text":"For VM scenario, the page fault entry point is async_page_fault , not the page_fault .","title":"Misc"},{"location":"notes/xperf/#howto-run","text":"FAT NOTE: Enabling k2u code might bring crash It\u2019s not safe to disable KPTI Switch back to normal kernel after testing Make sure if you have a way to reboot your machine! Steps: Copy your current kernel\u2019s .config into this repo make oldconfig Disable CONFIG_PAGE_TABLE_ISOLATION Compile kernel and install. Reboot into new kernel Disable hugepage echo never > /sys/kernel/mm/transparent_hugepage/enabled Run xperf/xperf.c , you will get a report.","title":"HOWTO Run"},{"location":"notes/source_code/20200501-on-graphic-softwares/","text":"On Unix Graphic Softwares \u00b6 Version History Date Description Dec 4, 2020 Add high level libaries May 1, 2020 Initial Version Part I \u00b6 For work reason, I use VNC a lot recently. I need to login into our lab\u2019s servers and perform intensive graphic operations. Somehow I\u2019m not a fan of GUI-based systems, but it really got me wonder: how VNC works? Or, how graphics/GUI works in general? So I decided to look it up. The whole thing was very complex to me at the beginning. There are numerous layers of systems, and it not clear who is doing what. After getting a better understanding, I realize it is \u201cdo one thing and do it well\u201d works at its best: Each layer of the graphic stack is doing what it is supposed to do, nothing more and nothing less. Even though the line blurred over the years (i think), the principle persists. I like it. I\u2019m no where near explaing the whole thing well (still a bit confused myself :)). But you can find awesome references at: 1) Wiki Display Server , 2) Wayland Architecture 3) StackExchange Difference between Xorg and Gnome/KDE/Xfce 4) https://en.wikipedia.org/wiki/Free_and_open-source_graphics_device_driver Following are some figures I drew to show the architecture of all these softwares. In the graphic world, kernel\u2019s involvement is minimal, but a critical one. Kernel mainly need to deliver mouse/keyboard events, render frames via graphic cards, handle network. In other words, kernel provides a mechanism. The policy is left to userspace stacks. At the lowest level, we have Display Manager, or Display Server. Downstream, this layer interact with kernel, i.e., getting keyboard/mouse events from kernel evdev framework, rendering frames via DRM interfaces. Upstream, this layer accepts request from their clients (i.e., the widget layer) and make them happen in real displays. Typical systems at this layer are X.org server and Wayland. They follow the client-server model, communication has a certain protocol and is via socket (I guess?). Next up, is the widget toolkit , or UX library layer. The famous GTK/Qt belong to this layer. What this layer is doing? So this one is a collection of widgets, like buttons, menu, dropdown, i.e., GUI elements. Both GTK/Qt offer a lot such stuff (if you are using GNOME desktop, try run gtk3-widget-factory ). This layer ask the display manager layer (e.g. X.org server) to display stuff. GNOME/KDE are desktop envionment , they present the whole desktop experience, it includes many applications built based on GTK and Qt, respectively. You probably have seen gnome-shell , yup, this is GNOME\u2019s main program. The highest layer is user applications, like Chrome (which by default uses GTK on linux , code on ui/gtk ). All these linux GUI applications, they are usually built on top of either GTK or Qt\u2019s libraries. That being said, if you want to develop GUI-based apps on Linux, chances are, you will use either of the libraries. This is a landscape overview: But how VNC fits into the big picture? In short, VNC sits in the middle between X and GTK/Qt. On one hand, VNC appears as a client of X. On the other, VNC appears as an X server to GTK/Qt. Middleman works at its best lol. There are, however, many different implementation choices. If you have used TigerVNC, which in turn uses Xvnc, its man page says: Xvnc is the X VNC (Virtual Network Computing) server. It is based on a standard X server, but it has a \u201cvirtual\u201d screen rather than a physical one. X applications display themselves on it as if it were a normal X display, but they can only be accessed via a VNC viewer - see vncviewer(1). So Xvnc is really two servers in one. To the applications it is an X server, and to the remote VNC users it is a VNC server. Thus it looks like this with VNC: A machine have multiple such instances, thus multiple virtual and physical display can coexist. And for that, I think it\u2019s all because of the clear separation of layers and good engineering (man, those graphic framebuffer code is monstrous): This post remind me of \u201cWhat happens when you type google.com into your browser and press enter?\u201d ? As always, hope you enjoyed this blog. Part II \u00b6 This part wants to look at those high-level libaries used by developers every day. I somewhat got interested when I started playing Steam games and saw \u201cVulkan Shaders\u201d. Vulkan is an alternative system to OpenCL/Direct3D. Instead of hiding details, Vulkan expose quite a lot low-level details and let programmers do the tuning. So, what\u2019s the difference between Vulkan/OpenCL/Direct3D with gtk/Qt? I guess the former is for graphic development, any shape. While the latter is some predefined gadgets and a framework for developing standadrd GUI apps?","title":"Graphics"},{"location":"notes/source_code/20200501-on-graphic-softwares/#on-unix-graphic-softwares","text":"Version History Date Description Dec 4, 2020 Add high level libaries May 1, 2020 Initial Version","title":"On Unix Graphic Softwares"},{"location":"notes/source_code/20200501-on-graphic-softwares/#part-i","text":"For work reason, I use VNC a lot recently. I need to login into our lab\u2019s servers and perform intensive graphic operations. Somehow I\u2019m not a fan of GUI-based systems, but it really got me wonder: how VNC works? Or, how graphics/GUI works in general? So I decided to look it up. The whole thing was very complex to me at the beginning. There are numerous layers of systems, and it not clear who is doing what. After getting a better understanding, I realize it is \u201cdo one thing and do it well\u201d works at its best: Each layer of the graphic stack is doing what it is supposed to do, nothing more and nothing less. Even though the line blurred over the years (i think), the principle persists. I like it. I\u2019m no where near explaing the whole thing well (still a bit confused myself :)). But you can find awesome references at: 1) Wiki Display Server , 2) Wayland Architecture 3) StackExchange Difference between Xorg and Gnome/KDE/Xfce 4) https://en.wikipedia.org/wiki/Free_and_open-source_graphics_device_driver Following are some figures I drew to show the architecture of all these softwares. In the graphic world, kernel\u2019s involvement is minimal, but a critical one. Kernel mainly need to deliver mouse/keyboard events, render frames via graphic cards, handle network. In other words, kernel provides a mechanism. The policy is left to userspace stacks. At the lowest level, we have Display Manager, or Display Server. Downstream, this layer interact with kernel, i.e., getting keyboard/mouse events from kernel evdev framework, rendering frames via DRM interfaces. Upstream, this layer accepts request from their clients (i.e., the widget layer) and make them happen in real displays. Typical systems at this layer are X.org server and Wayland. They follow the client-server model, communication has a certain protocol and is via socket (I guess?). Next up, is the widget toolkit , or UX library layer. The famous GTK/Qt belong to this layer. What this layer is doing? So this one is a collection of widgets, like buttons, menu, dropdown, i.e., GUI elements. Both GTK/Qt offer a lot such stuff (if you are using GNOME desktop, try run gtk3-widget-factory ). This layer ask the display manager layer (e.g. X.org server) to display stuff. GNOME/KDE are desktop envionment , they present the whole desktop experience, it includes many applications built based on GTK and Qt, respectively. You probably have seen gnome-shell , yup, this is GNOME\u2019s main program. The highest layer is user applications, like Chrome (which by default uses GTK on linux , code on ui/gtk ). All these linux GUI applications, they are usually built on top of either GTK or Qt\u2019s libraries. That being said, if you want to develop GUI-based apps on Linux, chances are, you will use either of the libraries. This is a landscape overview: But how VNC fits into the big picture? In short, VNC sits in the middle between X and GTK/Qt. On one hand, VNC appears as a client of X. On the other, VNC appears as an X server to GTK/Qt. Middleman works at its best lol. There are, however, many different implementation choices. If you have used TigerVNC, which in turn uses Xvnc, its man page says: Xvnc is the X VNC (Virtual Network Computing) server. It is based on a standard X server, but it has a \u201cvirtual\u201d screen rather than a physical one. X applications display themselves on it as if it were a normal X display, but they can only be accessed via a VNC viewer - see vncviewer(1). So Xvnc is really two servers in one. To the applications it is an X server, and to the remote VNC users it is a VNC server. Thus it looks like this with VNC: A machine have multiple such instances, thus multiple virtual and physical display can coexist. And for that, I think it\u2019s all because of the clear separation of layers and good engineering (man, those graphic framebuffer code is monstrous): This post remind me of \u201cWhat happens when you type google.com into your browser and press enter?\u201d ? As always, hope you enjoyed this blog.","title":"Part I"},{"location":"notes/source_code/20200501-on-graphic-softwares/#part-ii","text":"This part wants to look at those high-level libaries used by developers every day. I somewhat got interested when I started playing Steam games and saw \u201cVulkan Shaders\u201d. Vulkan is an alternative system to OpenCL/Direct3D. Instead of hiding details, Vulkan expose quite a lot low-level details and let programmers do the tuning. So, what\u2019s the difference between Vulkan/OpenCL/Direct3D with gtk/Qt? I guess the former is for graphic development, any shape. While the latter is some predefined gadgets and a framework for developing standadrd GUI apps?","title":"Part II"},{"location":"notes/source_code/commands/","text":"Terminal Commands \u00b6 Version History Date Description Dec 23, 2020 extracted from the summary doc Essential Unix Commands \u00b6 The following repos have the essential UNIX commands like ls, cat, demsg. I don\u2019t think it is a good idea to blindly read the source code. Rather, I think they should be used as references whenever we need to check how something is implemented. Large Collections BusyBox GNU Coreutils util-linux FreeBSD strace System call tracer at userspace I\u2019ve designed one for LegoOS in kernel space Network \u00b6 iperf3 is a TCP, UDP, and SCTP network bandwidth measurement tool arping tcpdump OpenSSH is our ssh! scapy : Python-based interactive packet manipulation program & library. Very neat tcpstat : C-based simple tool that could dump network traffic. Seems using pcap interface, the one used by tcpdump? Also checkout FreeBSD as it has tools like ifconfig , if and many more Misc \u00b6 Tools tmux git FFmpeg FFmpeg project is famous for its clean and neat C code. This project is used by a lot online video service companies CRIU: Checkpoint and Restore in Userspace The reason I love this repo is because it has so many interesting pieces on how to interact with kernel, save states, and restore them. In addition, it shows how to properly use many less well known syscalls. GRUB2: bootloader Learn how modern bootloader works. Detailed analysis of Linux booting sequence (how it transit from real-mode to protected mode, and finally to 64-bit mode, how to navigate Linux source code etc.) Editors vim neovim Libraries \u00b6 GNU glibc: libc, elf, and dynamic linker It is the default C library used by almost everyone It includes ld.so , the dynamic linker I wrote some notes about GOT/PLT and explains what has happend before main() is called. GNU binutils: gas, static linker, and more This repo has a lot commands like as , ld , objdump , nm and so on ld is static linker and I like the magic of its linker script I guess another useful repo is elfutils C Library GNU glibc used by major Linux distributions musl libc is a small libc impl used by Alpine Linux. Clean code. uClibc is a small libc targeting embedded cases bionic is Android\u2019s C library, math library, and dynamic linker C++ Library NVIDIA libcu++","title":"Terminal Commands"},{"location":"notes/source_code/commands/#terminal-commands","text":"Version History Date Description Dec 23, 2020 extracted from the summary doc","title":"Terminal Commands"},{"location":"notes/source_code/commands/#essential-unix-commands","text":"The following repos have the essential UNIX commands like ls, cat, demsg. I don\u2019t think it is a good idea to blindly read the source code. Rather, I think they should be used as references whenever we need to check how something is implemented. Large Collections BusyBox GNU Coreutils util-linux FreeBSD strace System call tracer at userspace I\u2019ve designed one for LegoOS in kernel space","title":"Essential Unix Commands"},{"location":"notes/source_code/commands/#network","text":"iperf3 is a TCP, UDP, and SCTP network bandwidth measurement tool arping tcpdump OpenSSH is our ssh! scapy : Python-based interactive packet manipulation program & library. Very neat tcpstat : C-based simple tool that could dump network traffic. Seems using pcap interface, the one used by tcpdump? Also checkout FreeBSD as it has tools like ifconfig , if and many more","title":"Network"},{"location":"notes/source_code/commands/#misc","text":"Tools tmux git FFmpeg FFmpeg project is famous for its clean and neat C code. This project is used by a lot online video service companies CRIU: Checkpoint and Restore in Userspace The reason I love this repo is because it has so many interesting pieces on how to interact with kernel, save states, and restore them. In addition, it shows how to properly use many less well known syscalls. GRUB2: bootloader Learn how modern bootloader works. Detailed analysis of Linux booting sequence (how it transit from real-mode to protected mode, and finally to 64-bit mode, how to navigate Linux source code etc.) Editors vim neovim","title":"Misc"},{"location":"notes/source_code/commands/#libraries","text":"GNU glibc: libc, elf, and dynamic linker It is the default C library used by almost everyone It includes ld.so , the dynamic linker I wrote some notes about GOT/PLT and explains what has happend before main() is called. GNU binutils: gas, static linker, and more This repo has a lot commands like as , ld , objdump , nm and so on ld is static linker and I like the magic of its linker script I guess another useful repo is elfutils C Library GNU glibc used by major Linux distributions musl libc is a small libc impl used by Alpine Linux. Clean code. uClibc is a small libc targeting embedded cases bionic is Android\u2019s C library, math library, and dynamic linker C++ Library NVIDIA libcu++","title":"Libraries"},{"location":"notes/source_code/dotconfigs/","text":"Dot Configs \u00b6 Version History Date Description Dec 17, 2020 started Like many others, I maintain my own dot-file repo: https://github.com/lastweek/dot-home . It helps me setup the terminal whenever I start using a new machine. I\u2019m a heavy terminal user. For whatever coding task (e.g., kernel, RDMA, FPGA, scala, C), I use terminal. I sometimes use terminal to write paper as well. There are several important tools I rely on: zsh, git, neovim, and tmux. And I\u2019m grateful for folks working on these tools and their plugins. For zsh, I use oh-my-zsh. For git, I use git alias . For tmux, I use tpm . I used to cook status line myself, but I have switched to powerline. For nvim, I use vundle . And I have several cooked keys. VIM \u00b6 I quite like my current VIM setting. I\u2019m using several popular tools: NERD Tree, NERD commenter, GitGutter, and Tagbar. I created the following mapped keys so that I could invoke them quite easily. Basically I press \\ first, and then press t , or f , or g . map \\t :TagbarToggle<Enter> => to toggle tagbar map \\f :NERDTreeToggle<CR> => to toggle nerd file tree map \\g :GitGutterLineHighlightsToggle<Enter> :GitGutterSignsToggle<Enter> => to highlight git difference Besides, I have several extra syntax files, for C, ASM, scala, and verilog. I started this when I was hacking linux kernel. It has so many new awesome macros (e.g., BUG_ON , for_each_cpu ) and I want to diffrentiate them from normal functions. So I added those after/syntax files.","title":"Terminal Configs"},{"location":"notes/source_code/dotconfigs/#dot-configs","text":"Version History Date Description Dec 17, 2020 started Like many others, I maintain my own dot-file repo: https://github.com/lastweek/dot-home . It helps me setup the terminal whenever I start using a new machine. I\u2019m a heavy terminal user. For whatever coding task (e.g., kernel, RDMA, FPGA, scala, C), I use terminal. I sometimes use terminal to write paper as well. There are several important tools I rely on: zsh, git, neovim, and tmux. And I\u2019m grateful for folks working on these tools and their plugins. For zsh, I use oh-my-zsh. For git, I use git alias . For tmux, I use tpm . I used to cook status line myself, but I have switched to powerline. For nvim, I use vundle . And I have several cooked keys.","title":"Dot Configs"},{"location":"notes/source_code/dotconfigs/#vim","text":"I quite like my current VIM setting. I\u2019m using several popular tools: NERD Tree, NERD commenter, GitGutter, and Tagbar. I created the following mapped keys so that I could invoke them quite easily. Basically I press \\ first, and then press t , or f , or g . map \\t :TagbarToggle<Enter> => to toggle tagbar map \\f :NERDTreeToggle<CR> => to toggle nerd file tree map \\g :GitGutterLineHighlightsToggle<Enter> :GitGutterSignsToggle<Enter> => to highlight git difference Besides, I have several extra syntax files, for C, ASM, scala, and verilog. I started this when I was hacking linux kernel. It has so many new awesome macros (e.g., BUG_ON , for_each_cpu ) and I want to diffrentiate them from normal functions. So I added those after/syntax files.","title":"VIM"},{"location":"notes/source_code/firmware-softwares/","text":"Open-source Firmware and Bootloaders \u00b6 Version History Date Description Dec 7, 2020 add iPXE May 6, 2020 Initial Version In this blog post, I will review the current firmware and bootloader ecosystem. Landscape \u00b6 I admire those firmware projects, maybe because that\u2019s where I started. At first I used SeaBIOS (the default one used by QEMU) to build OS. Then I came across UEFI, though I have never used it. There are a lot open-source firmware projects. I was trying to understand their relationship. After some research, I drew the following landscape figure. Bottom-up: Coreboot/Libreboot/UEFI: for motherboard init, e.g., init memory controller. UEFI/BIOS iPXE GRUB2/U-Boto: Bootloader OS Projects \u00b6 Coreboot and Libreboot Coreboot seems very interesting. It\u2019s only doing one job, which is initialize the very low-level memory controller and on-board resources. It uses cache as memory. SeaBIOS: the default BIOS used by QEMU qboot: an alternative and lightweight BIOS for QEMU Those are massive hackers, respect. My experience about BIOS is calling them while the kernel (LegoOS) is running at 16-bit. BIOS is the OS for a just-booted kernel. I remember the lower 1MB is never cleared, maybe we could invoke the BIOS at 32 or 64-bit mode? UEFI UEFI EDK II \u201cEDK II is a firmware development environment for the UEFI and UEFI Platform Initialization (PI) specifications\u201d Part of the TianoCore project, an open-source UEFI platform The Unified Extensible Firmware Interface (UEFI) is a specification that defines a software interface between an operating system and platform firmware. UEFI is designed to replace the Basic Input/Output System (BIOS) firmware interface. OVMF : OVMF is an EDK II based project to enable UEFI support for Virtual Machines. OVMF contains sample UEFI firmware for QEMU and KVM. Microsoft Project Mu, a separate fork of EDK II \u201cProject Mu is a modular adaptation of TianoCore\u2019s edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern\u201d It\u2019s homepage explains the motivation behind it. Microsoft Surface is using it. A book: Beyond BIOS Developing with the Unified Extensible Firmware Interface . Then boot loaders such as GRUB and U-Boot iPXE , network bootloader, this is an open-source version. As their website says, iPXE allows you to: boot from a web server via HTTP boot from an iSCSI SAN boot from a Fibre Channel SAN via FCoE boot from an AoE SAN boot from a wireless network boot from a wide-area network boot from an Infiniband network control the boot process with a script If you are using a normal laptop or desktop, chances are, none of those firmware is used. Normally machines are shipped with commercial firmwares. To me, I like SeaBIOS project the most. It\u2019s simple and can boot everything we need. (For example, Linux, LegoOS as well).","title":"Firmware and Boorloader"},{"location":"notes/source_code/firmware-softwares/#open-source-firmware-and-bootloaders","text":"Version History Date Description Dec 7, 2020 add iPXE May 6, 2020 Initial Version In this blog post, I will review the current firmware and bootloader ecosystem.","title":"Open-source Firmware and Bootloaders"},{"location":"notes/source_code/firmware-softwares/#landscape","text":"I admire those firmware projects, maybe because that\u2019s where I started. At first I used SeaBIOS (the default one used by QEMU) to build OS. Then I came across UEFI, though I have never used it. There are a lot open-source firmware projects. I was trying to understand their relationship. After some research, I drew the following landscape figure. Bottom-up: Coreboot/Libreboot/UEFI: for motherboard init, e.g., init memory controller. UEFI/BIOS iPXE GRUB2/U-Boto: Bootloader OS","title":"Landscape"},{"location":"notes/source_code/firmware-softwares/#projects","text":"Coreboot and Libreboot Coreboot seems very interesting. It\u2019s only doing one job, which is initialize the very low-level memory controller and on-board resources. It uses cache as memory. SeaBIOS: the default BIOS used by QEMU qboot: an alternative and lightweight BIOS for QEMU Those are massive hackers, respect. My experience about BIOS is calling them while the kernel (LegoOS) is running at 16-bit. BIOS is the OS for a just-booted kernel. I remember the lower 1MB is never cleared, maybe we could invoke the BIOS at 32 or 64-bit mode? UEFI UEFI EDK II \u201cEDK II is a firmware development environment for the UEFI and UEFI Platform Initialization (PI) specifications\u201d Part of the TianoCore project, an open-source UEFI platform The Unified Extensible Firmware Interface (UEFI) is a specification that defines a software interface between an operating system and platform firmware. UEFI is designed to replace the Basic Input/Output System (BIOS) firmware interface. OVMF : OVMF is an EDK II based project to enable UEFI support for Virtual Machines. OVMF contains sample UEFI firmware for QEMU and KVM. Microsoft Project Mu, a separate fork of EDK II \u201cProject Mu is a modular adaptation of TianoCore\u2019s edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern\u201d It\u2019s homepage explains the motivation behind it. Microsoft Surface is using it. A book: Beyond BIOS Developing with the Unified Extensible Firmware Interface . Then boot loaders such as GRUB and U-Boot iPXE , network bootloader, this is an open-source version. As their website says, iPXE allows you to: boot from a web server via HTTP boot from an iSCSI SAN boot from a Fibre Channel SAN via FCoE boot from an AoE SAN boot from a wireless network boot from a wide-area network boot from an Infiniband network control the boot process with a script If you are using a normal laptop or desktop, chances are, none of those firmware is used. Normally machines are shipped with commercial firmwares. To me, I like SeaBIOS project the most. It\u2019s simple and can boot everything we need. (For example, Linux, LegoOS as well).","title":"Projects"},{"location":"notes/source_code/fpga/","text":"FPGA \u00b6 What is HDL? Hard and Difficult Language. :) This page reflects on various FPGA projects I came across. Code \u00b6 Tools \u00b6 Languages SpinalHDL Chisel Google XLS Simulators Verilator iVerilog Misc cocotb MyHDL gtkwave Network \u00b6 Alex Forencich\u2019s Verilog Ethernet This repo includes Ethernet PHY, MAC, IP, and UDP layer IPs. It works on various boards. THE BEST choice if you are trying to connect your board to network. Written in Verilog Alex Forencich\u2019s Corundum NIC This repo is a full-fledged NIC implementation including the above Verilog-Ethernet part, DMA engines, PCIe controller, interrupts, and so on. A NIC has more features than a basic FPGA Ethenet solution. You need a NIC if you are working with host softwares, otherwise you should consider using the verilog-ethernet version. Written in Verilog TCP/IP, RoCEv2 from ETH There are several papers published using this repo. It provides the basic TCP/IP and RoCE v2 stack (StRom, EuroSys\u201819). Personally I haven\u2019t used this repo so I don\u2019t have any comments. Written in Xilinx HLS. Memory \u00b6 TODO. Partial Reconfiguration \u00b6 TODO. Soft Cores \u00b6 VexRiscv, based on SpinalHDL ZipCPU, RISC CPU, written in Verilog OpenPOWER a2i and a2o MISC \u00b6 OpenWIFI NyuziProcessor, a GPGPU Processor HDMI FPGACosmacELF, based on SpinalHDL My Story with FPGA \u00b6 Back at late 2018, I started using FPGA to do datacenter research. More specific, we used FPGA to build a disaggregated memory component, which was intended as a follow-up to our prior work LegoOS, OSDI\u201818. Along the way, our idea spin-off a bit. I started looking into building an real OS into FPGA: we tried to build sched (temporal and spacial), mm , net , and various OS functionalties into FPGA (more than a traditional FPGA shell, and other FPGA OSs that a lot of acadamic papers claim!). This experiences enriched me with all sorts of low-level FPGA knowledge. I spent quit a lot of time digging into partial reconfiguration and various hacks to avoid its limitations (see Bitstream Explained , Morphous PR , Ultrascale SSI ). This FPGA OS project did not go well and we decided to suspend it.","title":"FPGA"},{"location":"notes/source_code/fpga/#fpga","text":"What is HDL? Hard and Difficult Language. :) This page reflects on various FPGA projects I came across.","title":"FPGA"},{"location":"notes/source_code/fpga/#code","text":"","title":"Code"},{"location":"notes/source_code/fpga/#tools","text":"Languages SpinalHDL Chisel Google XLS Simulators Verilator iVerilog Misc cocotb MyHDL gtkwave","title":"Tools"},{"location":"notes/source_code/fpga/#network","text":"Alex Forencich\u2019s Verilog Ethernet This repo includes Ethernet PHY, MAC, IP, and UDP layer IPs. It works on various boards. THE BEST choice if you are trying to connect your board to network. Written in Verilog Alex Forencich\u2019s Corundum NIC This repo is a full-fledged NIC implementation including the above Verilog-Ethernet part, DMA engines, PCIe controller, interrupts, and so on. A NIC has more features than a basic FPGA Ethenet solution. You need a NIC if you are working with host softwares, otherwise you should consider using the verilog-ethernet version. Written in Verilog TCP/IP, RoCEv2 from ETH There are several papers published using this repo. It provides the basic TCP/IP and RoCE v2 stack (StRom, EuroSys\u201819). Personally I haven\u2019t used this repo so I don\u2019t have any comments. Written in Xilinx HLS.","title":"Network"},{"location":"notes/source_code/fpga/#memory","text":"TODO.","title":"Memory"},{"location":"notes/source_code/fpga/#partial-reconfiguration","text":"TODO.","title":"Partial Reconfiguration"},{"location":"notes/source_code/fpga/#soft-cores","text":"VexRiscv, based on SpinalHDL ZipCPU, RISC CPU, written in Verilog OpenPOWER a2i and a2o","title":"Soft Cores"},{"location":"notes/source_code/fpga/#misc","text":"OpenWIFI NyuziProcessor, a GPGPU Processor HDMI FPGACosmacELF, based on SpinalHDL","title":"MISC"},{"location":"notes/source_code/fpga/#my-story-with-fpga","text":"Back at late 2018, I started using FPGA to do datacenter research. More specific, we used FPGA to build a disaggregated memory component, which was intended as a follow-up to our prior work LegoOS, OSDI\u201818. Along the way, our idea spin-off a bit. I started looking into building an real OS into FPGA: we tried to build sched (temporal and spacial), mm , net , and various OS functionalties into FPGA (more than a traditional FPGA shell, and other FPGA OSs that a lot of acadamic papers claim!). This experiences enriched me with all sorts of low-level FPGA knowledge. I spent quit a lot of time digging into partial reconfiguration and various hacks to avoid its limitations (see Bitstream Explained , Morphous PR , Ultrascale SSI ). This FPGA OS project did not go well and we decided to suspend it.","title":"My Story with FPGA"},{"location":"notes/source_code/gpu/","text":"GPU \u00b6 For research purpose, I started digging into GPU for the first time. Spent some time just learning the basics. And for now just want to know how large systems are using GPUs, esp. CUDA. Just get a basic sense. Systems \u00b6 tensorflow CUDA: tensorflow/core/common_runtime/gpu quite complicated. tvm CUDA: src/runtime/cuda OpenCL: src/runtime/opencl both seem quite small. And they have documentation: https://tvm.apache.org/docs/dev/codebase_walkthrough.html?highlight=cuda . Hooray! pytorch and caffee2 CUDA: over all the places. well. nvidia cuda samples","title":"GPU"},{"location":"notes/source_code/gpu/#gpu","text":"For research purpose, I started digging into GPU for the first time. Spent some time just learning the basics. And for now just want to know how large systems are using GPUs, esp. CUDA. Just get a basic sense.","title":"GPU"},{"location":"notes/source_code/gpu/#systems","text":"tensorflow CUDA: tensorflow/core/common_runtime/gpu quite complicated. tvm CUDA: src/runtime/cuda OpenCL: src/runtime/opencl both seem quite small. And they have documentation: https://tvm.apache.org/docs/dev/codebase_walkthrough.html?highlight=cuda . Hooray! pytorch and caffee2 CUDA: over all the places. well. nvidia cuda samples","title":"Systems"},{"location":"notes/source_code/misc/","text":"Misc \u00b6 EDB Debugger \u00b6 https://github.com/eteran/edb-debugger Blender \u00b6 The 3D creation suit. https://github.com/blender/blender","title":"Misc"},{"location":"notes/source_code/misc/#misc","text":"","title":"Misc"},{"location":"notes/source_code/misc/#edb-debugger","text":"https://github.com/eteran/edb-debugger","title":"EDB Debugger"},{"location":"notes/source_code/misc/#blender","text":"The 3D creation suit. https://github.com/blender/blender","title":"Blender"},{"location":"notes/source_code/os/","text":"Awesome Operating System \u00b6 Version History Date Description Dec 18, 2020 extracted from the summary doc You know what, this page is AWESOME: https://github.com/jubalh/awesome-os . Mainstream \u00b6 Linux 0.0.1 This is the first linux source code released by Linus. Despite several designs are static or obsolete from today\u2019s point of view, it showcases a simple and elegant solution. Plan 9 OS Legendary OS. So many systems are influended by Plan 9 (e.g., Go, gVisor) illumos , a fork of the Oracle Solaris OS. seL4 Microkernel MacOS Darwin BSD BSD releases all the companion software packages along with the kernel. So there is a tighter relation between them. If you ever wondered how XXX is done, or how to get YYY from OS, this is where you can look into. FreeBSD OpenBSD NetBSD TrueOS Unikernel OSv. A lightweight unikernel. IncludeOS Rumprun Solo5. Unikernel as processes! Google Fuchsia TODO. (Image source: https://commons.wikimedia.org/wiki/File:Unix_timeline.en.svg ) Hobby \u00b6 Visopsys \u201cIt features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory\u201d BootOS Academic \u00b6 Singularity. A research OS from MSR. Very interesting one. It leverages certain PL features to write secure and dependable OS. It also allows verification. It never landed as a commercial one, but it does inspire certain follow-up works. Several old research OSes have also used certain language features to carry out security measures (e.g., V++). Linux Distribution \u00b6 Ever thought about how to go from Linux Kernel to a full Linux Distribution? Read: Linux From Scratch systemd","title":"OS"},{"location":"notes/source_code/os/#awesome-operating-system","text":"Version History Date Description Dec 18, 2020 extracted from the summary doc You know what, this page is AWESOME: https://github.com/jubalh/awesome-os .","title":"Awesome Operating System"},{"location":"notes/source_code/os/#mainstream","text":"Linux 0.0.1 This is the first linux source code released by Linus. Despite several designs are static or obsolete from today\u2019s point of view, it showcases a simple and elegant solution. Plan 9 OS Legendary OS. So many systems are influended by Plan 9 (e.g., Go, gVisor) illumos , a fork of the Oracle Solaris OS. seL4 Microkernel MacOS Darwin BSD BSD releases all the companion software packages along with the kernel. So there is a tighter relation between them. If you ever wondered how XXX is done, or how to get YYY from OS, this is where you can look into. FreeBSD OpenBSD NetBSD TrueOS Unikernel OSv. A lightweight unikernel. IncludeOS Rumprun Solo5. Unikernel as processes! Google Fuchsia TODO. (Image source: https://commons.wikimedia.org/wiki/File:Unix_timeline.en.svg )","title":"Mainstream"},{"location":"notes/source_code/os/#hobby","text":"Visopsys \u201cIt features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory\u201d BootOS","title":"Hobby"},{"location":"notes/source_code/os/#academic","text":"Singularity. A research OS from MSR. Very interesting one. It leverages certain PL features to write secure and dependable OS. It also allows verification. It never landed as a commercial one, but it does inspire certain follow-up works. Several old research OSes have also used certain language features to carry out security measures (e.g., V++).","title":"Academic"},{"location":"notes/source_code/os/#linux-distribution","text":"Ever thought about how to go from Linux Kernel to a full Linux Distribution? Read: Linux From Scratch systemd","title":"Linux Distribution"},{"location":"notes/source_code/rdma/","text":"On DPDK and RDMA Related Software \u00b6 Version History Date Description Dec 14, 2020 More on DPDK May 28, 2020 Copied from summary DPDK \u00b6 Personal Notes \u00b6 DPDK DPDK uses VFIO to directly access physical device. Just like how we directly assign device to guest OS in QEMU (AFAIK, it is different for Mellanox NICs). Even though both DPDK and RDMA bypass kernel, their control path is very different. For DPDK, there is a complete device driver in the user space, and this driver communicate with the device via MMIO. After VFIO ioctls, all data and control path bypass kernel. For rdma-core, a lot control-path IB verbs (e.g., create_pd, create_cq) communicate with kernel via Infiniband device file ioctl. And you can see all those uverb hanlders in drivers/infiniband/core/uverbs.c Those control verbs will mmap some pages between user and kernel, so all following datapath IB verbs (e.g., post_send) will just bypass kernel and talk to device MMIO directly. Although rdma-core also has some vendor-specific \u201cdrivers\u201d, but this is really different from the above DPDK\u2019s userspace PCIe driver, per se. Userspace \u201crdma-core\u201d vendor-driver deals with the kernel devel vendor-level driver details. FWIW, if you are using a Mellanox VPI card in Ethernet mode (e.g. CX3-5), DPDK will use its built-in mlx driver, which further use libibverbs, which further relies on kernel IB stack. It\u2019s not a complete user solution somehow. Note that DPDK built-in mlx driver uses RAW_PACKET QPs. Internal \u00b6 Top-down: The user-facing part is called Envionmemt Abstraction Layer (EAL) , which provides a set of portable interfaces among many OSes. We can think it of as a \u201cPOSIX\u201d interface. This EAL has quite a lot useful and handy APIs, e.g., multicore support where you can call a function on arbitray cores (like the linux on_each_cpu core), timers, atomic operations, memory management APIs. I have built all these components myself, still very pleased to see this. Poll Mode Driver - we cover the mlx ones above Various other drivers RDMA \u00b6 Below is a list of RDMA-based systems I have used or the ones I think are useful. Mellanox libvma An userspace IB verbs based layer providing POSIX socket APIs. (The SocketDirect, SIGCOMM\u201819 paper was building a similar thing). verbs perftest The collection contains a set of bandwidth and latency benchmark such as: Send - ib_send_bw and ib_send_lat RDMA Read - ib_read_bw and ib_read_lat RDMA Write - ib_write_bw and ib_wriet_lat RDMA Atomic - ib_atomic_bw and ib_atomic_lat Native Ethernet (when working with MOFED2) - raw_ethernet_bw , raw_ethernet_lat rdma-core This is the core userspace IB verbs library (e.g., libibverbs). Whenever you are writing userspace RDMA applications, you are using this library. It is interesting to learn how userspace IB layer communicates with kernel. It is using ioctl() and mmap() to do the trick, quite standard. Not sure how io_uring would help here. The ABI interface (i.e., data structures) are quite complex and has several versions. libibverbs/example asyncwatch.c device_list.c devinfo.c pingpong.c rc_pingpong.c srq_pingpong.c uc_pingpong.c ud_pingpong.c xsrq_pingpong.c infiniband-diags ibv_devinfo iblinkinfo ibping ibaddr Kernel Infiniband stack RPC gRPC eRPC, NSDI\u201819","title":"DPDK and RDMA"},{"location":"notes/source_code/rdma/#on-dpdk-and-rdma-related-software","text":"Version History Date Description Dec 14, 2020 More on DPDK May 28, 2020 Copied from summary","title":"On DPDK and RDMA Related Software"},{"location":"notes/source_code/rdma/#dpdk","text":"","title":"DPDK"},{"location":"notes/source_code/rdma/#personal-notes","text":"DPDK DPDK uses VFIO to directly access physical device. Just like how we directly assign device to guest OS in QEMU (AFAIK, it is different for Mellanox NICs). Even though both DPDK and RDMA bypass kernel, their control path is very different. For DPDK, there is a complete device driver in the user space, and this driver communicate with the device via MMIO. After VFIO ioctls, all data and control path bypass kernel. For rdma-core, a lot control-path IB verbs (e.g., create_pd, create_cq) communicate with kernel via Infiniband device file ioctl. And you can see all those uverb hanlders in drivers/infiniband/core/uverbs.c Those control verbs will mmap some pages between user and kernel, so all following datapath IB verbs (e.g., post_send) will just bypass kernel and talk to device MMIO directly. Although rdma-core also has some vendor-specific \u201cdrivers\u201d, but this is really different from the above DPDK\u2019s userspace PCIe driver, per se. Userspace \u201crdma-core\u201d vendor-driver deals with the kernel devel vendor-level driver details. FWIW, if you are using a Mellanox VPI card in Ethernet mode (e.g. CX3-5), DPDK will use its built-in mlx driver, which further use libibverbs, which further relies on kernel IB stack. It\u2019s not a complete user solution somehow. Note that DPDK built-in mlx driver uses RAW_PACKET QPs.","title":"Personal Notes"},{"location":"notes/source_code/rdma/#internal","text":"Top-down: The user-facing part is called Envionmemt Abstraction Layer (EAL) , which provides a set of portable interfaces among many OSes. We can think it of as a \u201cPOSIX\u201d interface. This EAL has quite a lot useful and handy APIs, e.g., multicore support where you can call a function on arbitray cores (like the linux on_each_cpu core), timers, atomic operations, memory management APIs. I have built all these components myself, still very pleased to see this. Poll Mode Driver - we cover the mlx ones above Various other drivers","title":"Internal"},{"location":"notes/source_code/rdma/#rdma","text":"Below is a list of RDMA-based systems I have used or the ones I think are useful. Mellanox libvma An userspace IB verbs based layer providing POSIX socket APIs. (The SocketDirect, SIGCOMM\u201819 paper was building a similar thing). verbs perftest The collection contains a set of bandwidth and latency benchmark such as: Send - ib_send_bw and ib_send_lat RDMA Read - ib_read_bw and ib_read_lat RDMA Write - ib_write_bw and ib_wriet_lat RDMA Atomic - ib_atomic_bw and ib_atomic_lat Native Ethernet (when working with MOFED2) - raw_ethernet_bw , raw_ethernet_lat rdma-core This is the core userspace IB verbs library (e.g., libibverbs). Whenever you are writing userspace RDMA applications, you are using this library. It is interesting to learn how userspace IB layer communicates with kernel. It is using ioctl() and mmap() to do the trick, quite standard. Not sure how io_uring would help here. The ABI interface (i.e., data structures) are quite complex and has several versions. libibverbs/example asyncwatch.c device_list.c devinfo.c pingpong.c rc_pingpong.c srq_pingpong.c uc_pingpong.c ud_pingpong.c xsrq_pingpong.c infiniband-diags ibv_devinfo iblinkinfo ibping ibaddr Kernel Infiniband stack RPC gRPC eRPC, NSDI\u201819","title":"RDMA"},{"location":"notes/source_code/spdk/","text":"SPDK \u00b6 Version History Date Description Dec 14, 2020 More on DPDK May 28, 2020 Copied from summary The source code is here: https://github.com/spdk/spdk .","title":"SPDK"},{"location":"notes/source_code/spdk/#spdk","text":"Version History Date Description Dec 14, 2020 More on DPDK May 28, 2020 Copied from summary The source code is here: https://github.com/spdk/spdk .","title":"SPDK"},{"location":"notes/source_code/summary/","text":"Source Code Study \u00b6 Version History Date Description Dec 7, 2020 add sanitizers section Sep 13, 2020 some notes for python; add tcpstat Jul 26, 2020 Add OpenJDK! Hinted by Hacker News :) Jun 2, 2020 Add librcu Apr 26, 2020 Add wayland, X, gnome, gtk etc Apr 10, 2020 add graphics section Apr 6, 2020 add verbs perftes Mar 3, 2020 add FreeBSD, some fpga stuff Feb 4, 2020 add io_uring, firecracker Jan 31, 2020 Add some good stuff Jan 18, 2020 Initial Beautiful code is art. This page documents all the interesting & practical software/hardware/firmware I came across during my work. Nutrition Operating Systems Network Virtualization Compilers Bootloader and Firmware Web Servers KVS Databases RDMA and More Graphics FPGA Sanitizers Nutrition \u00b6 Projects supporting our day-to-day work. GNU glibc: libc, elf, and dynamic linker It is the default C library used by almost everyone It includes ld.so , the dynamic linker I wrote some notes about GOT/PLT and explains what has happend before main() is called. GNU binutils: gas, static linker, and more This repo has a lot commands like as , ld , objdump , nm and so on ld is static linker and I like the magic of its linker script I guess another useful repo is elfutils C Library GNU glibc used by major Linux distributions musl libc is a small libc impl used by Alpine Linux. Clean code. uClibc is a small libc targeting embedded cases bionic is Android\u2019s C library, math library, and dynamic linker [C++ Library] NVIDIA libcu++ strace System call tracer at userspace I\u2019ve designed one for LegoOS in kernel space Unix Commands Of course almost all other listed repos in this section have some sort of commands. But they are not essential. The following repos have the essential UNIX commands like ls, cat. It\u2019s not possible to go through all of them. But rather, I think they serve as references when we want to know how certain things are implemented (e.g., how dmesg get kernel log). BusyBox GNU Coreutils util-linux FreeBSD and its friends Tools tmux git Editors vim neovim C for life Some small and useful C projects cJSON : A lightweight JSON parser in C. userspace-rcu : A userspace RCU implementation library. Outliers CRIU: Checkpoint and Restore in Userspace The reason I love this repo is because it has so many interesting pieces on how to interact with kernel, save states, and restore them. In addition, it shows how to properly use many less well known syscalls. GRUB2: bootloader Learn how modern bootloader works. Detailed analysis of Linux booting sequence (how it transit from real-mode to protected mode, and finally to 64-bit mode, how to navigate Linux source code etc.) FFmpeg FFmpeg project is famous for its clean and neat C code. Besides, this project is used by a lot online video service companies io uring user liburing kernel io_uring.c Operating Systems \u00b6 See here . Network \u00b6 iperf3 is a TCP, UDP, and SCTP network bandwidth measurement tool tcpdump OpenSSH is our ssh! scapy : Python-based interactive packet manipulation program & library. Very neat tcpstat : C-based simple tool that could dump network traffic. Seems using pcap interface, the one used by tcpdump? Also checkout FreeBSD as it has tools like ifconfig , if . Click Modular Router Virtualization \u00b6 Also see: http://lastweek.io/notes/source_code/virt/ . libvirt: virsh and more QEMU Firecracker rust-vmm cloud-hypervisor Containers runc in go. containerd in go. docker in go. k8s in go. Compilers \u00b6 Clang, LLVM, in C++ This is a collection of projects. Clang is the frontend, compiles C/C++ code into LLVM\u2019s own IR format. The the backend LLVM will take multiple Passes to optimize the IR and the finally generate the assembly. The beauty of Clang and LLVM is that they can be used as libraries, and we could invoke them to manipulate the compilation results, to do source-to-source transforms, modify Pass\u2019s IR etc. I found this super interesting! To get started, I strongly recommend LLVM for Grad Students OpenJDK JRE = JVM + Runtime Classes => JVM is the one parsing the bytecode, along with some extra classes/libraries, they form JRE. JDK = JRE + Development Tools => JDK as in Development Kit therefore consists of some tools in addition to JRE. JDK is a monster collection of resources in one place. The JVM here is called HotSpot , a reference JVM implementation written in C++, Since JDK also has so many runtime support, it has a lot Java code. Personally I haven\u2019t written Java since 2013 or so. Although I\u2019m not using it anytime soon, I\u2019m curious how it performs nowadays. Python, in C cpython, as its name suggested, whose core is written in C. The core is within the Python folder. For those common built-in functions , they are organized here: https://github.com/lastweek/source-cpython/blob/master/Python/bltinmodule.c#L2878 GNU GCC Rustc, in Rust PHP, in C Google V8, in C++ Apple Swift, in C++ TCL, in C Perl 5, in C Lua, in C Ruby, in C Scala SpinalHDL Bootloader and Firmware \u00b6 See here . The open-source firmware landscape: FPGA \u00b6 My own Collection My own Paper Readings Partial Reconfiguration Partial Reconfiguration Building Framework Intepret Xilinx Bitstream HLS-based ICAP Controller Network Corundum: an FPGA-based NIC This is THE BEST network stack out there. This is not simply a network stack, it is a NIC. So what makes a NIC? First, PHY and MAC are basic. Second, PCIe connection between host and board. Third, DMA using PCIe, for TX and RX packets between host and board. Fourth, a host NIC driver; Fifth, some opt modules at NIC. This project has it all. Most amazingly, it works on so many boards. They have an FCCM\u201820 paper (finally!) describing the small modules inside. Verilog-Ethernet Self-made PHY, MAC IPs, ARP, IP, UDP stack This is also used by the Corundum project. Limago, HLS-based 100 GbE TCP/IP FPGA Network Stack This one came from ETH as well. This one is used by many papers, as far as i know, StRoM, EuroSys\u201820. It\u2019s mostly HLS-based. And has ETH/IP/UDP/TCP, RoCE v2 stack. Simulation, Synthesis, and P&R Icarus iverilog . iverilog is a compiler that translates Verilog source code into executable programs for simulation, or other netlist formats for further processing man page . VMware Cascade . Just-in-time compilation for Verilog, what a brilliant idea. Verilog-to-routing . Synthesis ( ODIN II ) Logic Optimization & Technology Mapping ( ABC ) Placement and Route ( VPR ) Web Servers \u00b6 Apache httpd nginx Key Value Stores \u00b6 Point of interests: 1) in-memory, and can it extend to use disk/ssd? 2) persistence support 3) network support RocksDB: A persistent KVS for Flash and RAM Storage. C++ LevelDB. C++ Memcached. C Redis. C etcd: Distributed reliable KVS. Go Databases \u00b6 MySQL PostgresSQL Yugabyte, distributed SQL RDMA and More \u00b6 See here Graphics \u00b6 More here X Server and Wayland X is being replaced by Wayland now.. Wayland code seems clean xvnc xvnc and its friends, are sitting on top of display manager (i.e., X/Wayland). They are clients of X/Wayland, but they act as X/Wayland servers for upper layer application such as GTK/Qt. It\u2019s a middleman, bringing network between X and GTK. TigerVNC, TurboVNC and so on. GNOME Shell and GTK GTK\u2019s default backend is X. GNOME shell is a layer on top of GTK+. Similar for KDE/Qt. xRDP, an RDP server. In C FreeRDP, client and server. In C Took a brief read of the code, it\u2019s super neat. Should take a serious look sometime. Vulkan/OpenCL Proton The landscape: Sanitizers \u00b6 There are many tools in both user and kernel space helping programmers identify various issues early on. Those issues including memory safty issue, threading issue, and others. Personally I have not used these tools a lot. But I am very interested in them. I think they could greatly improve productivity. TODO: https://github.com/google/sanitizers","title":"All-in-One"},{"location":"notes/source_code/summary/#source-code-study","text":"Version History Date Description Dec 7, 2020 add sanitizers section Sep 13, 2020 some notes for python; add tcpstat Jul 26, 2020 Add OpenJDK! Hinted by Hacker News :) Jun 2, 2020 Add librcu Apr 26, 2020 Add wayland, X, gnome, gtk etc Apr 10, 2020 add graphics section Apr 6, 2020 add verbs perftes Mar 3, 2020 add FreeBSD, some fpga stuff Feb 4, 2020 add io_uring, firecracker Jan 31, 2020 Add some good stuff Jan 18, 2020 Initial Beautiful code is art. This page documents all the interesting & practical software/hardware/firmware I came across during my work. Nutrition Operating Systems Network Virtualization Compilers Bootloader and Firmware Web Servers KVS Databases RDMA and More Graphics FPGA Sanitizers","title":"Source Code Study"},{"location":"notes/source_code/summary/#nutrition","text":"Projects supporting our day-to-day work. GNU glibc: libc, elf, and dynamic linker It is the default C library used by almost everyone It includes ld.so , the dynamic linker I wrote some notes about GOT/PLT and explains what has happend before main() is called. GNU binutils: gas, static linker, and more This repo has a lot commands like as , ld , objdump , nm and so on ld is static linker and I like the magic of its linker script I guess another useful repo is elfutils C Library GNU glibc used by major Linux distributions musl libc is a small libc impl used by Alpine Linux. Clean code. uClibc is a small libc targeting embedded cases bionic is Android\u2019s C library, math library, and dynamic linker [C++ Library] NVIDIA libcu++ strace System call tracer at userspace I\u2019ve designed one for LegoOS in kernel space Unix Commands Of course almost all other listed repos in this section have some sort of commands. But they are not essential. The following repos have the essential UNIX commands like ls, cat. It\u2019s not possible to go through all of them. But rather, I think they serve as references when we want to know how certain things are implemented (e.g., how dmesg get kernel log). BusyBox GNU Coreutils util-linux FreeBSD and its friends Tools tmux git Editors vim neovim C for life Some small and useful C projects cJSON : A lightweight JSON parser in C. userspace-rcu : A userspace RCU implementation library. Outliers CRIU: Checkpoint and Restore in Userspace The reason I love this repo is because it has so many interesting pieces on how to interact with kernel, save states, and restore them. In addition, it shows how to properly use many less well known syscalls. GRUB2: bootloader Learn how modern bootloader works. Detailed analysis of Linux booting sequence (how it transit from real-mode to protected mode, and finally to 64-bit mode, how to navigate Linux source code etc.) FFmpeg FFmpeg project is famous for its clean and neat C code. Besides, this project is used by a lot online video service companies io uring user liburing kernel io_uring.c","title":"Nutrition"},{"location":"notes/source_code/summary/#operating-systems","text":"See here .","title":"Operating Systems"},{"location":"notes/source_code/summary/#network","text":"iperf3 is a TCP, UDP, and SCTP network bandwidth measurement tool tcpdump OpenSSH is our ssh! scapy : Python-based interactive packet manipulation program & library. Very neat tcpstat : C-based simple tool that could dump network traffic. Seems using pcap interface, the one used by tcpdump? Also checkout FreeBSD as it has tools like ifconfig , if . Click Modular Router","title":"Network"},{"location":"notes/source_code/summary/#virtualization","text":"Also see: http://lastweek.io/notes/source_code/virt/ . libvirt: virsh and more QEMU Firecracker rust-vmm cloud-hypervisor Containers runc in go. containerd in go. docker in go. k8s in go.","title":"Virtualization"},{"location":"notes/source_code/summary/#compilers","text":"Clang, LLVM, in C++ This is a collection of projects. Clang is the frontend, compiles C/C++ code into LLVM\u2019s own IR format. The the backend LLVM will take multiple Passes to optimize the IR and the finally generate the assembly. The beauty of Clang and LLVM is that they can be used as libraries, and we could invoke them to manipulate the compilation results, to do source-to-source transforms, modify Pass\u2019s IR etc. I found this super interesting! To get started, I strongly recommend LLVM for Grad Students OpenJDK JRE = JVM + Runtime Classes => JVM is the one parsing the bytecode, along with some extra classes/libraries, they form JRE. JDK = JRE + Development Tools => JDK as in Development Kit therefore consists of some tools in addition to JRE. JDK is a monster collection of resources in one place. The JVM here is called HotSpot , a reference JVM implementation written in C++, Since JDK also has so many runtime support, it has a lot Java code. Personally I haven\u2019t written Java since 2013 or so. Although I\u2019m not using it anytime soon, I\u2019m curious how it performs nowadays. Python, in C cpython, as its name suggested, whose core is written in C. The core is within the Python folder. For those common built-in functions , they are organized here: https://github.com/lastweek/source-cpython/blob/master/Python/bltinmodule.c#L2878 GNU GCC Rustc, in Rust PHP, in C Google V8, in C++ Apple Swift, in C++ TCL, in C Perl 5, in C Lua, in C Ruby, in C Scala SpinalHDL","title":"Compilers"},{"location":"notes/source_code/summary/#bootloader-and-firmware","text":"See here . The open-source firmware landscape:","title":"Bootloader and Firmware"},{"location":"notes/source_code/summary/#fpga","text":"My own Collection My own Paper Readings Partial Reconfiguration Partial Reconfiguration Building Framework Intepret Xilinx Bitstream HLS-based ICAP Controller Network Corundum: an FPGA-based NIC This is THE BEST network stack out there. This is not simply a network stack, it is a NIC. So what makes a NIC? First, PHY and MAC are basic. Second, PCIe connection between host and board. Third, DMA using PCIe, for TX and RX packets between host and board. Fourth, a host NIC driver; Fifth, some opt modules at NIC. This project has it all. Most amazingly, it works on so many boards. They have an FCCM\u201820 paper (finally!) describing the small modules inside. Verilog-Ethernet Self-made PHY, MAC IPs, ARP, IP, UDP stack This is also used by the Corundum project. Limago, HLS-based 100 GbE TCP/IP FPGA Network Stack This one came from ETH as well. This one is used by many papers, as far as i know, StRoM, EuroSys\u201820. It\u2019s mostly HLS-based. And has ETH/IP/UDP/TCP, RoCE v2 stack. Simulation, Synthesis, and P&R Icarus iverilog . iverilog is a compiler that translates Verilog source code into executable programs for simulation, or other netlist formats for further processing man page . VMware Cascade . Just-in-time compilation for Verilog, what a brilliant idea. Verilog-to-routing . Synthesis ( ODIN II ) Logic Optimization & Technology Mapping ( ABC ) Placement and Route ( VPR )","title":"FPGA"},{"location":"notes/source_code/summary/#web-servers","text":"Apache httpd nginx","title":"Web Servers"},{"location":"notes/source_code/summary/#key-value-stores","text":"Point of interests: 1) in-memory, and can it extend to use disk/ssd? 2) persistence support 3) network support RocksDB: A persistent KVS for Flash and RAM Storage. C++ LevelDB. C++ Memcached. C Redis. C etcd: Distributed reliable KVS. Go","title":"Key Value Stores"},{"location":"notes/source_code/summary/#databases","text":"MySQL PostgresSQL Yugabyte, distributed SQL","title":"Databases"},{"location":"notes/source_code/summary/#rdma-and-more","text":"See here","title":"RDMA and More"},{"location":"notes/source_code/summary/#graphics","text":"More here X Server and Wayland X is being replaced by Wayland now.. Wayland code seems clean xvnc xvnc and its friends, are sitting on top of display manager (i.e., X/Wayland). They are clients of X/Wayland, but they act as X/Wayland servers for upper layer application such as GTK/Qt. It\u2019s a middleman, bringing network between X and GTK. TigerVNC, TurboVNC and so on. GNOME Shell and GTK GTK\u2019s default backend is X. GNOME shell is a layer on top of GTK+. Similar for KDE/Qt. xRDP, an RDP server. In C FreeRDP, client and server. In C Took a brief read of the code, it\u2019s super neat. Should take a serious look sometime. Vulkan/OpenCL Proton The landscape:","title":"Graphics"},{"location":"notes/source_code/summary/#sanitizers","text":"There are many tools in both user and kernel space helping programmers identify various issues early on. Those issues including memory safty issue, threading issue, and others. Personally I have not used these tools a lot. But I am very interested in them. I think they could greatly improve productivity. TODO: https://github.com/google/sanitizers","title":"Sanitizers"},{"location":"notes/source_code/virt/","text":"Notes about Virtualization \u00b6 Version History Date Description Feb 4, 2020 Add VFIO stuff Jan 26, 2020 Minor adjustment Jan 25, 2020 Initial Document We read the source code of QEMU and KVM, try to understand their interactions and the function flow. End of the day, you should be able to map your knowledge onto real codes. The document was orginally written in a Google Document, the following presentation is just an embedded version. For better readibility, you can also check out the: Google Doc Version PDF Version","title":"Virtualization"},{"location":"notes/source_code/virt/#notes-about-virtualization","text":"Version History Date Description Feb 4, 2020 Add VFIO stuff Jan 26, 2020 Minor adjustment Jan 25, 2020 Initial Document We read the source code of QEMU and KVM, try to understand their interactions and the function flow. End of the day, you should be able to map your knowledge onto real codes. The document was orginally written in a Google Document, the following presentation is just an embedded version. For better readibility, you can also check out the: Google Doc Version PDF Version","title":"Notes about Virtualization"},{"location":"rdma/rdma/","text":"Restless Dumb Memory Assassinate (RDMA) \u00b6 Q \u00b6 Atomic Operations: what exactly does the atomic mean in this context? RPC: SEND or RDMA Write with Immediate, which is better and why? Except \u00b6 One-sided v.s. Two sided SEND and RECV are two sided as the CPU at the responder needs to post a RECV in order for an incoming SEND to be processed. Unlike memory verbs, the responder\u2019s CPU is involved. One thing I do like to note is: the actual data transfer will not bother responder\u2019s CPU, the generated CQE will not bother it as well, only the pre-post action need CPU involvement. (HERD) CQE Generation Requester side: On completing a verb, the requester\u2019s NIC optionally signals completion by DMA-ing a completion entry (CQE) to a completion queue (CQ) associated with the QP. Of course, some WQE can be un-signaled. Responder side: NIC must DMA a CQE for completed RECV. (So I think this will not involve responder\u2019s CPU, right?) IB Specification \u00b6 The QP is the virtual interface that the hardware provides to an IBA consumer ; it serves as a virtual communication port for the consumer. Memory Region, L_Key, R_Key (sec 3.5.3/3.5.4) Used in RDMA requests. This is key in many design choices. Addressing (sec 3.5.10 and sec 4) Each QP has as queue pair number ( QPN ) assigned by the channel adapter which uniquely identifies the QP within the channel adapter . QPN GID, LID stuff IBA Semantic (sec 3.6) Channel (Send/Receive), classical I/O channel The message transmitted on the wire only names the destination\u2019s QP, the message does not describe where in the destination consumer\u2019s memory space the message content will be written. Instead, the destination QP contains addressing information used to deliver the message to the appropriate memory location. Pre-Post Receive Buffer (a channel semantic operation for SEND from remote.) Memory (RDMA) With memory semantics the initiating party directly reads or writes the virtual address space of a remote node. The remote party needs only communicate the location of the buffer; it is not involved with the actual transfer of the data. Hence, this style is sometimes referred to as single-ended communications. L_Key and R_Key used to validate access permission. Immediate Data RDMA Write and SEND can carry 4 bytes of Immediate data . sec 3.6 SEND can carry Immediate data for each send message. If included, the Immediate data is contained within an additional header field on the last packet of the SEND Operation (sec 9.4.1 SEND Operation). sec 3.7.4 An RDMA Write with immediate data will consume a receive WQE even though the QP did not place any data into the receive buffer since the IMMDT is placed in a CQE that references the receive WQE and indicates that the WQE has completed. sec 9.4.3 If specified by the verbs layer, Immediate data is included in the last packet of an RDMA WRITE message . The Immediate data is not written to the target virtual address range, but is passed to the client after the last RDMA WRITE packet is successfully processed. sec 10.7.2.2 C10-86: The responder\u2019s Receive Queue shall consume a Work Request when Immediate Data is specified in a successfully completed incoming RDMA Write. QP transport services RC RD UC UD IB Layers The network and link protocols deliver a packet to the desired destination. The transport portion of the packet delivers the packet to the proper QP and instructs the QP how to process the packet\u2019s data. Upper Layers (Consumer Operations). This is the layer most people focus on and try to optimize, right? IB Transaction Flow (sec 3.8) Describe the general flow. A nice read. So the WQE of RDMA Write/Read, the sender side\u2019s driver will create a CQE when sender get ACK from receiver? And that marks the end of a RDMA Read/Write? For RC, I think so. According to: When the originator receives an acknowledgment, it creates a CQE on the CQ and retires the WQE from the send queue. sec 3.2.1 Each time the remote consumer successfully executes a SEND operation, the hardware takes the next entry from the receive queue, places the received data in the memory location specified in that receive WQE, and places a CQE on the completion queue indicating to the consumer that the receive operation has completed. Thus the execution of a SEND operation causes a receive queue operation at the remote consumer. That is one important claim, the receiver side can poll the CQ to know if it has received a SEND or not. IB I/O Operations (sec 3.9) Interesting. So, instead of a Host Channel Adapter (HCA), we have Target Channel Adapter (TCA), which is attached to a IO device such as SSD. If we look from the IB layered architecture, everything below upper level protocols remain the same. In upper level protocols, which used to be Consumer, now is I/O controller. Do we have this kind of hardware on market? Fabric over NVMe? Transport Layer (sec 9) The transport header contains the information required by the endnode to complete the specified operation, e.g. delivery of data payload to the appropriate entity within the endnode such as a thread or IO controller . For a host platform, the client of the transport layer is the Verbs software layer . The client posts buffers or commands to these queues and hardware transfers data from or into the buffers. Reliable transport has response (acknowledge). Unreliable transport does not use acknowledgment messages. SEND can carry 4 bytes of Immediate data for each send message. If included, the Immediate data is contained within an additional header field on the last packet of the SEND Operation (sec 9.4.1 SEND Operation). WQ Packet Ordering Stuff (sec 9.5 Transaction Ordering): A requester shall transmit request messages in the order that the Work Queue Elements (WQEs) were posted. Reliable Service (sec 9.7) Before it can consider a WQE completed, the requester must wait for the necessary response(s) to arrive. If the requester requires an explicit response such that it can complete a given WQE, then the requester shall be responsible to take the necessary steps to ensure that the needed response is forthcoming. This section is still too much details on hardware behavior. But Mel must have more detailed stuff in house. Software Transport interface (sec 10) I think this section is trying to describe the various software concepts, such as HCA, Protection domain, and so on. The actual manipulations are carried out by Verbs, which are described in sec 11. A QP, which is a component of the channel interface, is NOT directly accessible by the Verbs consumer and can only be manipulated through the use of Verbs. A CQ can be used to multiplex work completions from multiple work queues across queue pairs on the same HCA. Shared Receive Queue (sec 10.2.9) (Is it used in Lego?) Memory Management (sec 10.6) Memory Region Able to register a virtually contiguous address range , even though the physical pages are not contiguous. Able to register a physically contiguous address range . Prior to invoking a Register Physical Memory Region or Reregister Physical Memory Region Verb, the Consumer should pin down in physical memory every physical buffer within the Memory Region. (But now Mellanox supports pgfault in their products, right?) Work Request (sec 10.7 and sec 10.8) Signaled Completion and Unsignaled Completion (sec 10.7.3.1) Finally meet these two description in tech documents. In Lego, we used to use unsignaled (polling), and then we change that to signaled handler. Submitting a list of Work Requests.. (10.8.2.1) .. the HCA is notified that one or more WQEs are ready to be processed. What is the mechanism of this notification? How does HCA got notified? HCA polling, or driver write something into HCA? Completion Queue Operations: poll a specified CQ for a Work Completion, that is ib_poll_cq() ! (sec 11.4.2) SG list Based on discussion with Shin-Yeh and Yiying. x3: if a sender uses one-sided RDMA write/read to send a sg-list to remote, the receiver side can only receive a consecutive memory buffer. x3: if a sender uses two-sided RDMA to SEND a sg-list to remote, the receiver can get a sg-list of buffers by pre-post RECV to receive queue. x4 and x5: looks like the User-Mode Memory Registration (UMR) can help to solve the one-sided RDMA issue (not verified).","title":"Restless Dumb Memory Assassinate (RDMA)"},{"location":"rdma/rdma/#restless-dumb-memory-assassinate-rdma","text":"","title":"Restless Dumb Memory Assassinate (RDMA)"},{"location":"rdma/rdma/#q","text":"Atomic Operations: what exactly does the atomic mean in this context? RPC: SEND or RDMA Write with Immediate, which is better and why?","title":"Q"},{"location":"rdma/rdma/#except","text":"One-sided v.s. Two sided SEND and RECV are two sided as the CPU at the responder needs to post a RECV in order for an incoming SEND to be processed. Unlike memory verbs, the responder\u2019s CPU is involved. One thing I do like to note is: the actual data transfer will not bother responder\u2019s CPU, the generated CQE will not bother it as well, only the pre-post action need CPU involvement. (HERD) CQE Generation Requester side: On completing a verb, the requester\u2019s NIC optionally signals completion by DMA-ing a completion entry (CQE) to a completion queue (CQ) associated with the QP. Of course, some WQE can be un-signaled. Responder side: NIC must DMA a CQE for completed RECV. (So I think this will not involve responder\u2019s CPU, right?)","title":"Except"},{"location":"rdma/rdma/#ib-specification","text":"The QP is the virtual interface that the hardware provides to an IBA consumer ; it serves as a virtual communication port for the consumer. Memory Region, L_Key, R_Key (sec 3.5.3/3.5.4) Used in RDMA requests. This is key in many design choices. Addressing (sec 3.5.10 and sec 4) Each QP has as queue pair number ( QPN ) assigned by the channel adapter which uniquely identifies the QP within the channel adapter . QPN GID, LID stuff IBA Semantic (sec 3.6) Channel (Send/Receive), classical I/O channel The message transmitted on the wire only names the destination\u2019s QP, the message does not describe where in the destination consumer\u2019s memory space the message content will be written. Instead, the destination QP contains addressing information used to deliver the message to the appropriate memory location. Pre-Post Receive Buffer (a channel semantic operation for SEND from remote.) Memory (RDMA) With memory semantics the initiating party directly reads or writes the virtual address space of a remote node. The remote party needs only communicate the location of the buffer; it is not involved with the actual transfer of the data. Hence, this style is sometimes referred to as single-ended communications. L_Key and R_Key used to validate access permission. Immediate Data RDMA Write and SEND can carry 4 bytes of Immediate data . sec 3.6 SEND can carry Immediate data for each send message. If included, the Immediate data is contained within an additional header field on the last packet of the SEND Operation (sec 9.4.1 SEND Operation). sec 3.7.4 An RDMA Write with immediate data will consume a receive WQE even though the QP did not place any data into the receive buffer since the IMMDT is placed in a CQE that references the receive WQE and indicates that the WQE has completed. sec 9.4.3 If specified by the verbs layer, Immediate data is included in the last packet of an RDMA WRITE message . The Immediate data is not written to the target virtual address range, but is passed to the client after the last RDMA WRITE packet is successfully processed. sec 10.7.2.2 C10-86: The responder\u2019s Receive Queue shall consume a Work Request when Immediate Data is specified in a successfully completed incoming RDMA Write. QP transport services RC RD UC UD IB Layers The network and link protocols deliver a packet to the desired destination. The transport portion of the packet delivers the packet to the proper QP and instructs the QP how to process the packet\u2019s data. Upper Layers (Consumer Operations). This is the layer most people focus on and try to optimize, right? IB Transaction Flow (sec 3.8) Describe the general flow. A nice read. So the WQE of RDMA Write/Read, the sender side\u2019s driver will create a CQE when sender get ACK from receiver? And that marks the end of a RDMA Read/Write? For RC, I think so. According to: When the originator receives an acknowledgment, it creates a CQE on the CQ and retires the WQE from the send queue. sec 3.2.1 Each time the remote consumer successfully executes a SEND operation, the hardware takes the next entry from the receive queue, places the received data in the memory location specified in that receive WQE, and places a CQE on the completion queue indicating to the consumer that the receive operation has completed. Thus the execution of a SEND operation causes a receive queue operation at the remote consumer. That is one important claim, the receiver side can poll the CQ to know if it has received a SEND or not. IB I/O Operations (sec 3.9) Interesting. So, instead of a Host Channel Adapter (HCA), we have Target Channel Adapter (TCA), which is attached to a IO device such as SSD. If we look from the IB layered architecture, everything below upper level protocols remain the same. In upper level protocols, which used to be Consumer, now is I/O controller. Do we have this kind of hardware on market? Fabric over NVMe? Transport Layer (sec 9) The transport header contains the information required by the endnode to complete the specified operation, e.g. delivery of data payload to the appropriate entity within the endnode such as a thread or IO controller . For a host platform, the client of the transport layer is the Verbs software layer . The client posts buffers or commands to these queues and hardware transfers data from or into the buffers. Reliable transport has response (acknowledge). Unreliable transport does not use acknowledgment messages. SEND can carry 4 bytes of Immediate data for each send message. If included, the Immediate data is contained within an additional header field on the last packet of the SEND Operation (sec 9.4.1 SEND Operation). WQ Packet Ordering Stuff (sec 9.5 Transaction Ordering): A requester shall transmit request messages in the order that the Work Queue Elements (WQEs) were posted. Reliable Service (sec 9.7) Before it can consider a WQE completed, the requester must wait for the necessary response(s) to arrive. If the requester requires an explicit response such that it can complete a given WQE, then the requester shall be responsible to take the necessary steps to ensure that the needed response is forthcoming. This section is still too much details on hardware behavior. But Mel must have more detailed stuff in house. Software Transport interface (sec 10) I think this section is trying to describe the various software concepts, such as HCA, Protection domain, and so on. The actual manipulations are carried out by Verbs, which are described in sec 11. A QP, which is a component of the channel interface, is NOT directly accessible by the Verbs consumer and can only be manipulated through the use of Verbs. A CQ can be used to multiplex work completions from multiple work queues across queue pairs on the same HCA. Shared Receive Queue (sec 10.2.9) (Is it used in Lego?) Memory Management (sec 10.6) Memory Region Able to register a virtually contiguous address range , even though the physical pages are not contiguous. Able to register a physically contiguous address range . Prior to invoking a Register Physical Memory Region or Reregister Physical Memory Region Verb, the Consumer should pin down in physical memory every physical buffer within the Memory Region. (But now Mellanox supports pgfault in their products, right?) Work Request (sec 10.7 and sec 10.8) Signaled Completion and Unsignaled Completion (sec 10.7.3.1) Finally meet these two description in tech documents. In Lego, we used to use unsignaled (polling), and then we change that to signaled handler. Submitting a list of Work Requests.. (10.8.2.1) .. the HCA is notified that one or more WQEs are ready to be processed. What is the mechanism of this notification? How does HCA got notified? HCA polling, or driver write something into HCA? Completion Queue Operations: poll a specified CQ for a Work Completion, that is ib_poll_cq() ! (sec 11.4.2) SG list Based on discussion with Shin-Yeh and Yiying. x3: if a sender uses one-sided RDMA write/read to send a sg-list to remote, the receiver side can only receive a consecutive memory buffer. x3: if a sender uses two-sided RDMA to SEND a sg-list to remote, the receiver can get a sg-list of buffers by pre-post RECV to receive queue. x4 and x5: looks like the User-Mode Memory Registration (UMR) can help to solve the one-sided RDMA issue (not verified).","title":"IB Specification"}]}